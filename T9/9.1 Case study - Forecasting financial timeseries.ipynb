{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FBZtL3uj7uo"
      },
      "source": [
        "## Forecasting financial timeseries\n",
        "### Introduction\n",
        "\n",
        "Stock market time series are not easy to model. The dynamics of a stock market time series is often described as a random walk, meaning it is considered as a stochastic process, which means there may be no patterns in the time series to model.\n",
        "\n",
        "Others suggest that the stock market is not necessarily a pure stochastic process, but that there may be some hidden pattern that a machine learning algorithm could be trained to uncover.\n",
        "\n",
        "In this lesson, we will make the assumption that we can accurately model stock prices, so that we can decide when to buy or sell a particular stock. Time series modelling can help us achieve this aim.\n",
        "\n",
        "We will start by looking over some basic techniques for modelling time series data before exploring a LSTM model. This will allow us to start simple and build up, as well as, set the context on why stock market timeseries is complex to model.\n",
        "\n",
        "First, objective task is to select a machine learning model that can take into account the history of a sequence of data and then predict future values of the sequence over a period of time in the future that we specify. We will look at some simple methods, before moving on to a more complex example. But first, we need some data!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOgjIkFIkT9X"
      },
      "source": [
        "### Financial timeseries data\n",
        "\n",
        "There are several ways to obtain stock data.  You can download historic stock data from a repository or you can use one of the many stock APIs, which provide both historic and current data about a whole range of stocks. StockAPIs will require you to sign up for an API key in order to use their service. You will also have a limit on the amount of data you can obtain.  Some potential sources of data include:\n",
        "\n",
        "- *Polygon.io*: https://polygon.io\n",
        "\n",
        "- *Yahoo Finance*: https://finance.yahoo.com\n",
        "\n",
        "There are some limitations when using stockAPIs, particularly on the free tier.  We need a lot of data to be able identify both long term and short term patterns, and these APIs may not provide the access to the volume of data we would need. Therefore we will download a prepared dataset for our experiment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoJnEdDOMFAZ"
      },
      "source": [
        "### Install Python Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3J5abkoUMFAa"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow pandas numpy matplotlib scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhZs9JWK4an6"
      },
      "source": [
        "### Loading the data\n",
        "In this case study, we have downloaded a dataset of *IBM* stock prices covering the period 1962 to 2017.\n",
        "\n",
        "The dataset is a historical record of daily market quotes, spanning from 1962 onwards. Each row represents one trading day, identified by the `Date` column in YYYY-MM-DD format. For example, 1962-01-02 indicates the first trading day of 1962. Because markets are closed at weekends and on bank holidays, successive dates may skip days; for instance, the entry after 1962-01-12 is 1962-01-15, reflecting the weekend gap. Chronological ordering is essential: downstream modelling relies on knowing that row 1's values precede row 2 in time.\n",
        "\n",
        "Following the date, there are four price fields: `Open`, `High`, `Low`, and `Close`:\n",
        "\n",
        "- `Open` is the price at which trading began that day; on 1962-01-02 it was 6.413.\n",
        "\n",
        "- `High` is the maximum price reached during the session, which for 1962-01-02 also happens to be 6.413, indicating the price never rose above the opening.\n",
        "\n",
        "- `Low` is the minimum price that day (6.3378 on 2 January 1962), and `Close` is the final price at which the security traded as the market closed (6.3378 on 2 January 1962).\n",
        "\n",
        "These four fields allow us to gauge intraday volatility. For example, on 5 January 1962, the price opened at 6.3211, dipped to a low of 6.1958, and closed at 6.2041. Those intraday swings can be used to compute metrics like the daily range or to engineer features such as percentage changes between `Open` and `Close` or between `High` and `Low`.\n",
        "\n",
        "- `Volume` column records the total number of contracts or shares traded over the course of the day. `Volume` is a core indicator of market liquidity; higher numbers typically signal heavier trading activity. For instance, on 8 January 1962, volume reached 655,676, which is substantially higher than the preceding day's 440,112, suggesting a surge of interest in that contract. Analysts often compare each day's trading volume against rolling averages to identify unusually active days, which could foreshadow breakouts or impending trend reversals.\n",
        "\n",
        "- `OpenInt` (Open Interest) column is the count of outstanding contracts that have not yet been settled. In this particular extract, however, every entry in the `OpenInt` column is zero, implying that either the data source did not provide open interest for this contract or that open interest data wasn't available for that period. Although open interest can be a valuable gauge of how many participants maintain positions overnight, it is not strictly necessary for simple `Close`-price forecasting. As a result, most modelling exercises focus on the OHLCV (Open, High, Low, Close, Volume) fields and disregard the zeroed open interest.\n",
        "\n",
        "Let's load the dataset. One important thing to do afterward we have loaded our data, is to ensure any time or date related columns are sorted chronologically:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcH16hwb4an6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/martyn-harris-bbk/AppliedMachineLearning/refs/heads/main/data/ibm.us.txt')\n",
        "\n",
        "# Sort DataFrame by date order\n",
        "df = df.sort_values('Date')\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNG8HZFFlwZu"
      },
      "source": [
        "We will also set some parameters to matplotlib for when we plot the time series data as we work through our examples, so that everything is consistent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIKlJzo94an3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# We will apply a clean, high‐contrast style to a Matplotlib plot.\n",
        "# Our parameters update tick and axis colours, as well as, font sizes.\n",
        "\n",
        "\n",
        "# Define parameters for tick and axis colours (black)\n",
        "params = {\n",
        "    \"ytick.color\": \"k\",       # y‐axis tick labels in black\n",
        "    \"xtick.color\": \"k\",       # x‐axis tick labels in black\n",
        "    \"axes.labelcolor\": \"k\",   # x/y label text in black\n",
        "    \"axes.edgecolor\": \"k\"     # axis spines in black\n",
        "}\n",
        "# Update Matplotlib's runtime configuration with these parameters\n",
        "plt.rcParams.update(params)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We visualise the daily closing prices over time. The plot draws directly from the `Close` column of our dataset and maps it against time on the x-axis to visual how the stock's value has evolved.\n",
        "\n",
        "Plotting this series is first step in any time-series analysis. It gives us an immediate sense of whether the data exhibits an overall upward or downward trend, whether it shows signs of periodic or seasonal behaviour, and where any sharp spikes or dips occur—perhaps due to market shocks, news events, or structural breaks. These features are difficult to spot in tabular form but are visually obvious on a line chart.\n",
        "\n",
        "Importantly, visualising the raw `Close` price helps us determine whether the data requires further preprocessing before being passed into a forecasting model. For instance, if we see very strong trends, we may consider detrending; if we observe erratic swings, smoothing techniques like moving averages might help; and if we notice missing periods or outliers, we can take corrective steps before training. This plot, then, informs us about the asset's historical behaviour but also guides how we shape the modelling pipeline:\n"
      ],
      "metadata": {
        "id": "qLXRMXKab3Jj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZv9st0j4an6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15,6))\n",
        "\n",
        "df['Close'].plot(color='blue', lw=1, label=\"Close\", alpha=1)\n",
        "\n",
        "plt.xlabel('Time (t)',fontsize=12)\n",
        "plt.ylabel('$ Close Price',fontsize=12)\n",
        "\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we zoom in on a specific 1,000‐day window (indices 10,000–10,999) to inspect the intraday price ranges more closely. Plotting the `Low` values in red and the `High` values in green allows us to clearly see how each day's price oscillated between its minimum and maximum.\n",
        "\n",
        "Overlaying the yellow `Mid` line, gives us the average of that day's low and high, and a smoothed centre point that often lies between the extremes.\n",
        "\n",
        "Examining such a window helps reveal short-term volatility and any abrupt shifts in trading behaviour; for instance, days where the gap between low and high suddenly widens might signal important news events or market reactions. We can then quickly identify each series and appreciate the amplitude of daily price swings, and how the midpoint evolves relative to those bounds:"
      ],
      "metadata": {
        "id": "sSIiUKUMckSJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKXIzrau4an7"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "plt.plot(\n",
        "    range(10000, 11000),               # X‐axis: indices from 10000 to 10999\n",
        "    df['Low'][10000:11000],            # Y‐axis: the Low values for that slice\n",
        "    color='red',                       # Plot in red to distinguish Low\n",
        "    lw=1,\n",
        "    label=\"Low\",                       # Legend label for this series\n",
        "    alpha=0.7                          # Transparent so overlaps are visible\n",
        ")\n",
        "\n",
        "plt.plot(\n",
        "    range(10000, 11000),               # Same x‐axis window\n",
        "    df['High'][10000:11000],           # Y‐axis: the High values for days 10000–10999\n",
        "    color='green',                     # Plot in green to distinguish High\n",
        "    lw=1,\n",
        "    label=\"High\",                      # Legend label for this series\n",
        "    alpha=0.7\n",
        ")\n",
        "\n",
        "plt.plot(\n",
        "    range(10000, 11000),               # Again, same x‐axis indices\n",
        "    ((df['Low'] + df['High']) / 2.0)[10000:11000],  # Midpoint: average of Low and High\n",
        "    color='yellow',\n",
        "    lw=1,\n",
        "    label=\"Mid\",                       # Legend label for the midpoint series\n",
        "    alpha=0.7\n",
        ")\n",
        "\n",
        "plt.xlabel('Time (t)')\n",
        "plt.ylabel('Prices ($)')\n",
        "\n",
        "plt.legend()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbGVrh_q4an7"
      },
      "source": [
        "### Extracting features\n",
        "\n",
        "There are several attributes we could use when modelling stock prices—such as the daily closing price, which reflects the final value of the stock at the end of each trading session. However, rather than relying solely on the close, we aim to capture a more balanced view of the daily movement by using a *mid-price*.\n",
        "\n",
        "The mid-price is calculated as the average of the day`s highest and lowest prices. This gives us a single feature that smooths out some of the daily noise and offers a clearer sense of the underlying trend. This mid-price will serve as the sole input feature for our model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCGzAisY4an7"
      },
      "outputs": [],
      "source": [
        "# First we calculate the mid prices from the highest and lowest prices for the stock\n",
        "high_prices = df.loc[:,'High'].to_numpy()\n",
        "low_prices = df.loc[:,'Low'].to_numpy()\n",
        "\n",
        "mid_prices = (high_prices + low_prices) * 0.5 # We take an average"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resampling\n",
        "\n",
        "To evaluate how well our model generalises, we need to divide the time series into training and testing sets. Unlike classification problems where we might use `train_test_split` from Scikit-learn to randomly partition features (`X`) and labels (`y`), forecasting problems require us to preserve the **temporal order** of the data. This is because future values must not leak into the training process.\n",
        "\n",
        "In this context, we are predicting the next value in a sequence rather than classifying a static label. Therefore, we manually split the series, using the earlier portion for training and holding back the later portion for testing. This approach allows us to simulate how the model would behave in a real-world setting, where future data is unseen during training.\n",
        "\n",
        "We use the mid-price sequence and reserve approximately 33% of it as test data. Computing a split index based on the total number of time steps (see code below) ensures the first 67% of the sequence is used to train the model, and the remaining 33% is kept for evaluation. This kind of chronological split respects the natural ordering of time-series data.\n",
        "\n",
        "Below is the code that performs this division, along with print statements to confirm the size of each subset:\n"
      ],
      "metadata": {
        "id": "GbTy7Vn7J5I1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the total number of mid-price observations\n",
        "size = len(mid_prices)\n",
        "\n",
        "# Define the proportion of data to reserve for testing (33%)\n",
        "test_size = 0.33\n",
        "\n",
        "# Compute the index at which to split the series:\n",
        "# size * test_size gives the size of the test set;\n",
        "# subtracting this from the total gives the split index\n",
        "test_split = size - round(size * test_size)\n",
        "\n",
        "# Slice the array to create the training set (first 67% of data)\n",
        "train_data = mid_prices[:test_split]\n",
        "\n",
        "# Slice the array to create the testing set (last 33% of data)\n",
        "test_data = mid_prices[test_split:]\n",
        "\n",
        "# Record the actual sizes of each set for reference\n",
        "train_size = len(train_data)\n",
        "test_size = len(test_data)\n",
        "\n",
        "# Display the sizes to confirm the split\n",
        "print(train_size)\n",
        "print(test_size)"
      ],
      "metadata": {
        "id": "np-lIjMXJ_lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This ensures we build the model on past information and assess it on genuinely unseen future data, mimicking a real forecasting scenario."
      ],
      "metadata": {
        "id": "6Qc7LjZeJuQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing the Data for Neural Network Training\n",
        "\n",
        "Neural networks are sensitive to the scale of input features. When training on raw price data, especially stock prices that can range widely over time, large input magnitudes can lead to unstable optimisation often causing issues such as vanishing or exploding gradients. This makes training slow or ineffective.\n",
        "\n",
        "To address this, we apply *min–max* scaling to  rescale the data to lie within a consistent range (typically between 0 and 1). This ensures that all input values are treated proportionally, allowing the network to converge more reliably during training (hopefully).\n",
        "\n",
        "We begin by reshaping the training and testing arrays into the 2D format expected by scikit-learn’s `MinMaxScaler`. Importantly, we fit the scaler *only* on the training set. This avoids *data leakage*, a situation where information from the test set influences the training process, leading to overly optimistic performance estimates. Once fitted, we apply the same transformation to the test data, ensuring both subsets are scaled consistently:"
      ],
      "metadata": {
        "id": "jh-xfqGDLZHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Initialise the MinMaxScaler to normalise inputs to the range [0, 1]\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Reshape the data to 2D arrays (required by sklearn scalers)\n",
        "train_data = train_data.reshape(-1, 1)\n",
        "test_data = test_data.reshape(-1, 1)\n",
        "\n",
        "# Fit the scaler on the training set only and transform it\n",
        "train_data = scaler.fit_transform(train_data)\n",
        "\n",
        "# Apply the same transformation to the test set\n",
        "test_data = scaler.transform(test_data)"
      ],
      "metadata": {
        "id": "QcG3PK8dLZRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step is essential for training an LSTM (or any deep learning model) on financial data, as it ensures stable gradient flow and efficient learning across epochs.\n",
        "\n",
        "We now smooth the time series by applying a window size of 2000, and transform the values obtained from each sliding window. We select a large value as the window size, as the sliding window process can create a break at the end of each window, due to the fact that each window is normalised independently.  This means that there are points in our data that are affected by this process, causing a small loss in information, but there tends not to be enough data points affected for this to have a large impact on performance of the model.\n",
        "\n",
        "We also reshape the training and test data back to the original row and column shape ready for the next step:"
      ],
      "metadata": {
        "id": "WawJMuyJLRoF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cm68J2UA4an8"
      },
      "outputs": [],
      "source": [
        "# Train the MinMaxScaler with training data and smooth data\n",
        "smoothing_window_size = 2000\n",
        "\n",
        "# Generate a series of windows over the training data for smoothing (window size = 2000).\n",
        "for di in range(0, 8000, smoothing_window_size):\n",
        "    scaler.fit(train_data[di : di + smoothing_window_size, : ])\n",
        "    train_data[di: di + smoothing_window_size, : ] = scaler.transform(train_data[di : di + smoothing_window_size, :])\n",
        "\n",
        "# Normalise the remaining data\n",
        "scaler.fit(train_data[di + smoothing_window_size: , :])\n",
        "train_data[di + smoothing_window_size : , : ] = scaler.transform(train_data[di + smoothing_window_size: , :])\n",
        "\n",
        "# Reshape both train and test to their original arrangement.\n",
        "train_data = train_data.reshape(-1)\n",
        "\n",
        "# Normalise test data using the scaler that we fit to the training data\n",
        "test_data = scaler.transform(test_data).reshape(-1)\n",
        "\n",
        "mid_prices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxqtqXln4an8"
      },
      "source": [
        "### Basic methods: One-Step Ahead Prediction\n",
        "\n",
        "Before diving into complex models, it’s helpful to establish some baseline methods for forecasting. One of the simplest ways to predict the next time step in a stock price series is to use averaging techniques. These methods estimate the future price based on the historical values, without the need for training or tuning parameters.\n",
        "\n",
        "You may have seen impressive examples where researchers apply deep learning models to predict exact future stock prices. But if precise forecasting were that easy, such models would be used to consistently beat the market, and their creators would likely be very wealthy. In reality, exact-value prediction in financial time series is highly uncertain and often impractical. Simple, interpretable methods can often perform just as well or better in short-term forecasts. Here, we explore two straightforward techniques:\n",
        "\n",
        "- *Simple averaging*, where we take the mean of the last few observations, and\n",
        "- *Exponential Moving Average (EMA)*, which gives more weight to recent prices.\n",
        "\n",
        "We’ll apply each method to predict the next value in the series, one step at a time. For each prediction, we compute the squared difference from the actual next value—this gives us a *Mean Squared Error (MSE)* for each time step.\n",
        "\n",
        "We will be collecting all these individual errors into an array, so that we can then compute the overall average MSE for each method. This gives us a quantitative basis for comparing the two approaches and helps us understand how well these naive predictors perform before we introduce more advanced models:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Simple average\n",
        "\n",
        "First, we attempt to predict the stock market mid price for the next time step (`t+1`) as an average of the previously observed stock market prices within a fixed window size.  We will set the size of the window to a 30 day period representing a month:"
      ],
      "metadata": {
        "id": "ryUfnV8DhYc1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ewX3to94an8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Set the window size for the averaging method — the number of past points to use for each prediction\n",
        "window_size = 30\n",
        "\n",
        "# Get the total number of observations in the training data\n",
        "N = train_data.size\n",
        "\n",
        "# Initialise empty lists to store predicted values, corresponding dates, and error terms\n",
        "std_avg_predictions = []  # one-step-ahead predictions using simple average\n",
        "std_avg_x = []            # x-axis labels (dates) for plotting\n",
        "mse_errors = []           # squared error between predicted and actual values\n",
        "\n",
        "# Loop through each time step starting from the end of the first full window\n",
        "for pred_idx in range(window_size, N):\n",
        "\n",
        "    # Select the date corresponding to this prediction index\n",
        "    if pred_idx >= N:\n",
        "        # (This clause will not trigger inside the loop, but is often used when forecasting beyond known data)\n",
        "        date = dt.datetime.strptime(k, '%Y-%m-%d').date() + dt.timedelta(days=1)\n",
        "    else:\n",
        "        # For in-sample predictions, fetch the corresponding date from the original dataframe\n",
        "        date = df.loc[pred_idx, 'Date']\n",
        "\n",
        "    # Extract the previous `window_size` data points (e.g., previous 30 days)\n",
        "    window_of_observations = train_data[pred_idx - window_size : pred_idx]\n",
        "\n",
        "    # Calculate the mean of the window — our predicted value for the next time step\n",
        "    avg = np.mean(window_of_observations)\n",
        "\n",
        "    # Append the prediction to the list\n",
        "    std_avg_predictions.append(avg)\n",
        "\n",
        "    # Compute and store the squared error for this prediction\n",
        "    mse_errors.append((avg - train_data[pred_idx]) ** 2)\n",
        "\n",
        "    # Store the date for plotting\n",
        "    std_avg_x.append(date)\n",
        "\n",
        "# Compute and print the average Mean Squared Error, multiplied by 0.5 (optional, for comparison with half-MSE formulations)\n",
        "print('MSE error for standard averaging: %.5f' % (0.5 * np.mean(mse_errors)))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Combine train and test data for full-series visualisation\n",
        "full_data = np.concatenate([train_data, test_data], axis=0)\n",
        "\n",
        "# Fit the scaler on the full data just for visualisation purposes (not used here directly)\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "all_mid_data_scaled = scaler.fit_transform(full_data.reshape(-1, 1)).astype(\"float32\")\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# Plot the true mid-price series (grey) across the full timeline\n",
        "plt.plot(range(df.shape[0]), full_data, color='grey', label='True Mid-Price', lw=1, alpha=0.7)\n",
        "\n",
        "# Overlay our predictions from the simple moving average (red)\n",
        "# Start plotting from index = window_size since that's when predictions begin\n",
        "plt.plot(range(window_size, N), std_avg_predictions, color='red', label='Simple Average Prediction', lw=1, alpha=0.8)\n",
        "\n",
        "plt.xlabel('Time (t)')\n",
        "plt.ylabel('Mid Price ($)')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.title('One-Step ahead prediction using simple Averaging')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipNuHflR4an9"
      },
      "source": [
        "As we can see it follows the pattern of the stock price quite closely. The overall MSE suggests that the model performed quite well, but only if we want to obtain very short predictions into the future. This approach seems reasonable when we consider the fact that the stock price is unlikely to deviate too extremely over a 30 day period.\n",
        "\n",
        "However, there are more sophisticated averaging techniques that we can explore in the form of the *Exponential Moving Average (EMA)*:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAdOlliu4an9"
      },
      "source": [
        "#### Exponential Moving Average (EMA)\n",
        "\n",
        "The Exponential Moving Average (EMA) is a variation of the moving average that gives more weight to recent data points. While a simple moving average (SMA) treats all values in the window equally, EMA applies a decaying weight, so newer observations influence the result more than older ones.\n",
        "\n",
        "This weighting makes the EMA more responsive to recent changes in stock price, which can be especially useful when tracking short-term trends or detecting reversals more quickly. Unlike the SMA, which smooths data evenly, the EMA reacts more sharply to recent movements while still preserving the smoothing benefits of averaging.\n",
        "\n",
        "In our case, we apply an EMA over a 30-day window and use a smoothing factor of 0.8. This parameter controls how heavily recent values are weighted: a higher value (closer to 1) gives more influence to the most recent data, making the EMA even more sensitive to price fluctuations. A lower value (closer to 0) would behave more like a traditional moving average with a longer memory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6a8VM214an9"
      },
      "outputs": [],
      "source": [
        "# Smoothing factor for the exponential moving average (EMA)\n",
        "# A value closer to 1 gives more weight to past EMA, a value closer to 0 gives more weight to the latest data point\n",
        "weight = 0.8\n",
        "\n",
        "# Total number of points in the training dataset\n",
        "N = train_data.size\n",
        "\n",
        "# Lists to store EMA predictions, corresponding x-values (dates), and squared errors\n",
        "run_avg_predictions = []\n",
        "run_avg_x = []\n",
        "mse_errors = []\n",
        "\n",
        "# Initialise the running mean (EMA) to 0 before starting\n",
        "running_mean = 0.0\n",
        "\n",
        "# The first EMA prediction (at index 0) is set to the initial running_mean (0.0)\n",
        "run_avg_predictions.append(running_mean)\n",
        "\n",
        "# Loop over each index in the training data to update the EMA and compute errors\n",
        "for pred_idx in range(1, N):\n",
        "    # Update the running mean (EMA) using the previous EMA and the last actual data point:\n",
        "    #    EMA_t = weight * EMA_{t-1} + (1 - weight) * actual_{t-1}\n",
        "    running_mean = running_mean * weight + (1.0 - weight) * train_data[pred_idx - 1]\n",
        "\n",
        "    # Append the new EMA value as the prediction for the current index\n",
        "    run_avg_predictions.append(running_mean)\n",
        "\n",
        "    # Compute the squared error between the EMA prediction and the true value at this index\n",
        "    mse_errors.append((run_avg_predictions[-1] - train_data[pred_idx]) ** 2)\n",
        "\n",
        "    # Record the corresponding date (or index) for plotting—'date' must be defined earlier to align with pred_idx\n",
        "    run_avg_x.append(date)\n",
        "\n",
        "# After looping through all points, calculate and print the mean squared error (MSE) for the EMA predictions:\n",
        "# We divide by 2 to match a half‐MSE convention (0.5 * mean squared error).\n",
        "print('MSE error for EMA averaging: %.5f' % (0.5 * np.mean(mse_errors)))\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize = (15, 6))\n",
        "\n",
        "plt.plot(range(df.shape[0]), full_data, color='grey', lw=1, label='True', alpha=0.7)\n",
        "plt.plot(range(0,N), run_avg_predictions, color='red', lw=1, label='Prediction', alpha=0.5)\n",
        "\n",
        "plt.xlabel('Time (t)')\n",
        "plt.ylabel('Mid Price ($)')\n",
        "\n",
        "plt.title('One-Step ahead prediction using EMA averaging')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfHsoxxO4an9"
      },
      "source": [
        "After plotting the results, we can see that EMA fits a perfect line over the true distribution with a very low MSE suggesting the model is a good fit.\n",
        "\n",
        "As we have seen, we have done a good job of predicting the next value of a stock. However, we are only limited to one-step ahead prediction. We would ideally like to model longer periods into the future than a single day in order to have more time to act should we wish to buy or sell stock.\n",
        "\n",
        "In general, predicting the actual value of a stock at future points in time is not that useful for time series analysis.  We are more interested in summarising the trend of the stock price in the future. We would like to know whether it will go up or down in price over the next 30 days, for instance, but to do this we need to address the short-comings of these short-term predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0Soh-xj4an9"
      },
      "source": [
        "### LSTMs approach\n",
        "\n",
        "LSTM models have dominated time series prediction recently, as they are well-suited to modelling sequential data that have both short and long range dependencies between data points in the past.  We can therefore use Long Short-Term Memory models to predict any number of time points into the future for our stock.\n",
        "\n",
        "To recap from an earier lesson, an LSTM node (or cell) has 5 components, which allow it to model both long-term and short-term data points by storing a memory of past observations, which it keeps, updates, or discards using a number of different gates:\n",
        "\n",
        "- *Cell state*: The internal memory which stores short and long-term memories\n",
        "- *Hidden state*: The output state information calculated using the combinations of the current input, previous hidden state and current cell input which are used to predict the future stock prices. The hidden state selects between the short or long-term memory (sometimes both types) stored in the cell state to make the next prediction.\n",
        "- *Input gate*: Selects how much information from the current input flows into the cell state\n",
        "- *Forget gate*: Selects how much information from the current input and the previous cell state flows into the current cell state.\n",
        "- *Output gate*: Decides on how much information from the current cell state flows into the hidden state, selecting between the long-term memories, or both short-term memories and long-term memories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GTXJibJ4an9"
      },
      "source": [
        "### Smoothing\n",
        "Before training our LSTM model, we first smooth the input series using the Exponential Moving Average (EMA) introduced earlier. This step helps to reduce short-term noise—those sharp, erratic fluctuations in price—and instead emphasises the broader trend over time.\n",
        "\n",
        "Smoothing the data makes it easier for the model to learn meaningful patterns without being distracted by minor, high-frequency variations that may not contribute to long-term forecasting performance.\n",
        "\n",
        "In the following example, we apply this smoothing *only* on the training data, preserving the integrity of the test set for fair evaluation later on.\n",
        "\n",
        "We first initialise the smoothed value (EMA) to zero and define a smoothing constant *gamma*. A smaller value of gamma (e.g. 0.1) smooths more heavily, meaning the model reacts slowly to new changes. A higher value (e.g. 0.8) would make the EMA more responsive to recent movements (as we discussed).\n",
        "\n",
        "We then loop through the training data and recursively compute the smoothed series. Each new value is a weighted blend of the current raw input and the previously smoothed value. The result is a version of the training sequence with reduced noise, making it easier for the LSTM to model longer-term patterns.\n",
        "\n",
        "Once smoothing is complete, we concatenate the modified training data with the untouched test set. This combined series is then scaled using `Min–Max` normalisation, mapping all values to the range [0, 1] to ensure we have stable gradient updates and to avoid issues caused by differing value magnitudes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BBdiRl24an9"
      },
      "outputs": [],
      "source": [
        "# Apply exponential moving average smoothing to the training data.\n",
        "# This helps reduce short-term volatility and sharp spikes.\n",
        "EMA = 0.0             # Initialise the EMA value\n",
        "gamma = 0.1           # Smoothing factor (controls the weight of recent values)\n",
        "\n",
        "# Loop through the training set and apply recursive EMA smoothing\n",
        "for ti in range(train_size):\n",
        "    EMA = gamma * train_data[ti] + (1 - gamma) * EMA\n",
        "    train_data[ti] = EMA            # Overwrite the value with its smoothed version\n",
        "\n",
        "# Combine smoothed training data with raw test data\n",
        "# (used later for generating a full scaled dataset for inference and plotting)\n",
        "all_mid_data = np.concatenate([train_data, test_data], axis=0)\n",
        "\n",
        "# Initialise the MinMaxScaler to normalise all values between 0 and 1\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "# Apply scaling - reshape to 2D as required by sklearn\n",
        "all_mid_data_scaled = scaler.fit_transform(\n",
        "    all_mid_data.reshape(-1, 1)\n",
        ").astype(\"float32\")    # Keep data type consistent for compatibility with model inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvbQxCAz4an-"
      },
      "source": [
        "Essentially we have the same data as the previous EMA example, but you can begin to copy from this point forth if you wish to experiment in a separate notebook later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDA6lTmD4aoC"
      },
      "source": [
        "### The model\n",
        "\n",
        "We now get to the best part, and set up an LSTM model to learn patterns in our stock price data. LSTM stands for *Long Short-Term Memory*, as we saw it's a type of neural network that's especially good at working with sequences, like time series.\n",
        "\n",
        "The key idea behind an LSTM is that it doesn't just look at one point in time. Instead, it remembers what it saw before and uses that memory to make better predictions. At each step, the model takes in the stock price from the current day and also carries forward what it learned from previous days.\n",
        "\n",
        "Before we train the model, we need to set a few key settings. These are our *hyperparameters*. First, we define `D`, which is the number of input features. In our case, we’re using just one: the mid-price of the stock.\n",
        "\n",
        "Next is `num_unrollings`. This tells the model how many time steps to look back when it makes a prediction. Instead of learning from just one day at a time, we let the model learn from a sequence of, say, 50 days. The longer the sequence, the more context the model has, which often leads to better forecasts.\n",
        "\n",
        "We also set the `batch_size`. This tells the model how many training samples to process at once. For example, if `batch_size = 32`, the model will train on 32 small sequences of stock prices in parallel.\n",
        "\n",
        "Lastly, we define the number of neurons (or units) in each layer of the model, using the `num_nodes` parameter. These neurons are the model's internal processing units. In this setup, we use a deep LSTM with three layers: the first two layers have 200 units each, and the third has 150. More layers and neurons allow the model to learn more complex patterns, though they also require more data and time to train:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BaBkEiO4aoD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "D = 1                     # Dimensionality of the data (1-D series)\n",
        "num_unrollings = 50       # Number of time steps\n",
        "batch_size = 500          # Samples per batch\n",
        "\n",
        "num_nodes = [200,200,150] # Hidden units in each of the 3 LSTM layers\n",
        "\n",
        "n_layers = len(num_nodes) # 3 layers\n",
        "\n",
        "dropout = 0.2             # Dropout rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6kopPp34aoD"
      },
      "source": [
        "#### Inputs and outputs layers\n",
        "\n",
        "Before we can train our LSTM model, we need to reshape our price data into a format that the network can learn from. LSTMs are designed to learn from *sequences*, so instead of feeding the model input layer a single price at a time, we give it short windows of consecutive prices and ask it to predict the next one in the sequence.\n",
        "\n",
        "We do this using a technique called a *sliding window*. The function `make_windows()` breaks the data into overlapping sequences: for each window, it takes `num_unrollings` consecutive prices as the input (`X`) and the next price as the target output (`Y`). For example, if `window_size = 50`, the model sees the last 50 prices and tries to predict the 51st. The output `X_train` and `X_test` have three dimensions:\n",
        "\n",
        "- *number of samples*: how many windows we created.\n",
        "- *window size*: number of time steps in each input.\n",
        "- *features*: just 1 in our case, the mid-price.\n",
        "\n",
        "The labels `Y_train` and `Y_test` are 2D arrays, containing the price the model is trying to predict for each sequence. Finally, we wrap the training data into a TensorFlow `Dataset`. This is a high-performance pipeline that:\n",
        "\n",
        "- shuffles the windows (so the model doesn’t learn the order explicitly),\n",
        "- groups them into *batches* (so training runs faster),\n",
        "- and prefetches data to keep the training loop efficient.\n",
        "\n",
        "This preprocessing step ensures that the LSTM is trained on meaningful sequences of data, delivered in the right shape and format for learning time-dependent patterns in stock prices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jh-X_r9v4aoD"
      },
      "outputs": [],
      "source": [
        "# Build our own TensorFlow dataset for later use in training the LSTM\n",
        "import numpy as np\n",
        "\n",
        "# train_data and test_data are 1D NumPy arrays of prices (already smoothed and scaled earlier)\n",
        "# We now convert these into supervised learning datasets: sequences of past values predicting the next value\n",
        "\n",
        "def make_windows(data_array, window_size):\n",
        "    # Converts a 1D time series into overlapping input/output pairs:\n",
        "    # Each input X is a sequence of window_size consecutive prices\n",
        "    # Each output Y is the value that immediately follows the sequence\n",
        "\n",
        "    X, Y = [], []\n",
        "    for i in range(len(data_array) - window_size):\n",
        "        # Create one input window: [price_i, price_{i+1}, ..., price_{i+window_size-1}]\n",
        "        X.append(data_array[i : i + window_size])\n",
        "\n",
        "        # Store the next value after the window: price_{i+window_size}\n",
        "        Y.append(data_array[i + window_size])\n",
        "\n",
        "    # Reshape X to (samples, timesteps, features) as expected by LSTM\n",
        "    X = np.array(X).reshape(-1, window_size, 1)\n",
        "\n",
        "    # Reshape Y to (samples, 1) — each target is a single price value\n",
        "    Y = np.array(Y).reshape(-1, 1)\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "# Apply windowing to training and test sets using the same window size (num_unrollings)\n",
        "X_train, Y_train = make_windows(train_data, num_unrollings)\n",
        "X_test,  Y_test  = make_windows(test_data,  num_unrollings)\n",
        "\n",
        "# Prepare the training dataset using TensorFlow's efficient Dataset API\n",
        "train_dataset = (\n",
        "    tf.data.Dataset\n",
        "      .from_tensor_slices((X_train, Y_train))  # Wrap X and Y arrays into a tf.data.Dataset\n",
        "      .shuffle(buffer_size=10000)              # Shuffle data to improve training stability\n",
        "      .batch(batch_size, drop_remainder=True)  # Group samples into batches; discard leftover if incomplete\n",
        "      .prefetch(tf.data.AUTOTUNE)              # Optimise performance by preloading batches during training\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To recap, each sample in the dataset contains a small window of past prices used to predict the next price. The `tf.data.Dataset` pipeline enables efficient batching, shuffling, and preloading, ensuring smooth and scalable training with your LSTM model. This is also good, since you can easily pickle the dataset and load it back later."
      ],
      "metadata": {
        "id": "vNcr2Qliaigm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15RO3lTN4aoE"
      },
      "source": [
        "To forecast the next stock price value, we construct a deep LSTM model with three stacked LSTM layers followed by a dense output layer. Each LSTM layer processes sequential price data and passes its output to the next layer, enabling the network to learn increasingly complex temporal patterns.\n",
        "\n",
        "We begin by specifying the input shape: a sequence of num_unrollings time steps, each containing a single feature (the mid-price), which we define with a Keras Input layer. The data then flows through three LSTM layers, each configured with a specific number of hidden units (num_nodes). We apply dropout after each LSTM layer to reduce overfitting by randomly omitting a fraction of the units during training. This encourages the model to learn more robust features and generalise better to unseen data.\n",
        "\n",
        "The final layer is a fully connected Dense layer that takes the output of the last LSTM cell and produces a single numeric prediction representing the estimated price at the next time step. This acts as a simple linear regression output on top of the learned sequence representation. The full model construction and compilation code, is shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "basAVzYE4aoE"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, LSTM, Dropout, Dense\n",
        "\n",
        "# Define the input shape for the LSTM model\n",
        "# Each input sequence has `num_unrollings` time steps, with `D` features per time step (D = 1 for univariate data)\n",
        "inputs = Input(shape=(num_unrollings, D), name='lstm_input')\n",
        "\n",
        "# First LSTM layer:\n",
        "# Returns the full sequence of hidden states (one per time step) for the next LSTM layer to process\n",
        "# Number of hidden units is specified in num_nodes[0]\n",
        "x = LSTM(num_nodes[0], return_sequences=True)(inputs)\n",
        "# Dropout layer to help prevent overfitting by randomly disabling a fraction of units\n",
        "x = Dropout(dropout)(x)\n",
        "\n",
        "# Second LSTM layer:\n",
        "# Again returns the full sequence so the next LSTM layer receives the entire time series\n",
        "x = LSTM(num_nodes[1], return_sequences=True)(x)\n",
        "x = Dropout(dropout)(x)\n",
        "\n",
        "# Third LSTM layer:\n",
        "# This time we only return the last output in the sequence (i.e., a summary of the full input)\n",
        "x = LSTM(num_nodes[2], return_sequences=False)(x)\n",
        "x = Dropout(dropout)(x)\n",
        "\n",
        "# Final output layer:\n",
        "# A Dense layer with a single neuron to produce the final predicted value (e.g., next day's price)\n",
        "outputs = Dense(1, name='price_output')(x)\n",
        "\n",
        "# Define the full model by linking the input and output layers\n",
        "model = Model(inputs=inputs, outputs=outputs, name='three_layer_lstm')\n",
        "\n",
        "# Print a summary of the model structure\n",
        "model.summary()\n",
        "\n",
        "# Compile the model with:\n",
        "# Mean Squared Error (MSE) as the loss function for regression\n",
        "# Adam optimiser for efficient gradient descent\n",
        "learning_rate = 0.001\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "    loss='mse',     # loss function to minimise\n",
        "    metrics=['mse'] # also report MSE during training\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model training"
      ],
      "metadata": {
        "id": "qdR47UerPfuc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N90Voa914aoF"
      },
      "source": [
        "Here, we train an LSTM model to predict stock price movements over several training cycles (called *epochs*) and observe whether its predictions improve with each pass through the data. The goal is to see if the model can learn meaningful patterns in the time series and generalise to new, unseen points.\n",
        "\n",
        "During training, we evaluate the model's performance not just on the training data, but also by making rolling forecasts at specific locations within the test set. These test points serve as reference positions where we ask the model to predict several steps ahead and then compare those predictions to the actual market data.\n",
        "\n",
        "The prediction horizon defines how far into the future we want our model to forecast. In other words, it's the number of time steps the model is expected to predict beyond the current point. For example, if you're driving and your satnav tells you only the next 10 metres, it's not very helpful. But if it gives you directions 1 km ahead, you can plan your actions better. Similarly, the prediction horizon gives your model a chance to guide decisions further into the future.\n",
        "\n",
        "The training process involves the following key steps:\n",
        "\n",
        "- Select evaluation points (`test_points_seq`) across the time series where we want to test the model's forecasting ability.\n",
        "\n",
        "- Then, for each *epoch*:\n",
        "  - Unroll the data into short sequences of length `num_unrollings`.\n",
        "  - Train the model on these sequences, one batch at a time, updating its internal weights.\n",
        "  - Track the average loss on the training set to monitor progress.\n",
        "\n",
        "- After training for that epoch:\n",
        "\n",
        "  - For each test point:\n",
        "    - Reset or update the model’s internal state.\n",
        "    - Generate predictions step-by-step for `n_predict_once` time steps.\n",
        "    - Compute the Mean Squared Error (MSE) between predicted values and the actual future stock prices.\n",
        "\n",
        "The code then defines the required parameters (such as window length, prediction horizon, and batch size), and compiles the model with an appropriate optimiser and loss function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyZN3zIV4aoF"
      },
      "outputs": [],
      "source": [
        "# Get the total number of time steps in the training dataset\n",
        "train_seq_length = train_data.size  # Full length of the training data (used for iteration/tracking)\n",
        "\n",
        "# Lists to store performance metrics and predictions across epochs\n",
        "train_mse_ot = []             # Accumulate mean squared error on training set after each epoch\n",
        "test_mse_ot = []              # Accumulate mean squared error on test set after each epoch\n",
        "predictions_over_time = []   # Store the rolling predictions made at fixed test locations\n",
        "\n",
        "n = 50 # Number of steps to predict\n",
        "\n",
        "# Learning rate scheduler control: reduce learning rate if validation error stagnates\n",
        "loss_nondecrease_threshold = 2  # Number of epochs to wait before reducing the learning rate\n",
        "\n",
        "# Running average of training loss (used to monitor convergence)\n",
        "average_loss = 0\n",
        "\n",
        "# Define the region in the time series to evaluate model performance (have a look at previous plots for an interesting section of the timeseries).\n",
        "test_start = 11000            # Starting index of the test window - you can choose any really.\n",
        "test_end = 12000              # Ending index of the test window, likewise this is something you can decide on.\n",
        "\n",
        "# Choose specific points at which to perform multi-step prediction\n",
        "# This allows us to simulate how the model behaves at different stages of the time series\n",
        "test_points_seq = np.arange(test_start, test_end, n).tolist()  # One test every n-steps\n",
        "\n",
        "# A function to generate horizon-length predictions repeatedly across the test range\n",
        "def rolling_forecasts(model,\n",
        "                      series_scaled,          # The full, scaled time series (used as model input)\n",
        "                      scaler,                 # The MinMaxScaler used to inverse-transform predictions\n",
        "                      test_start=11000,       # Start of the test region\n",
        "                      test_end=12000,         # End of the test region\n",
        "                      window_size=50,         # Number of past steps the model uses to make a prediction\n",
        "                      horizon=50):            # Number of future steps to predict at each test point\n",
        "\n",
        "    # Define test points spaced by `horizon` (e.g., every 50 steps)\n",
        "    test_points = np.arange(test_start, test_end, horizon)\n",
        "\n",
        "    # Prepare output containers: x values (time indices) and y values (predictions)\n",
        "    x_axis_seq, y_pred_seq = [], []\n",
        "\n",
        "    # Loop through each test point to generate a rolling forecast\n",
        "    for w in test_points:\n",
        "        # Take a slice of the past window_size points before this test point\n",
        "        seed = series_scaled[w - window_size : w].copy()\n",
        "\n",
        "        # Array to hold horizon predictions (in scaled form)\n",
        "        preds_scaled = np.empty(horizon)\n",
        "\n",
        "        # Predict forward one step at a time, feeding predictions back into the model\n",
        "        for t in range(horizon):\n",
        "            # Predict the next value using the current seed window\n",
        "            y_next = model.predict(seed.reshape(1, window_size, 1), verbose=0)[0, 0]\n",
        "\n",
        "            # Save the prediction\n",
        "            preds_scaled[t] = y_next\n",
        "\n",
        "            # Shift the seed window and insert the predicted value at the end\n",
        "            seed = np.roll(seed, -1)\n",
        "            seed[-1] = y_next\n",
        "\n",
        "        # Convert predictions back to original price scale using the inverse of the scaler\n",
        "        preds_unscaled = scaler.inverse_transform(preds_scaled.reshape(-1, 1)).ravel()\n",
        "\n",
        "        # Record both the x-axis positions and the predicted values\n",
        "        y_pred_seq.append(preds_unscaled)\n",
        "        x_axis_seq.append(np.arange(w, w + horizon))\n",
        "\n",
        "    # Return time indices and predictions\n",
        "    return x_axis_seq, y_pred_seq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have constructed our model and ensured we are feeding the model with the data in the correct structure, we can proceed to put it all together and train the model.\n",
        "\n",
        "We wrap everything into something called a TensorFlow dataset. This special format makes training more efficient. It allows us to group the data into batches (so the model learns from several sequences at once) and shuffle the order of training samples to help avoid overfitting. We also pre-load upcoming data behind the scenes, so the training loop runs smoothly without waiting for data to be prepared.\n",
        "\n",
        "We then compile the model. This means specifying how it should learn. We use the Adam optimiser that adjusts how the model updates itself as it learns. We also use Mean Squared Error (MSE) as the metric to judge how far off the model's predictions are from the actual stock prices. The smaller the MSE, the better the model is doing.\n",
        "\n",
        "To make learning even more efficient, we use a helpful technique called *ReduceLROnPlateau*. This watches the model's progress and, if the performance on the validation set stops improving, it gently lowers the learning rate. This makes the model fine-tune its behaviour more carefully as it nears the best solution, rather than taking big, disruptive steps.\n",
        "\n",
        "Finally, we begin the actual training. We feed the model batches of sequences for several epochs, letting it adjust and improve each time. After every epoch, we check how it performs not only on the training data but also on a separate test set to see if it's generalising well to unseen patterns. All of this is reorded, giving us a detailed view of how well the model is learning over time:"
      ],
      "metadata": {
        "id": "lBAAEYT0RpsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau  # callback to adjust learning rate when progress stalls\n",
        "\n",
        "# Set the number of training cycles (epochs)\n",
        "epochs = 15\n",
        "\n",
        "# Prepare training and testing sequences using the sliding window approach\n",
        "# Each input sequence has num_unrollings time steps, and the label is the next value in the sequence\n",
        "X_train, y_train = make_windows(train_data, num_unrollings)\n",
        "X_test,  y_test  = make_windows(test_data,  num_unrollings)\n",
        "\n",
        "# Convert training data into a TensorFlow Dataset pipeline\n",
        "train_dataset = (\n",
        "    tf.data.Dataset.from_tensor_slices((X_train, y_train))  # Wrap input-output pairs\n",
        "      .shuffle(buffer_size=10000)                           # Randomise order to prevent overfitting to sequence order\n",
        "      .batch(batch_size, drop_remainder=True)               # Group into batches; drop the last few if not divisible\n",
        "      .prefetch(tf.data.AUTOTUNE)                           # Optimise performance by overlapping data loading and training\n",
        ")\n",
        "\n",
        "# Prepare test data similarly, but without shuffling (since we're evaluating, not training)\n",
        "test_dataset = (\n",
        "    tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "      .batch(batch_size, drop_remainder=True)\n",
        "      .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "# Compile the model with:\n",
        "# Adam optimiser (adaptive learning rate)\n",
        "# Mean Squared Error (MSE) as both the loss function and metric\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='mse',\n",
        "    metrics=['mse']\n",
        ")\n",
        "\n",
        "# Create a callback to reduce the learning rate when the model stops improving\n",
        "# monitor='val_loss' watches the validation loss\n",
        "# If it doesn't improve for 2 consecutive epochs (patience=2), the learning rate is halved (factor=0.5).\n",
        "# But, it won't go below min_lr=1e-6\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=2,\n",
        "    verbose=1,\n",
        "    min_lr=1e-6\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,                  # Training data\n",
        "    epochs=epochs,                  # Number of passes through the data\n",
        "    validation_data=test_dataset,  # Evaluate model on test set after each epoch\n",
        "    callbacks=[reduce_lr],         # Adjust learning rate automatically if needed\n",
        "    verbose=1                       # Print progress after each epoch\n",
        ")"
      ],
      "metadata": {
        "id": "79ak17tnRnNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n",
        "The training results show that the LSTM model has learned to forecast stock price trends with a good level of accuracy. During the first epoch, the training loss started relatively high, but the model quickly began to adjust, and by the second epoch, the training error had fallen sharply. This indicates that the model was able to capture meaningful patterns in the time series early on.\n",
        "\n",
        "Over the course of 15 epochs, the training loss steadily decreased to below 0.001, and the validation loss followed a similar downward trend. The model consistently performed well on unseen test data, with validation mean squared error (MSE) remaining low - typically around 0.002 to 0.003 in the final few epochs. This suggests the model generalised well and wasn't simply memorising the training set.\n",
        "\n",
        "The learning rate scheduler (`ReduceLROnPlateau`) played a role in fine-tuning. As soon as progress on the validation set began to plateau, the scheduler reduced the learning rate, allowing the model to make smaller, more refined updates. This adaptiveness helped the model stabilise and avoid overfitting, especially in later epochs when large updates might have disrupted the gains already made.\n",
        "\n",
        "Overall, the model's learning curve was smooth and effective. With such low validation error on a normalised scale, it's clear that the network has captured useful temporal dynamics from the financial time series. Let's visualise these results to double check.\n",
        "\n",
        "The first plot shows how the model's error changed over each epoch for both the training and validation sets. This helps us judge how well the model learned and whether it overfit or underfit:"
      ],
      "metadata": {
        "id": "5jPG7GAp9jwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate rolling forecasts using the trained model\n",
        "# x_axis_seq: x-values for plotting each prediction window\n",
        "# predictions_over_time: list of predicted sequences (each of length = horizon)\n",
        "x_axis_seq, predictions_over_time = rolling_forecasts(\n",
        "    model,\n",
        "    series_scaled=all_mid_data_scaled,  # scaled full dataset\n",
        "    scaler=scaler,                      # fitted MinMaxScaler to invert predictions later\n",
        "    window_size=num_unrollings,        # how many past time steps the model uses\n",
        "    horizon=num_unrollings             # how many steps ahead we forecast each time\n",
        ")\n",
        "\n",
        "# Plot training and validation loss curves to visualise learning progress over epochs\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# Plot mean squared error on the training data\n",
        "plt.plot(history.history['loss'], label='Train MSE')\n",
        "\n",
        "# Plot mean squared error on the validation data\n",
        "plt.plot(history.history['val_loss'], label='Val MSE')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "zZ8smfpa9kKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second plot visualises the model’s final 50-step-ahead forecast (unscaled back to actual price range), offering a concrete look at what the model expects after observing the last window of real stock data:"
      ],
      "metadata": {
        "id": "KGFlTqGsjyM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the final multi-step forecast (last prediction made after training)\n",
        "final_forecast = predictions_over_time[-1]  # Get the most recent forecast (last in the list)\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# Plot the unscaled forecast for the next n time steps\n",
        "plt.plot(final_forecast)\n",
        "\n",
        "plt.title(\"50‐Day forecast (Unscaled) from final epoch\")\n",
        "plt.xlabel(\"Steps ahead\")\n",
        "\n",
        "plt.ylabel(\"Price\")"
      ],
      "metadata": {
        "id": "r0mWw-Mjjy5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction\n",
        "Now that our model is trained, we can explore its predictions by overlaying them on the original time series. Instead of focusing on exact values, we are more interested in the overall direction of the predicted trend, that is, whether prices are moving upwards or downwards over short horizons. This is often more practical than pinpointing the precise stock price, as it can guide strategic decisions such as whether to buy, hold, or sell.\n",
        "\n",
        "To do this, we plot the original mid-price series, and then gradually overlay the predicted sequences in red. Each red line represents the model's forecast for the next n-days, based on what it had seen up to that point. By using a fading effect (increasing line transparency), we can visualise how predictions evolve over time: older forecasts appear faint, while newer predictions are shown more boldly.\n",
        "\n",
        "This gives us a view of how the model perceives the trend at different points in the series. For example, consistent downward sloping forecasts might signal a bearish phase, suggesting it's not the best time to invest. Likewise, if the forecast tilts upward, it might indicate the start of a bullish trend:"
      ],
      "metadata": {
        "id": "2UXphCcbcMnm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiOT-ElE4aoG"
      },
      "outputs": [],
      "source": [
        "# Plot the true mid-price series (original smoothed and scaled back data)\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "plt.plot(range(df.shape[0]), all_mid_data, color='b', lw=1)\n",
        "\n",
        "# Set the starting transparency (alpha) value for the oldest predictions\n",
        "start_alpha = 0.25\n",
        "\n",
        "# Count how many prediction windows we generated from rolling forecasts\n",
        "n_plots = len(x_axis_seq)\n",
        "\n",
        "# Create a smooth range of alpha values from faint (older) to opaque (recent)\n",
        "alphas = np.linspace(start_alpha, 1.0, n_plots) # Define the section of the time series we want to zoom in on for visualisation\n",
        "\n",
        "# Overlay each forecast (xs = time steps, ys = predicted prices) on top of the true series\n",
        "# Older forecasts appear more transparent; newer ones are bolder\n",
        "for a, (xs, ys) in zip(alphas, zip(x_axis_seq, predictions_over_time)):\n",
        "    plt.plot(xs, ys, color='r', alpha=a)  # plot each n-step prediction in red\n",
        "\n",
        "plt.title('Rolling n-step LSTM forecasts')\n",
        "\n",
        "plt.xlabel('Time (t)')\n",
        "plt.ylabel('Mid-price ($)')\n",
        "\n",
        "# Restrict the x-axis to the test range so we can zoom into the forecast period\n",
        "start_test = 10700\n",
        "end_test = 12200\n",
        "plt.xlim(start_test, end_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the LSTM did not do better than the standard averaging.\n",
        "We initialised the testing window with test data, predict the next point over that and create a new window with the next point. However, once we reach a point where the input window is fully composed of past predictions it terminates, moves forward one full window length, and resets the window with the true test data. We then start the whole process again.\n",
        "\n",
        "This produces multiple trend-line like predictions over the test data, which we can visually inspect to see how well the model identified future trends.\n",
        "\n",
        "We can see from the predicted trend lines that the network did appear to correctly predict the trends (and amplitude of trends) for a good portion of the time series.\n",
        "\n",
        "Although not necessarily perfect, it does provide an example of how useful LSTM deep neural networks can be when applied to time series problems. If we were to careful tune the hyperparameters through experimentation, then we could certainly achieve better results.  I leave this to you as a challenge!"
      ],
      "metadata": {
        "id": "fqxuVssISvgM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VvmgBLI4aoG"
      },
      "source": [
        "### What have we learnt?\n",
        "\n",
        "Predicting stock price movements using machine learning is a challenging and often uncertain task. While our LSTM model shows that it can capture broad trends and short-term fluctuations to some extent, it is far from a crystal ball. Financial markets are influenced by countless external factors, including news, policy, and sentiment, which are not directly visible in the historical prices used by our model.\n",
        "\n",
        "In our project, we observed that the LSTM was able to learn temporal patterns and generate reasonable multi-step forecasts. However, the accuracy of these forecasts depends heavily on the *hyperparameters* chosen. Key parameters like the learning rate, the number of LSTM layers, and the number of hidden units can affect the model's performance. Optimisers like Adam are commonly used and generally effective, but experimenting with different options is often worthwhile. Approaches such as *grid search* or *random search* can be helpful to systematically explore combinations of hyperparameters.\n",
        "\n",
        "It's also worth remembering that we scaled our price data between 0 and 1, so predictions were not in absolute currency units. That's okay, what matters more is the shape and direction of the trend: whether the model anticipates an upward or downward movement, rather than the precise numerical value. This kind of trend forecasting can support investment decisions even without pinpointing exact prices.\n",
        "\n",
        "However, LSTMs are not a panacea. One limitation is that they struggle with the non-stationary nature of financial data—market dynamics shift over time in ways that break assumptions made by many models. Emerging methods such as Bayesian deep learning offer more robust treatment of uncertainty in non-stationary environments. Likewise, newer architectures based on attention mechanisms, such as Transformers, have recently outperformed traditional RNNs like LSTM in many sequential tasks.\n",
        "\n",
        "In short, LSTMs provide a useful entry point for time series forecasting and can reveal meaningful structure in noisy data. But more sophisticated models and evaluation strategies are essential if we want to approach the complexity of real-world financial forecasting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4ehJigq4aoG"
      },
      "source": [
        "### Next steps\n",
        "We have also provided some additional stock market data for you to test, have a look in the <a href=\"https://github.com/martyn-harris-bbk/AppliedMachineLearning/tree/main/data\" target=\"_blank\">./data/</a> folder for more financial data sets (`iam.us.txt`, `bbc.us.txt`, and `aamc.us.txt`).  You could also download a non-finance dataset, for instance, a timeseries recording temperature or rainfall and attempt to model it.  You will need to make changes to the sliding windows and test data ranges as you may have more, or less data to work with than this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xI_YFApYMFAl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}