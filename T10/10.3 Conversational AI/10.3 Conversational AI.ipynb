{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational AI\n",
    "### Introduction\n",
    "Chatbots are computer programs designed to simulate human conversation, typically through a natural language interface. They are widely used to provide instant responses to routine queries and are often deployed in customer-facing roles such as website help desks, online banking support, or mobile apps. One of their major advantages is that they operate continuously 24 hours a day, 7 days a week, meaning users can receive support at any time without waiting for a human operator.\n",
    "\n",
    "From a business perspective, chatbots help reduce the burden on human support teams by handling frequently asked questions and directing users to relevant resources. This allows customer service staff to focus their attention on more complex cases that require empathy, discretion, or nuanced understanding. Studies have also shown that, if designed effectively, chatbots can enhance customer satisfaction and loyalty, particularly when users feel a sense of personality, or brand alignment in the chatbot's tone and behaviour.\n",
    "\n",
    "Although the terms *chatbot* and *conversational AI* are often used interchangeably, they refer to distinct approaches. The difference lies primarily in the level of sophistication and adaptability of the system:\n",
    "\n",
    "- *Chatbots* in their traditional form, are rule-based systems. They rely on a series of predefined decision trees or \"if-this-then-that\" logic to guide their responses. These rules are hand-coded by developers and typically require frequent manual updates to handle new queries or refine their performance. You'll often see this type of chatbot appear on websites as a pop-up interface, prompting visitors to ask questions, or choose from a limited set of options.\n",
    "\n",
    "- *Conversational AI* on the other hand, refers to systems that use advanced natural language processing (NLP) and machine learning (ML) to understand and respond to user input. Rather than matching exact keywords to pre-written responses, conversational AI can interpret meaning from phrasing, context, and user intent. This makes the system more flexible and capable of responding to questions phrased in different ways, including variations it may not have seen before.\n",
    "\n",
    "One of the key strengths of conversational AI is that we can improve it over time through exposure to real world queries from users to create more training data. When the system fails to correctly identify a user's request, that interaction can be logged and reviewed. Annotating these cases with the correct *intent* allows the model to learn from its mistakes and update its understanding without reprogramming the entire logic manually. This data-driven approach makes conversational AI systems much more scalable and adaptable in dynamic real-world settings.\n",
    "\n",
    "In this practical, we will explore the creation of a simple chatbot demonstrating how a basic conversational AI model can be trained to understand and respond to user questions. Rather than using hand-coded rules, we will develop a small neural network that learns patterns from training data consisting of questions and their corresponding intents.\n",
    "\n",
    "As a creative example, we will train the model using phrases that a user might pose to *Jane Austen*, a renowned British novelist, about her life, works, and legacy. This will showcase how conversational AI can be used in educational or cultural settings, beyond commercial applications.\n",
    "\n",
    "For comparison, we will also provide a second dataset representing a more traditional use case: a business-oriented chatbot designed to handle enquiries related to services, pricing, and support. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset\n",
    "\n",
    "The dataset defines the types of questions or prompts the model can recognise and respond to. The training data is composed of two elements: *patterns* and *intents*:\n",
    "\n",
    "- *Patterns* are example phrases or sentences that a user might say. These are written in natural language and represent the many ways a person could express a given request - even the same request with different vocabulary.\n",
    "- Each pattern is linked to a specific *intent*, which is a label that categorises what the user is trying to achieve, such as greeting the assistant, asking a question, or making a request.\n",
    "\n",
    "For this practical, we have created two custom datasets. We'll start by using the *Jane Austen dataset* (`austen_intents.json`) as our default. This data is currently used to train a neural network intended for use in an immersive exhibition, where the responses and 3D visuals will be projected via a holographic display. It provides a creative, cultural example based on the life of *Jane Austen*, which was hand-constructed using factual information gathered from public sources such as Wikipedia. This dataset is intended to simulate the kinds of questions one might pose to a virtual assistant portraying the author, for example, as part of an educational, or museum installation.\n",
    "\n",
    "We have also included a more typical business-oriented dataset (`business_intents.json`). This simulates a chatbot that might be deployed by a company to automate responses to high-volume enquiries. Many large organisations already use such systems to help users find answers quickly without involving a human agent. This is a hand-curated dataset built around common user enquiries such as opening hours, payment methods, or account issues.  \n",
    "\n",
    "Here is a sample of the structure of the training data we have, written in JSON format:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"intents\": [\n",
    "    {\n",
    "      \"tag\": \"greeting\",\n",
    "      \"patterns\": [\"Hi\", \"Is anyone there?\", \"Hello\", \"Good day\"],\n",
    "      \"responses\": [\"Hello, thanks for visiting\", \"Hi there, how can I help?\"]\n",
    "    },\n",
    "    {\n",
    "      \"tag\": \"thanks\",\n",
    "      \"patterns\": [\"Thanks\", \"Thank you\", \"That's helpful\"],\n",
    "      \"responses\": [\"Happy to help!\", \"Any time!\", \"My pleasure\"]\n",
    "    },\n",
    "    {\n",
    "      \"tag\": \"bot_or_not\",\n",
    "      \"patterns\": [\"Is this a bot?\", \"Are you a machine?\", \"Am I talking to a person?\"],\n",
    "      \"responses\": [\"That's for me to know, and you to find out.\", \"I'd really rather not say.\"]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "Each intent has a `tag` (its name), a list of `patterns` (word input examples), and a set of `responses` (pre-written replies the bot/we can choose from). When the model is trained on this data, it learns to associate different phrases with their respective intent labels. During inference, when a user types a message, the model attempts to classify it into one of these known intents and then selects an appropriate response. To add some variety to the responses we provide several possible responses in a list, we can then randomly select one of them to present to the user. This makes the chatbot come across as smarter than it actually is -  a simple trick we can employ with our chatbots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install tensorflow numpy --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To switch between datasets, you can simply comment or uncomment the relevant filename in the code below that loads the data, allowing you to experiment with both applications of conversational AI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# To run the Jane Austen conversation AI use this file:\n",
    "intent_file = 'austin_intents.json'\n",
    "\n",
    "# To run the business version uncomment this line:\n",
    "\n",
    "# intent_file = 'business_intents.json'\n",
    "\n",
    "with open('./data/' + intent_file) as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "print(intents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "We start by gathering all the sentences (patterns) from the dataset and converting them to lower case. This normalisation step ensures that words like \"Hello\" and \"hello\" are treated the same. We also remove punctuation such as question marks, which can introduce unnecessary variation in how the model sees different patterns.\n",
    "\n",
    "As we go through the data, we also compile a list of all the individual words used across the entire dataset. This list forms the model's vocabulary, which is the set of known words it can recognise when making predictions. This vocabulary defines the \"language\" of our conversational AI and therefore its extent limits the words it will be able to understand and classify.\n",
    "\n",
    "Each word will be converted into a numerical format, such as a bag-of-words vector, so that it can be fed into a neural network. The same goes for the intent labels, which are transformed into numerical class indices for classification.\n",
    "\n",
    "Our preprocessing step extracts and cleans the phrases and intents from the dataset, prepares the input through tokenisation, and the output for training. We have then built up a complete vocabulary that defines the scope of the assistant's knowledge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "ignore_words = [\"?\", \"the\", \"and\" \"a\"]\n",
    "\n",
    "# Define our own custom tokeniser for simplicity\n",
    "def simple_tokenise(text):\n",
    "    \"\"\"\n",
    "    Tokenise a sentence into words. Very simple, but you can expand on this.\n",
    "    Removes punctuation (except apostrophes) and splits on whitespace.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Keep apostrophes inside words (e.g. \"don't\") but remove other punctuation\n",
    "    text = re.sub(r\"[^\\w\\s']\", '', text)\n",
    "    return text.split()\n",
    "\n",
    "words = [] # The full vocabulary of all words across patterns.\n",
    "documents = [] # A list of (tokenised_pattern, intent) pairs.\n",
    "classes = []   # A list of all unique intent labels.\n",
    "\n",
    "# 'intents' is our loaded JSON dictionary\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenise\n",
    "        w = simple_tokenise(pattern)\n",
    "        # Add to our vocabulary\n",
    "        words.extend(w)\n",
    "\n",
    "        # add to documents in our corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "# Lower case each word and remove duplicates\n",
    "words = [w.lower() for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "# Remove duplicates\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"intent classes\", classes)\n",
    "\n",
    "# All words in our vocabulary\n",
    "print (len(words), \"unique stemmed words\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that our approach is *domain-specific*. The chatbot will only be able to understand and respond to questions that fall within the scope of the topics it has been trained on. For example, a chatbot trained to answer questions about Jane Austen will not know how to respond to questions about weather forecasts or sports scores. General-purpose conversational agents, like Siri or Alexa, require vastly larger datasets and more complex models to handle the wide variety of topics and language variations they encounter in real-world use.\n",
    "\n",
    "Before we can train our neural network, we convert our raw input, the user phrases and their corresponding intents, into a format the model can understand. This involves transforming each sentence into a *bag of words* vector, and each intent label into a *one-hot encoded* target vector.\n",
    "\n",
    "The bag of words approach works here by checking which known words from our vocabulary appear in a given sentence. For each word in the vocabulary, we place a `1` in the vector if the word is present in the sentence, and a `0` if it is not. This creates a fixed-length binary vector for every training pattern, where the position of each element corresponds to a specific word in the vocabulary.\n",
    "\n",
    "Alongside this, we create a one-hot vector for the *output class*. This is a list of zeros the same length as the number of intent labels, with a `1` in the position corresponding to the correct intent for that sentence. Together, the input vectors and their matching output vectors form our full training dataset.\n",
    "\n",
    "Once this has been done for all training examples, we shuffle the dataset to remove any unintended ordering effects and then separate the data into two arrays containing the input vectors (`train_X`), and one containing the corresponding output labels (`train_Y`). These are then converted to arrays, which are the standard format expected by most deep learning frameworks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "training = []  # This will hold our full training dataset\n",
    "output_empty = [0] * len(classes)  # A zeroed template for one-hot encoding output labels\n",
    "\n",
    "# Loop through each training example\n",
    "for doc in documents:\n",
    "    bag = []  # Initialise an empty bag-of-words vector for this sentence\n",
    "    pattern_words = doc[0]  # Get the tokenised words from the current pattern\n",
    "\n",
    "    # For each word in the vocabulary, set 1 if it appears in the pattern, else 0\n",
    "    for w in words:\n",
    "        bag.append(1 if w in pattern_words else 0)\n",
    "\n",
    "    # Create a one-hot encoded vector for the intent (output label)\n",
    "    output_row = list(output_empty)  # Start with all zeros\n",
    "    output_row[classes.index(doc[1])] = 1  # Set a 1 at the index of the correct intent\n",
    "\n",
    "    # Add the (input_vector, output_vector) pair to the training set\n",
    "    training.append((bag, output_row))\n",
    "\n",
    "# Randomly shuffle the training set to prevent bias during training\n",
    "random.shuffle(training)\n",
    "\n",
    "# Split the data into input (X) and output (Y) sets\n",
    "train_X = [item[0] for item in training]  # Feature vectors (bag-of-words)\n",
    "train_Y = [item[1] for item in training]  # One-hot intent labels\n",
    "\n",
    "# Convert lists to NumPy arrays (required by many ML libraries)\n",
    "train_X = np.array(train_X)\n",
    "train_Y = np.array(train_Y)\n",
    "\n",
    "# Shape of our data\n",
    "print(len(train_X), len(train_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model\n",
    "The model we are building is a feedforward neural network (also known as a multilayer perceptron), which is well-suited to basic classification tasks such as identifying user intents from short phrases. It consists of an input layer, a couple of hidden layers, and an output layer.\n",
    "\n",
    "Each input is a bag-of-words vector representing the words found in the user's question. The output is a probability distribution across the possible intent labels, calculated using a softmax activation function, which allows the model to choose the most likely intent for each input sentence.\n",
    "\n",
    "We will use TensorFlow's `Sequential` model, then specify how many units each layer has, what activation functions to use (in this case, *ReLU* for the hidden layers and *softmax* for the output), and how the model should learn (using categorical cross-entropy loss and the Adam optimiser). We also inspect a summary of the model's shape and parameters, which helps us verify that it has been set up correctly.\n",
    "\n",
    "This simple network is a solid baseline for intent classification and can be extended later with more layers, different activation functions, or other improvements such as dropout for regularisation if we need them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer with 8 hidden units\n",
    "model.add(Dense(8, input_shape=(len(train_X[0]),), activation='relu'))\n",
    "\n",
    "# Second hidden layer with 8 units\n",
    "model.add(Dense(8, activation='relu'))\n",
    "\n",
    "# Output layer with softmax activation for classification\n",
    "model.add(Dense(len(train_Y[0]), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Once the model has been defined, the next step is to train it on our prepared dataset. Training involves feeding the model examples (input vectors and their corresponding intent labels) and allowing it to adjust its internal weights to minimise error.\n",
    "\n",
    "In this example, we specify the number of epochs and the batch size (how many examples are processed at once before updating the model). We also set `verbose=1` to display the progress of training in the console, showing the loss and accuracy after each epoch.\n",
    "\n",
    "After training is complete, we save the model to a file (`model.keras`). This allows us to reuse the trained model later. For example, when deploying it in a chatbot application, without having to retrain it each time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "# Fit the model and store training history\n",
    "history = model.fit(train_X, train_Y,\n",
    "                    epochs=num_epochs,\n",
    "                    batch_size=8,\n",
    "                    verbose=1)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "We have not created test data in this practical so we can only evaluate how training went. Initially, the model began with a moderate accuracy of around 64% and a loss slightly above 1.10. This suggests it had already learned some basic patterns in the data, likely due to good model initialisation or helpful pre-processing. Over the first 20 epochs, we observe a consistent decline in the training loss, paired with a corresponding increase in accuracy, reaching above 75% by epoch 25. This early phase indicates that the model was effectively learning the underlying structure of the problem and adjusting its parameters meaningfully.\n",
    "\n",
    "As training continued through epochs 30 to 60, the model sustained this positive trend. Accuracy frequently rose above 85% and eventually passed 90%, with loss steadily decreasing to values well below 0.5. This middle period reflects a strong learning phase, where the model refined its decision boundaries and reduced its prediction errors. Around epochs 40–60, there were some fluctuations in loss and accuracy, which is expected in longer training cycles, possibly due to the optimiser navigating more complex regions of the parameter space or slight overfitting on certain mini-batches.\n",
    "\n",
    "In the final third of training, from epoch 70 onward, the model demonstrated impressive consistency and performance. Accuracy peaked at nearly 99% (epoch 88 and 99), and loss dropped as low as 0.1274 by the end. Such performance on training data suggests that the model has converged successfully and has very likely learned to generalise the training patterns with high precision. However, without a validation or test set, it's difficult to determine whether the model is overfitting. Still, the steady improvement and stable accuracy in the later epochs are strong indicators that the model is behaving well and would likely perform effectively on unseen data.\n",
    "\n",
    "We can now visualise how the model performed during training by plotting our two key metrics over time; accuracy and loss. The plot shows how the model's accuracy improved over each training epoch. This gives an indication of how well the model is learning to classify the correct intents. The training loss reflects how far off the model's predictions were from the correct answers. Ideally, this should decrease over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training accuracy and loss over epochs\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric')\n",
    "plt.title('Training Metrics Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "The final stage is to make predictions based on the natural language of the user. We first define some helper functions, which will convert the users sentences into a bag of words represented by a vector of 0's and 1s showing the presence of the word (or not) in out vocabulary:, then we can predict the intent and select a suitable response at random:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Preprocess the input sentence by tokenising it\n",
    "def clean_up_sentence(sentence):\n",
    "    return simple_tokenise(sentence)  # Custom tokeniser function used earlier in our pipeline\n",
    "\n",
    "# Convert a tokenised sentence into a bag-of-words (BoW) vector\n",
    "# This creates a binary vector where each position corresponds to a known word:\n",
    "# 1 if the word is present in the input sentence, 0 otherwise\n",
    "def bow(sentence, words):\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    bag = [1 if w in sentence_words else 0 for w in words]\n",
    "    return np.array(bag)\n",
    "\n",
    "# Predict the class label for an input sentence and return a matching response\n",
    "def predict(sentence):\n",
    "    # Convert the input sentence into a bag-of-words vector\n",
    "    word_vec = bow(sentence, words)\n",
    "\n",
    "    # Run the vector through the trained model to obtain class probabilities\n",
    "    pred_y = model.predict(np.array([word_vec]))[0]  # Predict returns a batch, take first element\n",
    "\n",
    "    # Find the class with the highest predicted probability\n",
    "    high_prob = max(pred_y)\n",
    "    idx = np.argmax(pred_y)\n",
    "    intent_label = classes[idx]  # Get the corresponding intent tag from the class list\n",
    "\n",
    "    # Search for the corresponding intent and pick a response at random\n",
    "    for intent in intents['intents']:\n",
    "        if intent['tag'] == intent_label:\n",
    "            answer = random.choice(intent['responses'])\n",
    "\n",
    "            # Print out the prediction and response\n",
    "            print(\">>\", sentence)\n",
    "            print(\"Predicted class:\", intent_label)\n",
    "            print(\"Response:\", answer)\n",
    "            print()\n",
    "            return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jane Austen\n",
    "\n",
    "This example assumes you trained the model on the ```austin_intents.json``` file. If you have trained the model on the ```business_intents.json``` file then you will see some interesting responses, which will demonstrate how conversational AIs fail when posed a question that they have not been designed or trained for.  In any case, let's have a conversaton with Jane Austen...:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"Hi there!\")\n",
    "predict(\"Do I know you?\")\n",
    "predict(\"Are you famous?\")\n",
    "predict(\"when were you born?\")\n",
    "predict(\"Am I talking to a robot?\")\n",
    "predict(\"Well, good bye then...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer query\n",
    "\n",
    "This example assumes you trained the model on the ```business_intents.json``` file.  If you used the ```austin_intents.json```, then as mentioned you might see some bizarre responses.  Otherwise, this might be how a customer interacts with a chat bot designed to address common customer queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"Hi\")\n",
    "predict(\"When do you open?\")\n",
    "predict(\"Can I pay by card?\")\n",
    "predict(\"Am I talking to a robot?\")\n",
    "predict(\"I'd like to close my account.\")\n",
    "predict(\"Well, good bye then...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What have we learnt?\n",
    "\n",
    "Now that you've seen how to run the chatbot using two example intent files, one creative and one business-focused, you're encouraged to experiment further by creating your own version. You can do this by editing or replacing the intent file with your own set of *patterns*, *intent labels*, and *responses*, tailored to a topic or context that interests you.\n",
    "\n",
    "This kind of intent-based chatbot is flexible and adaptable. In a real-world scenario, you might build a training dataset using real email exchanges, support tickets, or help desk queries. Doing so would allow you to train a chatbot that can automatically handle routine customer service interactions, saving time and resources while improving response times.\n",
    "\n",
    "Interestingly, this technology also raises questions that go beyond simple automation. For example, Microsoft has filed a <a href=\"https://pdfpiw.uspto.gov/.piw?PageNum=0&docid=10853717&IDKey=&HomeUrl=%2F\" target=\"_blank\">patent</a> for a chatbot that can simulate the personality and responses of a real person, living or deceased, based on their digital footprint. This opens the door to new and thought-provoking uses of conversational AI, from virtual memorials to personalised education, but also invites ethical questions about identity, consent, and digital legacy.\n",
    "\n",
    "So if you're building a practical assistant or a creative prototype, this practical shows how powerful and accessible conversational AI has become. If you wanted to create your own chatbot, you can simply replace the dataset with your own and it does not have to be that big to be functional as you have seen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
