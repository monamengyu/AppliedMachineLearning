{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efc9a777",
   "metadata": {
    "id": "efc9a777"
   },
   "source": [
    "## Activation functions: vanishing gradient problem\n",
    "\n",
    "### Introduction\n",
    "Deep learning models are powerful, but they sometimes encounter issues during training. One such issue is the *vanishing gradient problem*, where the gradients of earlier layers become extremely small, making it difficult for the network to learn effectively. This means the earlier layers of a deep network learn very slowly or stop learning completely.\n",
    "\n",
    "As you explore Deep Learning, you will come across terms such as *sigmoid*, *tanh*, and *ReLU*. These are what we call *activation functions* that determine the value of the output in each layer of a neural network. For example, predicting a 1 or 0 based on the presence of heart disease.\n",
    "\n",
    "Let's spend some time understanding what the vanishing gradient problem is, why it happens in deep neural networks, learn how it affects training, and explore different solutions and how they help. We will also explain how *batch normalisation* and *residual connections* work, and why these techniques can help. You will then have a clearer understanding of this fundamental issue and be equipped to fix it if it occurs.\n",
    "\n",
    "One of the learning outcomes is for you to understand why deep networks struggle to learn, how to detect the problem, and what techniques you can use to fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cae340",
   "metadata": {
    "id": "98cae340"
   },
   "source": [
    "### What are gradients?\n",
    "Before discussing the vanishing gradient problem, we need to understand what gradients are.\n",
    "\n",
    "- Neural networks learn using gradients, which indicate how much to adjust each weight to reduce the error (or loss).\n",
    "- This is done using *backpropagation*, a process that calculates the gradient at each layer of the network.\n",
    "- The gradient is a measure of how a small change in a weight affects the loss function.\n",
    "\n",
    "The key idea is that if the gradients become too small, the updates to the weights also become small, meaning the model struggles to learn effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6a50a9",
   "metadata": {
    "id": "7b6a50a9"
   },
   "source": [
    "#### Why do gradients vanish?\n",
    "\n",
    "When training deep neural networks, we use a method called backpropagation to adjust the weights in the network. This involves calculating how much each weight contributes to the final error, known as the *gradient*. But in deep networks, these gradients can become so small that they effectively disappear, this is called the *vanishing gradient* problem. This occurs for several reasons:\n",
    "\n",
    "- *Activation saturation*: Functions like *sigmoid* and *tanh* squash input values into a narrow range. When the input is very large or very small, these functions become almost flat, which means their gradient (slope) is close to zero. This makes it hard for the network to learn.\n",
    "\n",
    "- *Repeated multiplication*: During backpropagation, each layer multiplies the gradient from the layer after it. If each of these numbers is less than one, their repeated multiplication quickly shrinks the gradient to a tiny value, often close to zero.\n",
    "\n",
    "- *Poor weight initialisation*: If the weights are badly chosen at the start (too large or too small), activations may land straight in those flat areas of the activation function, causing the gradients to vanish early in training.\n",
    "\n",
    "When gradients vanish, the earlier layers of the network receive little or no useful information about how to improve. As a result, they stop learning and the deeper the network, the more serious the problem.\n",
    "\n",
    "To address this, modern networks often use alternative activation functions (such as *ReLU*) and improved weight initialisation techniques to help keep gradients flowing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97f7dce",
   "metadata": {},
   "source": [
    "### Install Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac5711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow torch numpy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c5a682",
   "metadata": {
    "id": "c5c5a682"
   },
   "source": [
    "### Plotting Activation functions and their derivatives\n",
    "Let's explore why certain activations (like sigmoid) are more prone to vanishing gradients, while others (like ReLU) avoid it.\n",
    "\n",
    "Before we start, a *derivative* represents the slope or steepness of an activation function, it tells us how much the output of the function changes when the input changes slightly. During *backpropagation*, these derivatives are used to calculate how much each *weight* in the network should be adjusted.\n",
    "\n",
    "If the derivative is very small (close to zero), it means that, as we said, changes in the input barely affect the output so the weight update is tiny.\n",
    "\n",
    "When this happens across many layers, the effect compounds, and the *gradient* becomes so small that earlier layers stop learning effectively. Now, different *activation functions* behave differently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-If5MohyKZcE",
   "metadata": {
    "id": "-If5MohyKZcE"
   },
   "source": [
    "\n",
    "### Sigmoid\n",
    "Sigmoid outputs values between 0 and 1, but it flattens out quickly for large positive or negative inputs. In those flat regions, the derivative becomes very small, which is why sigmoid is prone to vanishing gradients:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0Zwk4PbdIDxM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1747239706690,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "0Zwk4PbdIDxM",
    "outputId": "8e44380f-1865-4149-db3b-6e46ed83d540"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the derivative of the sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Generate 200 evenly spaced input values between -5 and 5\n",
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "# Compute the sigmoid values and their derivatives\n",
    "y_sigmoid = sigmoid(x)\n",
    "y_derivative = sigmoid_derivative(x)\n",
    "\n",
    "# Create a figure with two side-by-side subplots\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Plot the sigmoid function\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, y_sigmoid, label='Sigmoid', color='blue', linewidth=2)\n",
    "plt.title('Sigmoid function')                  \n",
    "plt.xlabel('Input value (x)')                  \n",
    "plt.ylabel('Output')                           \n",
    "plt.legend()                                   \n",
    "\n",
    "# Plot the derivative of the sigmoid function\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, y_derivative, label='Sigmoid Derivative', color='red', linestyle='--', linewidth=2)\n",
    "plt.title('Sigmoid Derivative')                \n",
    "plt.xlabel('Input value (x)')                  \n",
    "plt.ylabel('Gradient')                         \n",
    "plt.legend()                                   \n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T9P2LYVhKqvU",
   "metadata": {
    "id": "T9P2LYVhKqvU"
   },
   "source": [
    "The *sigmoid* function squashes any input into a value between 0 and 1. It has an *S-shape* — very flat at the far left and right, and steepest in the middle around 0. The *derivative* of the sigmoid tells us how much the output changes for a small change in input. This is used during learning to update weights. In the plot, you can see that the derivative is:\n",
    "  - Highest at the centre (around input = 0)\n",
    "  - Very small (close to zero) at the edges.\n",
    "\n",
    "This matters because during training, if most of the inputs to sigmoid fall into those flat edge areas, the derivative is tiny and that leads to *vanishing gradients*. This means the network struggles to update weights in those regions, especially in early layers of deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y9Ie3gH8K9Ys",
   "metadata": {
    "id": "y9Ie3gH8K9Ys"
   },
   "source": [
    "### TanH\n",
    "*Tanh* is similar in shape to *sigmoid*, but instead of squashing values into a range between 0 and 1, it outputs values between -1 and 1. This makes it *zero-centred*, which can help with optimisation, since it balances positive and negative values more naturally during training. \n",
    "\n",
    "However, like sigmoid, *tanh* still becomes flat at its extremes, when the input is very positive or very negative, the output levels off and the *derivative* becomes very small. As a result, the gradients passed back through these regions can still vanish. \n",
    "\n",
    "That said, because *tanh* has a steeper slope around zero and covers a wider range, it tends to suffer *slightly less* from the vanishing gradient problem compared to *sigmoid*. Nonetheless, it's still not ideal for very deep networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cRvyJDKFLE4M",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 297,
     "status": "ok",
     "timestamp": 1747239706990,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "cRvyJDKFLE4M",
    "outputId": "94e7e957-3cc3-4bb4-f430-7e1721d1da1c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the tanh activation function\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Define the derivative of the tanh function\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "# Generate 200 evenly spaced values from -5 to 5\n",
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "# Calculate the tanh function and its derivative for each x\n",
    "y_tanh = tanh(x)\n",
    "y_deriv = tanh_derivative(x)\n",
    "\n",
    "# Create a figure with two side-by-side plots\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Plot the tanh function\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, y_tanh, label='Tanh', color='blue', linewidth=2)\n",
    "\n",
    "plt.title('Tanh function')                  \n",
    "\n",
    "plt.xlabel('Input value (x)')               \n",
    "plt.ylabel('Output')                        \n",
    "\n",
    "plt.legend()                                \n",
    "\n",
    "# Plot the derivative of tanh\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, y_deriv, label='Tanh Derivative', color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "plt.title('Tanh Derivative')                \n",
    "plt.xlabel('Input value (x)')               \n",
    "plt.ylabel('Gradient')                      \n",
    "\n",
    "plt.legend()                                \n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xzc7-mciLbfU",
   "metadata": {
    "id": "Xzc7-mciLbfU"
   },
   "source": [
    "In the left plot, the solid curve shows the *tanh* activation function. It has an *S-shape*, similar to *sigmoid*, but instead of going from 0 to 1, it goes from -1 to 1. That means its output is *zero-centred*, which helps the network handle both positive and negative values more symmetrically.\n",
    "\n",
    "The dashed curve (right plot) shows the *derivative* of *tanh*. This tells us how much the function output changes when the input changes slightly, and it's this gradient that’s used during training to adjust the weights. From the plots, you can see:\n",
    "\n",
    "- The *tanh* function is steepest in the middle (around input = 0). This is where the gradient is largest, which means learning happens more effectively here.\n",
    "- As you move away from zero in either direction (large positive or negative inputs), the *tanh* curve flattens out, approaching -1 or 1.\n",
    "- In these flat outer regions, the *derivative* drops sharply toward zero, the dashed line gets very close to the horizontal axis.\n",
    "\n",
    "This means if your network’s activations fall into these extreme zones, the gradients become tiny. *Tanh* has this issue, but it tends to be slightly better than *sigmoid* because:\n",
    "- It’s centred around zero (which helps with weight updates)\n",
    "- It has a steeper slope near the origin (so the central gradient is stronger)\n",
    "\n",
    "But like *sigmoid*, it's not ideal for very deep networks, which is why *ReLU* and its variants are often preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noVji022IE-1",
   "metadata": {
    "id": "noVji022IE-1"
   },
   "source": [
    "### ReLU\n",
    "*ReLU* (Rectified Linear Unit) is one of the most widely used activation functions in deep learning. It’s very simple: for any input less than zero, the output is zero; for any input greater than zero, the output increases linearly. So the function looks like a flat line for negative values and a diagonal line for positives. This simplicity is actually its strength. The *derivative* of ReLU is:\n",
    "\n",
    "- 0 when the input is negative (because the function is flat there),\n",
    "- 1 when the input is positive (since the function just increases directly with input).\n",
    "\n",
    "This has two big advantages. There’s no *flattening* in the positive range unlike *sigmoid* and *tanh*, the gradient doesn’t shrink as the input grows. That means gradients stay large enough during backpropagation, so weights in early layers can continue updating effectively.\n",
    "\n",
    "However, there's a small drawback too. For negative inputs, the derivative is zero, so if a neuron's input is always negative, it may \"die\" (i.e. stop learning). Despite this, *ReLU* is still highly effective and forms the default choice in most modern deep networks. If you have heard of variants of ReLU, like *Leaky ReLU*, these essentially help address the “dying neuron” issue:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec00676",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 826,
     "status": "ok",
     "timestamp": 1747239707820,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "eec00676",
    "outputId": "411363b3-d922-4793-82a8-cc847cae280a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the ReLU activation function: outputs 0 for negatives, x for positives\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Define the derivative of ReLU: 0 for x ≤ 0, 1 for x > 0\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "# Generate 200 evenly spaced input values from -5 to 5\n",
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "# Calculate the ReLU values and their derivatives for these inputs\n",
    "y_relu = relu(x)\n",
    "y_deriv = relu_derivative(x)\n",
    "\n",
    "# Create a figure with two subplots side by side\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# First subplot: plot the ReLU function\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, y_relu, label='ReLU', color='blue', linewidth=2)\n",
    "\n",
    "plt.title('ReLU function')                   \n",
    "\n",
    "plt.xlabel('Input value (x)')               \n",
    "plt.ylabel('Output')                        \n",
    "\n",
    "plt.legend()                                \n",
    "\n",
    "# Second subplot: plot the derivative of the ReLU function\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plt.plot(x, y_deriv, label='ReLU Derivative', color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "plt.title('ReLU Derivative')                \n",
    "plt.xlabel('Input value (x)')               \n",
    "plt.ylabel('Gradient')                      \n",
    "\n",
    "plt.legend()                                \n",
    "\n",
    "# Automatically adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NmmqH9SsNDxc",
   "metadata": {
    "id": "NmmqH9SsNDxc"
   },
   "source": [
    "In the plot above, you’ll see:\n",
    "\n",
    "The solid line shows the *ReLU* function: flat at 0 for negative inputs, and increasing linearly for positives. The dashed line shows the *derivative*:\n",
    "  - It's 0 for all negative inputs: no change, hence no gradient.\n",
    "  - It's 1 for all positive inputs: meaning the function is learning at full strength.\n",
    "\n",
    "This is a big contrast to *sigmoid* and *tanh*, where the derivative becomes very small at both ends. With *ReLU*, there's no risk of the gradient vanishing on the positive side so information flows better through the network, and training is usually faster and more stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2852ff4",
   "metadata": {
    "id": "b2852ff4"
   },
   "source": [
    "### MNIST dataset\n",
    "We will demonstrate how a deep network using *sigmoid* can suffer from vanishing gradients, while a *ReLU-based* network trains more effectively. The *MNIST dataset*, as we know, contains images of handwritten digits (`0` to `9`). Each image is 28×28 pixels, and the task is to correctly identify the digit in the image. We will construct and train each model and then perform a final comparison, let's load and preprocess the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad76f8b",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5mKDPNMHlZpR",
   "metadata": {
    "id": "5mKDPNMHlZpR"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load the MNIST dataset (handwritten digits 0–9)\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79d0846",
   "metadata": {},
   "source": [
    "#### Resampling\n",
    "We create a smaller and random subset from our full training and test sets so that subsequent experiments run faster. We draw 5,000 unique indices from the rows of `X_train` and 1,000 from `X_test` (without replacement, so no sample is picked twice). Finally, we use those index arrays to create corresponding feature matrices (`X_train_sample`, `X_test_sample`) and their matching label vectors (`Y_train`, `Y_test`). The result is a smaller, randomly selected train-test split that mirrors the distribution of the full dataset but trains and evaluates more quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34883554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take random subsets of the data for demonstration\n",
    "np.random.seed(7)\n",
    "\n",
    "train_sample_size = 5000\n",
    "test_sample_size  = 1000\n",
    "\n",
    "train_idxs = np.random.choice(X_train.shape[0], size=train_sample_size, replace=False)\n",
    "test_idxs  = np.random.choice(X_test.shape[0],  size=test_sample_size,  replace=False)\n",
    "\n",
    "X_train = X_train[train_idxs]\n",
    "Y_train = Y_train[train_idxs]\n",
    "\n",
    "X_test  = X_test[test_idxs]\n",
    "Y_test  = Y_test[test_idxs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c59ac62",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "We normalise the pixel data by way of preprocessing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28618fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise pixel values to [0,1]\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test  = X_test.astype('float32')  / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XjqSumIJlUwk",
   "metadata": {
    "id": "XjqSumIJlUwk"
   },
   "source": [
    "We will set an equal number of epochs for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tePIvNYYOpCs",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1747239707825,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "tePIvNYYOpCs"
   },
   "outputs": [],
   "source": [
    "# Define a common number of epochs for each model (you can increase or decrease this if you wish)\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6903cd2f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 804764,
     "status": "ok",
     "timestamp": 1747238619619,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "6903cd2f",
    "outputId": "2539fd6b-cc1c-4d31-ec53-0afbfe3ca6f8"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Build the neural network \n",
    "model_sigmoid = Sequential([\n",
    "    # Define the input layer to accept 28×28 grayscale images\n",
    "    Input(shape=(28, 28)),          \n",
    "    # Flatten the 2D image into a 1D vector of length 784\n",
    "    Flatten(),                      \n",
    "    # First hidden layer with 128 neurons and sigmoid activation\n",
    "    Dense(128, activation='sigmoid'),\n",
    "    # Second hidden layer, same size and activation to add depth\n",
    "    Dense(128, activation='sigmoid'),\n",
    "    # Third hidden layer to capture additional non-linear patterns\n",
    "    Dense(128, activation='sigmoid'),\n",
    "    # Fourth hidden layer for even richer feature extraction\n",
    "    Dense(128, activation='sigmoid'),\n",
    "    # Output layer with 10 neurons (one per digit class), using softmax\n",
    "    # so the outputs sum to 1 and represent a probability distribution\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model:\n",
    "# SGD optimiser with learning rate 0.01 for weight updates\n",
    "# sparse_categorical_crossentropy loss since labels are integer-encoded\n",
    "# track accuracy to monitor how many digits are classified correctly\n",
    "model_sigmoid.compile(\n",
    "    optimizer=SGD(learning_rate=0.01),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model on the 5 000-sample subset:\n",
    "# X_train, Y_train: training data and labels\n",
    "# validation_data: uses X_test, Y_test to check generalisation after each epoch\n",
    "# epochs=num_epochs: number of full passes through the training data\n",
    "# verbose=1: show a progress bar with loss and accuracy\n",
    "history_sigmoid = model_sigmoid.fit(\n",
    "    X_train, Y_train,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    epochs=num_epochs,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed3c95",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "After ten epochs your model is stuck at around 10–11 % accuracy with a loss of roughly 2.3, which is exactly what you’d expect from random guessing on ten classes (–log(1/10) ≈ 2.3). In practice, stacking four sigmoid-activated layers causes the activations to saturate and gradients to vanish, so the network barely updates its weights from their random initial values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadcec5e",
   "metadata": {
    "id": "aadcec5e"
   },
   "source": [
    "### Training accuracy with Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bf79ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1747238619840,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "56bf79ba",
    "outputId": "55973f3f-bd8a-4d63-b9e7-2efa826edb07"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training accuracy over epochs for the sigmoid-based model\n",
    "plt.plot(history_sigmoid.history['accuracy'], label='Train Acc (Sigmoid)')\n",
    "\n",
    "# Plot the validation accuracy over epochs for the same model\n",
    "plt.plot(history_sigmoid.history['val_accuracy'], label='Val Acc (Sigmoid)')\n",
    "\n",
    "# Label the x-axis to show training epochs\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "# Label the y-axis to show accuracy\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Set the plot title\n",
    "plt.title('Network with Sigmoid')\n",
    "\n",
    "# Display the legend to differentiate between training and validation curves\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5ba8d0",
   "metadata": {
    "id": "7a5ba8d0"
   },
   "source": [
    "Notice that the accuracy increases slowly. This is a sign that gradients may be very small in earlier layers, slowing learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9003f3",
   "metadata": {
    "id": "0b9003f3"
   },
   "source": [
    "### Comparing with ReLU activation\n",
    "Now, let’s build a similar network using *ReLU* instead of sigmoid for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6f52a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 797330,
     "status": "ok",
     "timestamp": 1747239417174,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "9d6f52a1",
    "outputId": "14934be9-f92d-4ac9-eab7-7ae48ce4a035"
   },
   "outputs": [],
   "source": [
    "# Define a neural network model using ReLU activations\n",
    "model_relu = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),                 # Flatten 28x28 images into 784-dimensional vectors\n",
    "    Dense(128, activation='relu'),                 # First hidden layer with ReLU activation\n",
    "    Dense(128, activation='relu'),                 # Second hidden layer\n",
    "    Dense(128, activation='relu'),                 # Third hidden layer\n",
    "    Dense(128, activation='relu'),                 # Fourth hidden layer\n",
    "    Dense(10, activation='softmax')                # Output layer (10 classes, softmax for classification)\n",
    "])\n",
    "\n",
    "# Compile the model using:\n",
    "# Stochastic Gradient Descent (SGD) optimiser\n",
    "# Sparse categorical crossentropy loss\n",
    "# Accuracy as the evaluation metric\n",
    "model_relu.compile(\n",
    "    optimizer=SGD(learning_rate=0.01),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model on the training data and validate on the test set\n",
    "history_relu = model_relu.fit(\n",
    "    X_train, Y_train,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    epochs=num_epochs,   # Number of epochs\n",
    "    verbose=1             # Show training progress\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b422eb3c",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "Switching to ReLU activations immediately lets the network learn meaningful features: training accuracy jumps from about 16 % in the first epoch to over 90 % by epoch 8, and training loss steadily falls from around 2.26 down to 0.29. \n",
    "\n",
    "On the validation side you see a rapid improvement too, val accuracy climbs from roughly 51 % at the start to a peak of about 89.3 % in epoch 9, with val loss falling to around 0.33. However, by epoch 10 the validation loss creeps back up (to 0.38) and val accuracy dips to 88.6 %, signalling the first signs of overfitting. \n",
    "\n",
    "In practice you’d capture the best generalisation by stopping training around epoch 8 or 9 (where val loss is lowest).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NjzNTRN4k-YT",
   "metadata": {
    "id": "NjzNTRN4k-YT"
   },
   "source": [
    "### Training accuracy with ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e_9nR72OwrFI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 126,
     "status": "ok",
     "timestamp": 1747239417303,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "e_9nR72OwrFI",
    "outputId": "6b171b92-4009-4c68-b6c0-eff41031b13b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training accuracy over epochs for the ReLU-based model\n",
    "plt.plot(history_relu.history['accuracy'], label='Train Acc (ReLU)')\n",
    "\n",
    "# Plot the validation accuracy over epochs for the same model\n",
    "plt.plot(history_relu.history['val_accuracy'], label='Val Acc (ReLU)')\n",
    "\n",
    "plt.title('Network with ReLU')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# Render the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BGhAFqtnfmbj",
   "metadata": {
    "id": "BGhAFqtnfmbj"
   },
   "source": [
    "As we can see it does much better! Training accuracy increases quite quickly per epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe79d21b",
   "metadata": {
    "id": "fe79d21b"
   },
   "source": [
    "### Comparing Sigmoid vs ReLU\n",
    "We produce a validation accuracy plot to compare the two trained models. You will see that as training progresses over multiple epochs, the plot tracks how well each model performs on unseen validation data.\n",
    "\n",
    "The ReLU-based model generally shows a clear advantage. It tends to improve more quickly in the early stages of training and reaches a higher overall accuracy. This is because ReLU avoids the issue of *vanishing gradients* we mentioned, which often affects sigmoid functions in deeper networks. With ReLU, gradients remain strong for positive inputs, allowing the network to learn more effectively across all layers.\n",
    "\n",
    "In contrast, the sigmoid-based model usually improves more slowly and may plateau at a lower accuracy. This is due to the saturation of the sigmoid function at high or low input values, which causes its gradients to shrink. As a result, learning becomes inefficient in deeper parts of the network, and performance is limited.\n",
    "\n",
    "Overall, the plot highlights a key reason why ReLU has become the standard activation function in deep learning. It enables faster, more stable training and often leads to better generalisation on tasks like image classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69e6d51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 166,
     "status": "ok",
     "timestamp": 1747239417470,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "b69e6d51",
    "outputId": "fddeb6b4-6f2a-48fd-aa82-6484e8e215ed"
   },
   "outputs": [],
   "source": [
    "# Plot validation accuracy for both models across training epochs\n",
    "plt.plot(history_sigmoid.history['val_accuracy'], label='Val Acc (Sigmoid)')  # Sigmoid-based model\n",
    "plt.plot(history_relu.history['val_accuracy'], label='Val Acc (ReLU)')        # ReLU-based model\n",
    "\n",
    "# Label the x-axis (training epochs)\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "# Label the y-axis (validation accuracy)\n",
    "plt.ylabel('Validation Accuracy')\n",
    "\n",
    "# Add a title to describe the comparison\n",
    "plt.title('Sigmoid vs ReLU on MNIST')\n",
    "\n",
    "# Add a legend to identify each curve\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4987df2d",
   "metadata": {
    "id": "4987df2d"
   },
   "source": [
    "The key take away is that from the plot, we typically see faster improvement in ReLU, indicating it suffers *less* from the vanishing gradient problem. So this can be your go-to option for many tasks, but always experiment first!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e478f87c",
   "metadata": {
    "id": "e478f87c"
   },
   "source": [
    "### Batch normalisation\n",
    "\n",
    "*Batch Normalisation* (often shortened to *BatchNorm*) is a widely used technique in deep learning that helps speed up and stabilise the training of neural networks. It works by normalising the activations (i.e. the outputs) of each layer so that they have a more consistent distribution typically with a mean close to 0 and a standard deviation close to 1 within each mini-batch during training. It helps in several ways:\n",
    "\n",
    "- *Stable distributions*:  \n",
    "   Without batch normalisation, the output of one layer might vary significantly during training, especially as the layers deeper in the network adapt their weights. This forces the next layer to constantly readjust to changing inputs. BatchNorm fixes this by ensuring that each layer sees data with a more stable mean and variance, which makes learning smoother.\n",
    "\n",
    "- *Reduces internal covariate shift*:  \n",
    "   As training progresses, the distribution of activations within the network can shift, causing instability and slower learning. This is known as internal *covariate shift*. BatchNorm helps to reduce this problem by keeping the activation distributions more consistent across training iterations.\n",
    "   >\n",
    "   > *Covariate shift* happens when the type of data a model sees *changes* either during training or between training and testing. For example, suppose you train a model to recognise animals in colour photos. Later, you give it black-and-white photos, this means the inputs have changed. That change is a kind of *covariate shift*, i.e. the input data has shifted, even though the task (recognising animals), is the same. In deep learning, internal covariate shift means this kind of shift happens *inside* the network between layers. As the model trains, the outputs from one layer can change a lot, which makes it harder for the next layer to learn properly.\n",
    "   >\n",
    "- *Maintains larger gradients*:  \n",
    "   When activations are kept within a moderate, non-saturated range, the *gradients* used in backpropagation are less likely to vanish. This is especially useful for deep networks where earlier layers often struggle to learn. BatchNorm helps maintain useful gradient sizes, allowing the network to train more effectively from end to end.\n",
    "\n",
    "There are other benefits, which include faster convergence - networks often train much faster with batch normalisation, sometimes allowing for higher learning rates. Also, a regularisation effect - BatchNorm can reduce the need for techniques like dropout, as it introduces some noise during training (due to per-batch statistics), which has a slight regularising effect. Lastly, we get improved performance. In many cases, models with batch normalisation not only train faster but also achieve better final accuracy:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23737fe0",
   "metadata": {},
   "source": [
    "### Model\n",
    "This next code sets up a controlled experiment to compare how inserting Batch Normalisation affects a simple feed-forward network on MNIST. First, it one-hot encodes the integer labels into 10-dimensional vectors so that both models use categorical cross-entropy.\n",
    "\n",
    "The `no BN` model simply flattens each 28×28 image, applies two sigmoid-activated dense layers (128 then 64 units), and ends with a softmax output for the ten digit classes. The `with BN` model has the same overall shape, but each dense layer is split into a linear transform followed immediately by Batch Normalisation, then the sigmoid activation. Both models use vanilla SGD at a learning rate of 0.1 and are trained for three epochs on the full training set.\n",
    "\n",
    "Finally, the code builds two smaller sub-models that take the same input images and tap the activations of the first hidden layer (post-sigmoid for the no-BN model, post-BatchNorm+sigmoid for the BN model). \n",
    "\n",
    "When running a batch of 256 test images through these sub-models, you end up with two activation matrices (`act_no_bn` and `act_with_bn`) that we can inspecte to see how Batch Normalisation changes the distribution of neuron activations before the next layer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BM3-1fWvTB6j",
   "metadata": {
    "executionInfo": {
     "elapsed": 172511,
     "status": "ok",
     "timestamp": 1747239589981,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "BM3-1fWvTB6j"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Activation, Input\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Convert labels to one-hot encoded vectors\n",
    "Y_train_cat = to_categorical(Y_train, 10)\n",
    "\n",
    "# Build a model WITHOUT Batch Normalisation\n",
    "input_1 = Input(shape=(28, 28))                        # Input layer for 28x28 images\n",
    "x1 = Flatten()(input_1)                                # Flatten image to 784-d vector\n",
    "x1 = Dense(128, activation='sigmoid')(x1)              # First hidden layer with sigmoid\n",
    "x1 = Dense(64, activation='sigmoid')(x1)               # Second hidden layer\n",
    "output_1 = Dense(10, activation='softmax')(x1)         # Output layer with softmax for 10 classes\n",
    "model_no_bn = Model(input_1, output_1)                 # Create the model\n",
    "\n",
    "# Build a model WITH Batch Normalisation\n",
    "input_2 = Input(shape=(28, 28))                        # Input layer\n",
    "x2 = Flatten()(input_2)                                # Flatten image\n",
    "x2 = Dense(128)(x2)                                    # First hidden layer (no activation yet)\n",
    "x2 = BatchNormalization()(x2)                          # BatchNorm to stabilise activations\n",
    "x2 = Activation('sigmoid')(x2)                         # Apply sigmoid after normalisation\n",
    "x2 = Dense(64)(x2)                                     # Second hidden layer\n",
    "x2 = BatchNormalization()(x2)                          # BatchNorm again\n",
    "x2 = Activation('sigmoid')(x2)                         # Sigmoid activation\n",
    "output_2 = Dense(10, activation='softmax')(x2)         # Output layer\n",
    "model_with_bn = Model(input_2, output_2)               # Create the model\n",
    "\n",
    "# Create separate optimisers for both models\n",
    "opt_no_bn = SGD(learning_rate=0.1)\n",
    "opt_with_bn = SGD(learning_rate=0.1)\n",
    "\n",
    "# Compile both models using categorical cross-entropy loss and accuracy as a metric\n",
    "model_no_bn.compile(optimizer=opt_no_bn, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_with_bn.compile(optimizer=opt_with_bn, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train each model briefly for 3 epochs on the training set\n",
    "model_no_bn.fit(X_train, Y_train_cat, epochs=3, batch_size=128, verbose=0)\n",
    "model_with_bn.fit(X_train, Y_train_cat, epochs=3, batch_size=128, verbose=0)\n",
    "\n",
    "# Select a small batch of test images for visualising activations\n",
    "sample = X_test[:256]\n",
    "\n",
    "# Create models that output the first hidden layer's activations\n",
    "layer_no_bn = Model(inputs=model_no_bn.input, outputs=model_no_bn.layers[2].output)\n",
    "layer_with_bn = Model(inputs=model_with_bn.input, outputs=model_with_bn.layers[4].output)\n",
    "\n",
    "# Get activation values from the selected sample\n",
    "act_no_bn = layer_no_bn(sample).numpy()\n",
    "act_with_bn = layer_with_bn(sample).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03771359",
   "metadata": {},
   "source": [
    "### Visualise the effects of BatchNormalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HurXxoxpdVfz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 424,
     "status": "ok",
     "timestamp": 1747239590400,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "HurXxoxpdVfz",
    "outputId": "d2a47d7e-5c08-4e98-b862-5132ad368022"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with two side-by-side plots\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Plot for activations WITHOUT BatchNorm\n",
    "plt.subplot(1, 2, 1)  # First subplot (left side)\n",
    "plt.hist(act_no_bn.flatten(), bins=50, color='skyblue', edgecolor='black')  # Plot histogram of raw activations\n",
    "\n",
    "plt.title('Activations WITHOUT BatchNorm')  \n",
    "\n",
    "plt.xlabel('Activation Value')               \n",
    "plt.ylabel('Frequency')                      \n",
    "\n",
    "# Plot for activations WITH BatchNorm\n",
    "plt.subplot(1, 2, 2)  # Second subplot (right side)\n",
    "plt.hist(act_with_bn.flatten(), bins=50, color='lightgreen', edgecolor='black')  # Histogram after BatchNorm\n",
    "\n",
    "plt.title('Activations WITH BatchNorm')     \n",
    "\n",
    "plt.xlabel('Activation Value')              \n",
    "plt.ylabel('Frequency')                     \n",
    "\n",
    "# Adjust layout to avoid overlapping elements\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S1XS-qzJVuqD",
   "metadata": {
    "id": "S1XS-qzJVuqD"
   },
   "source": [
    "Without BatchNorm, the activation values, especially when using functions like *sigmoid*, often get pushed towards the extremes: very close to 0 or very close to 1. This is because as signals pass through multiple layers, the outputs can grow or shrink unpredictably. When the activations end up in these extreme zones, the *sigmoid* function flattens out, and its gradient becomes very small. This is called *saturation*, and it slows down learning, particularly in the early layers of the network.\n",
    "\n",
    "With BatchNorm, the activation values are automatically scaled and shifted so they stay in a more moderate, central range usually centred around 0 with a consistent spread. This keeps the values away from the saturated parts of the activation function, allowing the gradients to remain large enough for effective learning. The result is smoother gradient flow, better weight updates, and faster, more stable training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3ae320",
   "metadata": {
    "id": "4b3ae320"
   },
   "source": [
    "### Residual connections (Skip Connections)\n",
    "\n",
    "*Residual connections* introduced in *ResNet* architectures allow the network to \"skip over\" layers by adding the input of a block directly to its output. Rather than learning the full transformation, the network learns the *residual*, the difference between input and output.\n",
    ">\n",
    "> In *ResNet* (Residual Network) architectures, the key innovation is the introduction of *residual connections*, or *skip connections* that we are discussing now. As we said, these allow the network to bypass one or more layers by adding the input directly to the output of a block, helping to solve the degradation problem where adding more layers leads to worse performance. This design enables the successful training of very deep networks (e.g. 50, 101, or even 152 layers), overcoming issues like vanishing gradients that previously limited most convolutional networks to 20–30 layers.  \n",
    ">\n",
    "> ResNet essentially made training deep neural networks possible  and practical, laying the foundation for many modern architectures in computer vision and beyond. Its residual connections have become a core building block in deep learning, enabling both depth and stability in complex models.\n",
    "\n",
    "Let's visualise the activations to see how skip connections preserve information across layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SnuotnIiYIP0",
   "metadata": {
    "executionInfo": {
     "elapsed": 115582,
     "status": "ok",
     "timestamp": 1747239705983,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "SnuotnIiYIP0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Add, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "# Build a plain deep model (no skip connections)\n",
    "inp_plain = Input(shape=(28, 28))                      # Input for 28x28 images\n",
    "x = Flatten()(inp_plain)                               # Flatten to 784\n",
    "x = Dense(128, activation='relu')(x)                   # First dense layer\n",
    "x = Dense(128, activation='relu')(x)                   # Second dense layer\n",
    "x = Dense(128, activation='relu')(x)                   # Third dense layer\n",
    "out_plain = Dense(10, activation='softmax')(x)         # Output layer (softmax for 10 classes)\n",
    "model_plain = Model(inputs=inp_plain, outputs=out_plain)\n",
    "\n",
    "# Build a residual model (with skip connection)\n",
    "inp_res = Input(shape=(28, 28))                        # Input layer\n",
    "x = Flatten()(inp_res)                                 # Flatten image\n",
    "\n",
    "h1 = Dense(128, activation='relu')(x)                  # First hidden layer\n",
    "h2 = Dense(128, activation='relu')(h1)                 # Second hidden layer\n",
    "h3 = Dense(128)(h2)                                    # Third hidden layer (no activation yet)\n",
    "\n",
    "res = Add()([h1, h3])                                  # Add skip connection: h1 + h3\n",
    "res = Activation('relu')(res)                          # Apply activation after addition\n",
    "\n",
    "out_res = Dense(10, activation='softmax')(res)         # Output layer\n",
    "model_res = Model(inputs=inp_res, outputs=out_res)\n",
    "\n",
    "# Compile both models with separate optimisers\n",
    "model_plain.compile(optimizer=SGD(learning_rate=0.1),\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "model_res.compile(optimizer=SGD(learning_rate=0.1),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Train both models briefly for 3 epochs\n",
    "model_plain.fit(X_train, Y_train_cat, epochs=3, batch_size=128, verbose=0)\n",
    "model_res.fit(X_train, Y_train_cat, epochs=3, batch_size=128, verbose=0)\n",
    "\n",
    "# Take a small sample from the test set for activation visualisation\n",
    "sample = X_test[:256]\n",
    "\n",
    "# Create models to extract activations from the final hidden layer\n",
    "intermediate_plain = Model(inputs=model_plain.input, outputs=model_plain.layers[-2].output)\n",
    "intermediate_res = Model(inputs=model_res.input, outputs=model_res.layers[-2].output)\n",
    "\n",
    "# Get the activations for the sample\n",
    "act_plain = intermediate_plain(sample).numpy()\n",
    "act_res = intermediate_res(sample).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-b6bMUspiNm6",
   "metadata": {
    "id": "-b6bMUspiNm6"
   },
   "source": [
    "And plotting the results gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Rl3EBLRLdo-F",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 414,
     "status": "ok",
     "timestamp": 1747239706376,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "Rl3EBLRLdo-F",
    "outputId": "2c0b68f3-abd2-4f8a-b6ba-84191d62250c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with two subplots side by side\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot histogram of activations from the plain (non-residual) model\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.hist(act_plain.flatten(), bins=50, color='orange', edgecolor='black')\n",
    "\n",
    "plt.title('Plain model Activations')   \n",
    "         \n",
    "plt.xlabel('Activation Value')                  \n",
    "plt.ylabel('Frequency')                         \n",
    "\n",
    "# Plot histogram of activations from the residual model\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plt.hist(act_res.flatten(), bins=50, color='green', edgecolor='black')\n",
    "\n",
    "plt.title('Residual model Activations')         \n",
    "\n",
    "plt.xlabel('Activation Value')                  \n",
    "plt.ylabel('Frequency')                         \n",
    "\n",
    "# Automatically adjust spacing between plots\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jIU7cZMJYO8T",
   "metadata": {
    "id": "jIU7cZMJYO8T"
   },
   "source": [
    "Looking at the plot we see that in the *plain model*, activations may become more compressed or irregular as the network deepens, especially without techniques like BatchNorm or ReLU in skip-free layers.\n",
    "\n",
    "In the *residual model*, skip connections help maintain more dynamic activations, preserving variation, keeping gradients flowing, and helping the model learn deeper representations without degradation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1926c008",
   "metadata": {
    "id": "1926c008"
   },
   "source": [
    "### What have we learnt?\n",
    "We explored some of the core challenges and solutions in training deep neural networks, with a particular focus on activation functions, normalisation techniques, and architectural design strategies. We began by examining the *vanishing gradient* problem - a key issue that arises when gradients shrink as they are propagated backward through multiple layers. This is especially common when using activation functions like *sigmoid* or *tanh*, which saturate at their extremes and produce very small derivatives. As a result, early layers in a network may stop learning entirely, slowing or even stalling the training process.\n",
    "\n",
    "To address this, we looked at the *ReLU* activation function, which has become the default in many modern architectures. Unlike sigmoid and tanh, ReLU does not saturate in the positive range and has a constant derivative of 1 for positive inputs. This helps preserve gradient strength, enabling more efficient learning and deeper networks to be trained effectively. Through visualisations, we saw how ReLU maintains larger gradients, while sigmoid and tanh activations quickly plateau.\n",
    "\n",
    "We then turned to *Batch Normalisation*, a technique that standardises activations within each mini-batch during training. This stabilises the distribution of inputs seen by each layer, reducing internal *covariate shift* and allowing for faster and more reliable convergence. BatchNorm also helps keep gradients in a healthy range, mitigating vanishing gradients and making training less sensitive to weight initialisation. When comparing activation distributions with and without BatchNorm, we observed how this technique results in more centred and balanced outputs, which is a crucial factor in stable training.\n",
    "\n",
    "Finally, we explored *residual connections* (or *skip connections*), a structural innovation that allows inputs to bypass one or more layers and be added directly to a deeper layer’s output. Residual connections make it easier for the network to learn incremental improvements over previous representations, rather than learning everything from scratch. More importantly, they create direct pathways for gradients to flow backward, greatly improving the trainability of very deep networks.\n",
    "\n",
    "Through our code and visual comparison, we saw how residual connections help maintain activation diversity and prevent the degradation often seen in plain deep networks."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
