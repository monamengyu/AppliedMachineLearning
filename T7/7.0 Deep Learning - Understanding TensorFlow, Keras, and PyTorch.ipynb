{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2ecfc25",
   "metadata": {
    "id": "b2ecfc25"
   },
   "source": [
    "## Machine Learning vs Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a86292",
   "metadata": {
    "id": "93a86292"
   },
   "source": [
    "### Introduction\n",
    "Machine learning is a subset of Artificial Intelligence (AI) that enables computers to learn from data without being explicitly programmed. It involves algorithms that identify patterns in data and make predictions or decisions based on them. Common machine learning techniques include:\n",
    "- *Supervised Learning* (e.g., linear regression, decision trees, support vector machines)\n",
    "- *Unsupervised Learning* (e.g., clustering, principal component analysis)\n",
    "- *Reinforcement Learning* (e.g., Q-learning, policy gradients)\n",
    "\n",
    "*Deep Learning (DL)* is a specialised subset of machine learning that uses artificial neural networks with multiple layers to model complex data representations. Deep learning models can automatically extract features from raw data, reducing the need for manual feature engineering like that we have seen with traditional models.\n",
    "\n",
    "The \"deep\" in deep learning refers to the number of layers in a neural network. Traditional machine learning models, like logistic regression or decision trees, operate with a few layers of computation. In contrast, deep learning models use multiple hidden layers (often more than three), allowing them to learn hierarchical and complex features from the data. For example, in image recognition:\n",
    "\n",
    "- The *first layers* detect simple patterns like edges and textures.\n",
    "- The *middle layers* recognise shapes and structures.\n",
    "- The *final layers* identify objects and their relationships.\n",
    "\n",
    "### When should you move to Deep Learning?\n",
    "\n",
    "You should consider using deep learning when:\n",
    "- Data size is large – deep learning thrives on large datasets (millions of examples). If you have limited data, traditional machine learning models often perform better.\n",
    "- Feature engineering is complex – if manually extracting features from data is challenging (e.g., images, speech, raw text), deep learning can automatically learn these representations.\n",
    "- High computational power is available – training deep networks requires powerful GPUs/TPUs. If you lack resources, simpler machine learning models might be more efficient.\n",
    "- Problem complexity is high – tasks such as natural language processing, image recognition, and reinforcement learning benefit from deep learning because of its ability to capture intricate patterns.\n",
    "- End-to-End learning is required – when you want the model to learn directly from raw data (e.g., pixels in an image, audio waveforms) without extensive preprocessing.\n",
    "\n",
    "### When to stick with traditional machine learning?\n",
    "- Small to medium datasets – if your dataset is small, simpler models (e.g., random forests, SVMs) often generalise better.\n",
    "- Interpretable models – deep learning models are often \"black boxes\", whereas decision trees and linear models provide clear decision-making insights.\n",
    "- Faster training & deployment – traditional machine learning models train faster and require fewer computational resources.\n",
    "\n",
    "In summary, deep learning is a powerful tool for complex problems with large datasets, but traditional machine learning is often preferable for smaller, structured data and interpretable models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecd0c47",
   "metadata": {
    "id": "fecd0c47"
   },
   "source": [
    "### Understanding TensorFlow, Keras, and PyTorch\n",
    "Deep learning is a subset of machine learning that mimics the way the human brain processes information using artificial neural networks. These networks consist of layers of interconnected neurons that can learn complex patterns from data. We will cover the fundamentals of deep learning by looking at TensorFlow, Keras, and PyTorch, and demonstrate their use on a real dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Uj0BN2fAg7zg",
   "metadata": {
    "id": "Uj0BN2fAg7zg"
   },
   "source": [
    "### Install Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j3amqanXg8Sw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 125895,
     "status": "ok",
     "timestamp": 1746117567397,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "j3amqanXg8Sw",
    "outputId": "f764eb01-0f4a-457e-8707-f3389d0fa3af"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d662094",
   "metadata": {
    "id": "2d662094"
   },
   "source": [
    "### MNIST Dataset\n",
    "\n",
    "The *MNIST* dataset is one of the most well-known resources in machine learning, especially for image classification tasks. It consists of 70,000 grayscale images of handwritten digits ranging from 0 to 9. These images are already split into two sets: 60,000 are used for training a model, and the remaining 10,000 are reserved for testing how well the model performs on unseen data.\n",
    "\n",
    "Each image in the dataset is 28 by 28 pixels in size and contains a single handwritten digit, with the corresponding label provided. Because of its clean and consistent format, the MNIST dataset is especially helpful for beginners. The images are already centred and size-normalised, which means there’s very little preprocessing needed before feeding them into a model.\n",
    "\n",
    "One of the main reasons MNIST is so widely used is that it allows developers and researchers to quickly prototype and test ideas without needing a large or complex dataset. It’s often used to compare the performance of different algorithms, from basic classifiers like logistic regression to more advanced deep learning models like convolutional neural networks (CNNs). While MNIST is relatively simple by today’s standards, it remains a great first step in learning how to work with image data and deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yeuAwBIthDcI",
   "metadata": {
    "id": "yeuAwBIthDcI"
   },
   "source": [
    "### Load the data\n",
    "The dataset contains both a training set and a test set, so the data has already been resampled as it were:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r62neJRrhD2g",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12504,
     "status": "ok",
     "timestamp": 1746117579905,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "r62neJRrhD2g",
    "outputId": "d570bfd1-5fb9-4d0c-9c82-f7b6a15e5ef0"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load dataset (MNIST)\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079c857c",
   "metadata": {
    "id": "079c857c"
   },
   "source": [
    "### Preprocess\n",
    "The main step we perform is to normalise the pixel values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d591382b",
   "metadata": {
    "executionInfo": {
     "elapsed": 458,
     "status": "ok",
     "timestamp": 1746117580361,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "d591382b"
   },
   "outputs": [],
   "source": [
    "X_train, X_test = X_train / 255.0, X_test / 255.0  # Normalising pixel values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd8f1a9",
   "metadata": {
    "id": "7bd8f1a9"
   },
   "source": [
    "You can think of a *neural network* as a flexible, powerful extension of simpler models like *logistic regression*. In logistic regression, we’re essentially drawing a straight or curved line (depending on the input space) through the data to separate categories like “yes” vs “no” or “cat” vs “dog”. This line is learned by adjusting weights so that the model fits the training data as well as possible, without overfitting.\n",
    "\n",
    "A *neural network* works in a similar way, but instead of just one line or curve, it’s like drawing a very flexible, squiggly line that can twist and bend through the data in much more complex ways. Each *neuron* in the network contributes a little curve or bend to this overall shape. As you stack more layers and add more neurons, the network gets better at shaping the decision boundary in ways that simpler models can't, it becomes able to separate data that’s tangled, overlapping, or follows patterns too complex for straight lines.\n",
    "\n",
    "In this sense, deep learning is like giving your model a pencil that can draw not just straight lines or simple curves, but intricate, detailed shapes that thread through the data, adapting to the underlying structure in order to classify or predict more accurately. The process of training, adjusting all those weights, is what sculpts that squiggly line to best fit the data.\n",
    "\n",
    "But just like with logistic regression, there's a balance. Too much bending (overfitting), and the model memorises the training data instead of learning general rules. The challenge of using neural networks is to get the line just flexible enough to learn real patterns without chasing the noise in our data.\n",
    "\n",
    "> In the next few sections we will discuss the main deep learning Python libraries you can use. We will not cover the architecture involved in constructing these models. For now, we will focus on looking at the key differences between the available APIs. After this gentle introduction, we will look deeper into the architecture and methods needed to work with deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2369e0",
   "metadata": {
    "id": "4d2369e0"
   },
   "source": [
    "### Introduction to TensorFlow\n",
    "*TensorFlow* is an open-source platform developed by Google for building and training machine learning and deep learning models. It provides a powerful yet flexible way to create systems that can learn from data, recognise patterns, and make predictions, such as classifying images, translating languages, or detecting spam emails.\n",
    "\n",
    "TensorFlow allows developers and researchers to build models using a set of modular building blocks, like layers of artificial neurons, mathematical operations, and training algorithms. These components can be combined to create simple models for beginners or complex neural networks for advanced applications like image recognition or natural language processing.\n",
    "\n",
    "One of TensorFlow’s strengths is that it efficiently manages computations using CPUs or GPUs (or even TPUs), and supports automatic differentiation (automatically computes derivatives), which is crucial for training models. It also integrates well with high-level APIs like *Keras*, making it easier to design, train, and evaluate models with minimal code.\n",
    "\n",
    "TensorFlow is widely used in both academia and industry due to its flexibility and, scalability. Let's demonstrate how to work with tensorflow and our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74853692",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 49746,
     "status": "ok",
     "timestamp": 1746117630106,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "74853692",
    "outputId": "05f8c637-28e9-4cc2-aa61-2504eef80d82"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist # Load the dataset\n",
    "\n",
    "# Build a simple feedforward neural network model\n",
    "model = tf.keras.Sequential([\n",
    "\n",
    "    # This layer flattens the 28x28 input images into 1D vectors of length 784\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "\n",
    "    # A hidden layer with 128 neurons and ReLU activation function\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "\n",
    "    # Output layer with 10 neurons (one for each digit 0–9) and softmax activation\n",
    "    # Softmax turns the output into a probability distribution\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with appropriate settings:\n",
    "# Adam optimiser: adjusts the weights efficiently\n",
    "# Sparse categorical crossentropy: suitable for integer class labels\n",
    "# Accuracy: track how often predictions match the labels\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model for 5 passes (epochs) through the training data:\n",
    "# Also evaluate accuracy on the test data during training\n",
    "history = model.fit(X_train, Y_train, epochs=5, validation_data=(X_test, Y_test))\n",
    "\n",
    "# Print a summary of the model's layers and parameters\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7QwJZ-EB0a1m",
   "metadata": {
    "id": "7QwJZ-EB0a1m"
   },
   "source": [
    "The model has three layers. The first step (flatten) takes the input (for example, a small image) and unrolls it into a long line of numbers. The second step (dense) connects these numbers to 128 small \"units\" that help the model learn patterns. And the third step (`dense_1`) reduces everything down to just 10 numbers, usually one for each possible result (like the numbers 0–9 as we are recognising handwritten digits).\n",
    "\n",
    "In total, the model has about 305,000 small adjustable parts (parameters) that help it learn. About 101,000 of these are actively adjusted during training to improve the model’s performance. The rest are extra pieces the model’s optimiser uses to help improve learning.\n",
    "\n",
    "We can now evaluate the model by visually inspecting the training and validation loss (more on this later). however, in short, the next part of our code creates two clear charts that help us visualise how well the model is learning as it trains. We will create two plots showing how the model’s performance changes over time, to help us better understand if it’s learning well or if there are problems.\n",
    "\n",
    "The first thing we do is set up a wide figure space so it can place two plots side by side. On the *left-hand plot*, we draw two lines: one showing the *training loss* (how wrong the model is on the examples it was taught with) and one showing the *validation loss* (how wrong it is on new, unseen examples). Ideally, both of these lines should go down over time, meaning the model is making fewer mistakes as it trains.\n",
    "\n",
    "On the *right-hand plot*, we draw another pair of lines: one showing *training accuracy* (how often the model gets the training examples right) and the other showing *validation accuracy* (how often it gets the new examples right). Here, we want the lines to go up, showing that the model is improving and getting more correct answers over time.\n",
    "\n",
    "Viewed together, these two charts give us a clear picture of whether the model is learning effectively or if it’s starting to overfit, that is, doing well on training examples, but struggling on new ones. This kind of visual check is a simple way to monitor how the model is doing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c153d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "executionInfo": {
     "elapsed": 685,
     "status": "ok",
     "timestamp": 1746117630792,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "01c153d4",
    "outputId": "799d5cd7-1c54-42f1-f98a-8c9c7db01e35"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Plot the loss (how wrong the model is) over training epochs\n",
    "plt.subplot(1, 2, 1)  # Create the first subplot (1 row, 2 columns, first plot)\n",
    "\n",
    "# Plot the training loss over time\n",
    "plt.plot(history.history['loss'], marker='o', label='Train Loss')\n",
    "# Plot the validation loss (on unseen data) over time\n",
    "plt.plot(history.history['val_loss'], marker='o', label='Val Loss')\n",
    "\n",
    "plt.title('Loss over Epochs')  # Set the plot title\n",
    "plt.xlabel('Epoch')            # Label the x-axis (number of training rounds)\n",
    "plt.ylabel('Loss')             # Label the y-axis (error)\n",
    "plt.grid(True)                 # Add a grid for easier reading\n",
    "plt.legend()                   # Show the legend (labels for the lines)\n",
    "\n",
    "# Plot the accuracy (how often the model is right) over epochs\n",
    "plt.subplot(1, 2, 2)  # Create the second subplot (1 row, 2 columns, second plot)\n",
    "\n",
    "# Plot the training accuracy over time\n",
    "plt.plot(history.history['accuracy'], marker='o', label='Train Accuracy')\n",
    "# Plot the validation accuracy (on unseen data) over time\n",
    "plt.plot(history.history['val_accuracy'], marker='o', label='Val Accuracy')\n",
    "\n",
    "plt.title('Accuracy over Epochs')  # Set the plot title\n",
    "plt.xlabel('Epoch')                # Label the x-axis\n",
    "plt.ylabel('Accuracy')             # Label the y-axis (percentage correct)\n",
    "plt.grid(True)                     # Add a grid\n",
    "plt.legend()                       # Show the legend\n",
    "\n",
    "# Adjust layout so plots don't overlap and display the figure\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e0a1d0",
   "metadata": {
    "id": "b9e0a1d0"
   },
   "source": [
    "Now the model has been trained we can test it out by passing in an image and predict the label, and create a furter plot to show the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23cb45e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "executionInfo": {
     "elapsed": 155,
     "status": "ok",
     "timestamp": 1746117630950,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "a23cb45e",
    "outputId": "c7c71919-d07c-4e3b-f389-3f184a97dd16"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Choose an image index from the test set to examine\n",
    "sample_index = 0\n",
    "\n",
    "# Extract the image and its corresponding true label\n",
    "sample_image = X_test[sample_index]         # Shape: (28, 28)\n",
    "true_label = Y_test[sample_index]           # Ground truth digit\n",
    "\n",
    "# Display the selected image using matplotlib\n",
    "plt.imshow(sample_image, cmap='gray')       # Show grayscale image\n",
    "plt.title(f\"True Label: {true_label}\")      # Title with actual label\n",
    "plt.axis('off')                             # Hide axis ticks for cleaner look\n",
    "plt.show()\n",
    "\n",
    "# Prepare the image for prediction:\n",
    "# The model expects a batch dimension → input shape should be (1, 28, 28)\n",
    "input_image = np.expand_dims(sample_image, axis=0)  # Add batch dimension\n",
    "\n",
    "# Use the trained model to predict probabilities for each digit class (0–9)\n",
    "predictions = model.predict(input_image)\n",
    "\n",
    "# Find the class with the highest predicted probability\n",
    "predicted_label = np.argmax(predictions[0])  # Returns the digit with highest confidence\n",
    "\n",
    "# Print out the predicted digit\n",
    "print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a399b3",
   "metadata": {
    "id": "98a399b3"
   },
   "source": [
    "### Introduction to Keras\n",
    "*Keras* is a high-level API that sits on top of lower-level deep learning frameworks like TensorFlow. Its main goal is to make the process of building, training, and testing neural networks much simpler and more intuitive. Instead of writing complex code to define each part of a neural network manually, Keras allows you to build models using easy to understand, modular building blocks like layers, activation functions, and optimisers with just a few lines of code.\n",
    "\n",
    "In earlier versions, Keras was a separate library that could be used with different backends (like TensorFlow, or Theano). However, it is now fully integrated into TensorFlow itself, which means you no longer need to install or import it separately. You can access all of Keras’s functionality simply by using `tensorflow.keras`.\n",
    "\n",
    "You may notice that the code written using Keras inside TensorFlow looks almost identical to previous standalone Keras examples. This is because the core principles and syntax haven’t changed, only now, you have the full power and flexibility of TensorFlow behind the scenes, including better performance, access to lower-level tools if needed, and compatibility with TensorFlow’s ecosystem (like data pipelines, visualisation, and deployment tools).\n",
    "\n",
    "For most users using `tf.keras` is recommended. It offers the simplicity of Keras with the scalability of TensorFlow, making it easier to get started without sacrificing performance or flexibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24f6fb5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 44448,
     "status": "ok",
     "timestamp": 1746117675399,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "b24f6fb5",
    "outputId": "a276dcb5-d7eb-40d2-a4f1-f0d091cff8b8"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Define a simple feedforward neural network using Keras Sequential API:\n",
    "model_keras = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),      # Input layer: flattens 28x28 image to 1D vector of size 784\n",
    "    keras.layers.Dense(128, activation='relu'),      # Hidden layer: 128 neurons with ReLU activation\n",
    "    keras.layers.Dense(10, activation='softmax')     # Output layer: 10 neurons for 10 classes with softmax for probability output\n",
    "])\n",
    "\n",
    "# Compile the model:\n",
    "# Optimiser: Adam (adaptive learning rate)\n",
    "# Loss function: sparse categorical crossentropy (for integer labels in classification)\n",
    "# Metric: accuracy (track performance during training)\n",
    "model_keras.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model on the training data for 5 epochs:\n",
    "# Also evaluate performance on validation (test) data during each epoch\n",
    "history = model_keras.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=5,\n",
    "    validation_data=(X_test, Y_test)\n",
    ")\n",
    "\n",
    "# Print a summary of the model architecture and parameter count\n",
    "print(model_keras.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hdO9-_ou0-63",
   "metadata": {
    "id": "hdO9-_ou0-63"
   },
   "source": [
    "Above we see the same summary as before, showing the structure of the network and the number of parameters. Just like before, we plot our evaluation metrics (training and validation loss, and accuracy) from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fba278",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 400,
     "status": "ok",
     "timestamp": 1746117675814,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "c8fba278",
    "outputId": "fbf31746-59b5-49c2-c7f1-8fb47c0dca24"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with two side-by-side subplots (1 row, 2 columns)\n",
    "plt.figure(figsize=(10, 4))  # Set the overall figure size\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(1, 2, 1)  # First subplot (left side)\n",
    "\n",
    "# Plot training loss over epochs\n",
    "plt.plot(history.history['loss'], marker='o', label='Train Loss')\n",
    "\n",
    "# Plot validation loss over epochs\n",
    "plt.plot(history.history['val_loss'], marker='o', label='Val Loss')\n",
    "\n",
    "plt.title('Loss over Epochs')  # Title for the plot\n",
    "plt.xlabel('Epoch')            # X-axis label\n",
    "plt.ylabel('Loss')             # Y-axis label\n",
    "plt.grid(True)                 # Add gridlines\n",
    "plt.legend()                   # Display legend\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)  # Second subplot (right side)\n",
    "\n",
    "# Plot training accuracy over epochs\n",
    "plt.plot(history.history['accuracy'], marker='o', label='Train Accuracy')\n",
    "\n",
    "# Plot validation accuracy over epochs\n",
    "plt.plot(history.history['val_accuracy'], marker='o', label='Val Accuracy')\n",
    "\n",
    "plt.title('Accuracy over Epochs')  # Title for the plot\n",
    "plt.xlabel('Epoch')                # X-axis label\n",
    "plt.ylabel('Accuracy')             # Y-axis label\n",
    "plt.grid(True)                     # Add gridlines\n",
    "plt.legend()                       # Display legend\n",
    "\n",
    "# Automatically adjust layout so plots don't overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7389e9",
   "metadata": {
    "id": "de7389e9"
   },
   "source": [
    "Let's predict the label of an image fed into our model for demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d9ac5a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "executionInfo": {
     "elapsed": 153,
     "status": "ok",
     "timestamp": 1746117675970,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "92d9ac5a",
    "outputId": "b076a667-7500-42e2-c15b-431d18551594"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Pick an image from the test set by index\n",
    "sample_index = 1\n",
    "sample_image = X_test[sample_index]          # Extract a 28x28 image from the test set\n",
    "true_label = Y_test[sample_index]            # Extract the true label for reference\n",
    "\n",
    "# Display the selected image using matplotlib\n",
    "plt.imshow(sample_image, cmap='gray')        # Show the image in grayscale:\n",
    "plt.title(f\"True Label: {true_label}\")       # Display the true label in the plot title\n",
    "plt.axis('off')                              # Remove axis ticks for cleaner display\n",
    "plt.show()\n",
    "\n",
    "# Prepare the image for prediction:\n",
    "# The model expects input in batch format: shape (batch_size, 28, 28)\n",
    "# So we add a new first dimension (batch of size 1)\n",
    "input_image = np.expand_dims(sample_image, axis=0)  # New shape: (1, 28, 28)\n",
    "\n",
    "# Use the trained model to make predictions on the input image:\n",
    "# The output is a vector of 10 probabilities (one for each digit class)\n",
    "predictions = model_keras.predict(input_image)\n",
    "\n",
    "# Get the index of the class with the highest predicted probability\n",
    "predicted_label = np.argmax(predictions[0])\n",
    "\n",
    "# Print out the predicted label\n",
    "print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae1779f",
   "metadata": {
    "id": "1ae1779f"
   },
   "source": [
    "So far we have defined and trained the same neural network using the Keras API within TensorFlow, and they behave identically in terms of functionality and performance. The only difference lies in how the model is referenced. In the first example, the model is created using `tf.keras.Sequential`, which calls the Keras API through the main TensorFlow namespace.\n",
    "\n",
    "In the second example, the model is created using `keras.Sequential`, having imported Keras directly from `tensorflow`. Although the import styles differ slightly, both refer to the same `tf.keras` module, the version of Keras that is fully integrated into TensorFlow. Since TensorFlow 2.0, this is the recommended way to build models, as it ensures compatibility with other TensorFlow tools and simplifies the workflow.\n",
    "\n",
    "In practice, there is no behavioural or performance difference between the two approaches. Using `tf.keras.Sequential` is generally preferred for consistency and clarity, especially when working on larger projects that use the full TensorFlow ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef0e844",
   "metadata": {
    "id": "0ef0e844"
   },
   "source": [
    "### Introduction to PyTorch\n",
    "\n",
    "*PyTorch* is an open-source deep learning framework developed by Facebook’s AI Research lab. It is widely used for building and training neural networks, particularly in research and academic settings. PyTorch is known for its simplicity, flexibility, and Pythonic design, making it easy to learn and intuitive to use. Unlike some other frameworks, PyTorch allows for dynamic computation meaning models can be built and modified on the fly, which is especially helpful for tasks that require flexibility or experimentation.\n",
    "\n",
    "It provides a range of tools for building deep learning models, managing data, and optimising performance, all while integrating smoothly with Python libraries like NumPy and matplotlib. PyTorch also includes built-in support for GPUs, enabling faster training on large datasets.\n",
    "\n",
    "In PyTorch, all custom models must inherit from nn.Module. Calling `super().__init__()` ensures that all the internal mechanisms provided by `nn.Module` (like parameter tracking, saving/loading models, etc.) are correctly set up. Without this line, your model won’t function properly as a PyTorch model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b542a3fd",
   "metadata": {
    "executionInfo": {
     "elapsed": 5894,
     "status": "ok",
     "timestamp": 1746117681872,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "b542a3fd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load MNIST handwritten digits data from TensorFlow\n",
    "# X_train, Y_train → 60,000 training images and labels\n",
    "# X_test, Y_test → 10,000 test images and labels\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "# Normalise pixel values from [0, 255] to [0, 1] for better neural network performance\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# Split off part of the test set (first 5000 images) to use as a validation set\n",
    "X_val, Y_val = X_test[:5000], Y_test[:5000]             # Validation set (5000 examples)\n",
    "X_test_final, Y_test_final = X_test[5000:], Y_test[5000:]  # Remaining test set (5000 examples)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "# Convert training data to PyTorch tensors and flatten each 28x28 image into a 784-length vector\n",
    "x_train_torch = torch.tensor(X_train, dtype=torch.float32).view(-1, 28 * 28)\n",
    "y_train_torch = torch.tensor(Y_train, dtype=torch.long)\n",
    "\n",
    "# Convert validation data\n",
    "x_val_torch = torch.tensor(X_val, dtype=torch.float32).view(-1, 28 * 28)\n",
    "y_val_torch = torch.tensor(Y_val, dtype=torch.long)\n",
    "\n",
    "# Create datasets and DataLoaders\n",
    "# Combine input and label tensors into TensorDataset objects\n",
    "train_dataset = TensorDataset(x_train_torch, y_train_torch)\n",
    "val_dataset = TensorDataset(x_val_torch, y_val_torch)\n",
    "\n",
    "# Create DataLoaders to load data in mini-batches:\n",
    "# - Training loader shuffles the data each epoch for better learning\n",
    "# - Validation loader does not shuffle (order doesn’t matter, but consistency helps)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71b0f8d",
   "metadata": {
    "id": "b71b0f8d"
   },
   "source": [
    "We define a simple feedforward neural network using PyTorch. The goal of this model, as before, is to classify handwritten digits (0–9) from the MNIST dataset. \n",
    "\n",
    "We start by creating a custom class called `SimpleNN`, which inherits from `nn.Module`. This is the standard way to define neural networks in PyTorch, as it allows us to build and manage model layers and behaviour.\n",
    "\n",
    "Inside the `__init__` method (the constructor), we define the layers of our network:\n",
    "\n",
    "- The first layer is a *fully connected layer* (also called a dense layer) that takes in 784 inputs, this corresponds to the 28×28 pixels of the flattened image. It outputs 128 values, which represent learned features.\n",
    "- Next, we apply a *ReLU activation function*, which introduces non-linearity into the model. This allows the network to learn more complex patterns.\n",
    "- Finally, we have an *output layer* with 10 neurons, one for each digit class (0 to 9). The output values represent the model’s predictions.\n",
    "\n",
    "The `forward()` method defines how data moves through the network during prediction. Input `x` is passed through the first dense layer, transformed by the activation function, and then passed to the output layer. The result is a vector of 10 values, one for each class. During training, these values will be used to compute a loss and adjust the model’s weights accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a6cd33",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1746117681874,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "10a6cd33"
   },
   "outputs": [],
   "source": [
    "# Define a simple feedforward neural network (inherits from nn.Module)\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()  # Call the parent class constructor to set up\n",
    "\n",
    "        # Define the first fully connected (dense) layer:\n",
    "        # Input size is 28x28 (flattened image) → output size 128 neurons\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "\n",
    "        # Define a ReLU activation function (adds non-linearity)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Define the second fully connected (dense) layer:\n",
    "        # Input size 128 neurons → output size 10 neurons (for 10 digit classes)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    # Define the forward pass (how data flows through the network)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)   # Pass input through first dense layer\n",
    "        x = self.relu(x)  # Apply ReLU activation\n",
    "        x = self.fc2(x)   # Pass through second dense layer (outputs raw class scores)\n",
    "        return x          # Return the final output (logits, before softmax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98151a87",
   "metadata": {
    "id": "98151a87"
   },
   "source": [
    "Once we have defined a class, we can instantiate it and set up the rest of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90ebb32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42104,
     "status": "ok",
     "timestamp": 1746117723976,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "d90ebb32",
    "outputId": "f579a65a-0b16-434e-a959-688b0eafb80c"
   },
   "outputs": [],
   "source": [
    "# Instantiate the model, loss function, and optimiser\n",
    "model_pytorch = SimpleNN()                         # Create an instance of the neural network\n",
    "criterion = nn.CrossEntropyLoss()                 # Use cross-entropy loss for multi-class classification\n",
    "optimiser = optim.Adam(model_pytorch.parameters(), lr=0.001)  # Adam optimiser with learning rate 0.001\n",
    "\n",
    "# Set training parameters\n",
    "epochs = 10                                       # Number of times to loop over the full training data\n",
    "epoch_losses = []                                 # To store average training loss per epoch\n",
    "epoch_accuracies = []                             # To store average training accuracy per epoch\n",
    "val_epoch_losses = []                             # To store average validation loss per epoch\n",
    "val_epoch_accuracies = []                         # To store average validation accuracy per epoch\n",
    "\n",
    "# Main training loop\n",
    "for epoch in range(epochs):\n",
    "    model_pytorch.train()                         # Set the model to training mode\n",
    "    running_loss = 0.0                            # Accumulate total training loss this epoch\n",
    "    correct = 0                                   # Count correct predictions on training data\n",
    "    total = 0                                     # Count total training samples\n",
    "\n",
    "    # Loop over batches from the training DataLoader\n",
    "    for data, labels in train_dataloader:\n",
    "        optimiser.zero_grad()                     # Reset gradients from the previous batch\n",
    "        outputs = model_pytorch(data)             # Forward pass: compute predictions\n",
    "        loss = criterion(outputs, labels)         # Compute the loss against true labels\n",
    "        loss.backward()                           # Backward pass: compute gradients\n",
    "        optimiser.step()                          # Update model weights\n",
    "\n",
    "        running_loss += loss.item()               # Add batch loss to total\n",
    "        _, predicted = torch.max(outputs, 1)      # Get predicted class (index of max logit)\n",
    "        correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "        total += labels.size(0)                   # Count total samples seen\n",
    "\n",
    "    avg_loss = running_loss / len(train_dataloader)    # Average loss over epoch\n",
    "    accuracy = correct / total                          # Average accuracy over epoch\n",
    "    epoch_losses.append(avg_loss)                       # Save for plotting later\n",
    "    epoch_accuracies.append(accuracy)                  # Save for plotting later\n",
    "\n",
    "    # Validation loop\n",
    "    model_pytorch.eval()                           # Set model to evaluation mode (no dropout, etc.)\n",
    "    val_loss = 0.0                                # Accumulate total validation loss this epoch\n",
    "    val_correct = 0                               # Count correct predictions on validation data\n",
    "    val_total = 0                                 # Count total validation samples\n",
    "\n",
    "    with torch.no_grad():                         # Disable gradient tracking for validation\n",
    "        for val_data, val_labels in val_dataloader:\n",
    "            val_outputs = model_pytorch(val_data)        # Forward pass on validation data\n",
    "            v_loss = criterion(val_outputs, val_labels)  # Compute validation loss\n",
    "            val_loss += v_loss.item()                   # Add to total validation loss\n",
    "\n",
    "            _, val_predicted = torch.max(val_outputs, 1)  # Get predicted class\n",
    "            val_correct += (val_predicted == val_labels).sum().item()  # Count correct\n",
    "            val_total += val_labels.size(0)                        # Count total samples\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)        # Average validation loss\n",
    "    val_accuracy = val_correct / val_total               # Average validation accuracy\n",
    "    val_epoch_losses.append(avg_val_loss)               # Save for plotting\n",
    "    val_epoch_accuracies.append(val_accuracy)          # Save for plotting\n",
    "\n",
    "    # Print summary of the epoch’s performance\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, \"\n",
    "          f\"Train Loss: {avg_loss:.4f}, Train Acc: {accuracy * 100:.2f}%, \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wxSz9HvY2xIZ",
   "metadata": {
    "id": "wxSz9HvY2xIZ"
   },
   "source": [
    "As with our other models, we can now plot the training and validation loss, and accuracy to evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddde64f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "executionInfo": {
     "elapsed": 369,
     "status": "ok",
     "timestamp": 1746117724346,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "ddde64f5",
    "outputId": "cf454eae-621f-4c88-8e40-ad232c9af9ef"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting loss and accuracy over epochs\n",
    "plt.figure(figsize=(10, 4)) \n",
    "\n",
    "# Plot loss (left-hand side)\n",
    "plt.subplot(1, 2, 1)  # Create the first subplot (1 row, 2 columns, position 1)\n",
    "\n",
    "# Plot training loss over epochs\n",
    "plt.plot(epoch_losses, marker='o', label='Train Loss')\n",
    "\n",
    "# Plot validation loss over epochs\n",
    "plt.plot(val_epoch_losses, marker='o', label='Val Loss')\n",
    "\n",
    "# Add title and axis labels\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Add gridlines for easier reading\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the legend to label the lines\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracy (right-hand side)\n",
    "plt.subplot(1, 2, 2)  # Create the second subplot (1 row, 2 columns, position 2)\n",
    "\n",
    "# Plot training accuracy over epochs\n",
    "plt.plot(epoch_accuracies, marker='o', label='Train Accuracy')\n",
    "\n",
    "# Plot validation accuracy over epochs\n",
    "plt.plot(val_epoch_accuracies, marker='o', label='Val Accuracy')\n",
    "\n",
    "# Add title and axis labels\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Add gridlines\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the legend\n",
    "plt.legend()\n",
    "\n",
    "# Adjust layout to prevent overlap between plots and labels\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bd0422",
   "metadata": {
    "id": "b1bd0422"
   },
   "source": [
    "Let's see what the model achieved, by demonstrating how we make predictions from the model using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f4c348",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1746117724355,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": -60
    },
    "id": "89f4c348",
    "outputId": "a76f862a-f3c4-43fe-ad27-cc95f8d43a6f"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Get a single sample from the test set\n",
    "\n",
    "# Get the 4th image and label (index 3)\n",
    "sample_img = X_test_final[3]         # Shape: (28, 28), NumPy array\n",
    "sample_label = Y_test_final[3]       # Integer label\n",
    "\n",
    "# Convert the image to a PyTorch tensor and flatten it\n",
    "sample_img_torch = torch.tensor(sample_img, dtype=torch.float32).view(-1)  # Shape: (784,)\n",
    "\n",
    "# Reshape for visualisation (back to 28x28)\n",
    "image_2d = sample_img_torch.view(28, 28)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(image_2d, cmap='gray')\n",
    "plt.title(f\"True Label: {sample_label}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Prepare the image for prediction\n",
    "# Add batch dimension: shape becomes (1, 784)\n",
    "sample_input = sample_img_torch.unsqueeze(0)\n",
    "\n",
    "# Make a prediction with the trained model\n",
    "model_pytorch.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model_pytorch(sample_input)\n",
    "    predicted_label = torch.argmax(output, dim=1).item()\n",
    "\n",
    "# Display the model's predicted label\n",
    "print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9db42ee",
   "metadata": {
    "id": "c9db42ee"
   },
   "source": [
    "This kind of architecture, built manually in PyTorch, is a good starting point because it gives you more transparency and control over what’s happening under the hood. Compared to TensorFlow and Keras, PyTorch tends to be more flexible and intuitive for those who want to understand how neural networks work step by step.\n",
    "\n",
    "In PyTorch, you write your own training loop, manage gradients manually (via `backward()` and `optimiser.step()`), and can easily inspect intermediate outputs or tweak the computation at any point. This level of granular control is especially helpful when you're learning or debugging, or when you're working on custom research models that don’t fit into standard layers or workflows.\n",
    "\n",
    "Keras is great for getting things working quickly thanks to its high-level abstractions, but PyTorch gives you a deeper understanding of how learning actually happens, which is why many learners and researchers prefer to start with it before moving to higher-level tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c520a668",
   "metadata": {
    "id": "c520a668"
   },
   "source": [
    "### Summary of approaches\n",
    "*TensorFlow* is a powerful, all-in-one machine learning framework developed by Google. It allows developers to build, train, and deploy machine learning models, not just in research environments, but also in production settings like mobile apps, web services, and cloud platforms. One of its key strengths is its ability to scale from small experiments on a laptop to full-scale deployment across servers or devices. It’s widely used in industry because of its strong performance, deployment tools, and integration with services like TensorFlow Lite and TensorFlow Serving.\n",
    "\n",
    "*Keras*, which is now built into TensorFlow, is a high-level API that makes creating deep learning models more intuitive and beginner-friendly. Instead of having to write long and complex code, Keras allows users to define models in just a few lines, using simple building blocks like layers, activations, and loss functions. This makes it a great starting point for anyone new to deep learning or for quickly prototyping ideas.\n",
    "\n",
    "*PyTorch*, developed by Facebook, is often the go-to choice in academic and research settings. Its main appeal lies in how easy it is to use and modify. Unlike TensorFlow, which originally used a more rigid structure, PyTorch allows you to write and test code more like regular Python, which makes it easier to understand and debug. Researchers appreciate this flexibility when trying out new ideas or rapidly iterating on model designs.\n",
    "\n",
    "In short, TensorFlow is ideal for scaling and deploying models; Keras makes it easier to use; and PyTorch is popular in research because of its flexibility and ease of experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e633de",
   "metadata": {
    "id": "e9e633de"
   },
   "source": [
    "### What have we learnt?\n",
    "We've explored how different deep learning frameworks approach the task of building and training models. Both *TensorFlow* and *Keras* provide a structured and user-friendly way to define neural networks. These tools simplify the process of creating complex models and are designed to scale easily from small experiments to full-scale applications. Because of this, they are particularly well suited for real-world use cases, where models need to be deployed in production, whether on websites, apps, or embedded devices.\n",
    "\n",
    "On the other hand, *PyTorch* offers a more flexible and intuitive coding experience, which makes it very popular in academic research and experimentation. It allows researchers to test new ideas and make changes to model architecture on the fly, using code that closely resembles standard Python. This flexibility makes it a valuable tool for developing new techniques or quickly prototyping novel approaches.\n",
    "\n",
    "Each framework has its own strengths and ultimately, the choice of framework depends on what you're trying to achieve.\n",
    "\n",
    "For our purposes, we will primarily use *TensorFlow* with *Keras*, as this combination offers an excellent balance between ease of use and the ability to deploy models efficiently."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
