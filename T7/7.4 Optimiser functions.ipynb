{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bq6pZBVsw9wa",
   "metadata": {
    "id": "bq6pZBVsw9wa"
   },
   "source": [
    "## Regularisation and optimisation techniques\n",
    "\n",
    "### Introduction\n",
    "When training neural networks, it’s not enough to simply define an architecture and pick an activation function. We also need to control overfitting and improve convergence. This is where *regularisation* and *optimisation* techniques come into play. We will demonstrate why regularisation is important. demonstrate *dropout* and *L2 weight decay* on a simple model. We will also compare different *optimisers* (SGD, RMSProp, Adam) to see how they affect training and illustrate how these techniques can improve validation loss.\n",
    "\n",
    "*Regularisation* is about preventing a model from fitting the training data too closely, a scenario called *overfitting*. If a model memorises the training data, it might perform poorly on unseen data. By introducing controlled constraints or noise, we encourage the network to learn general patterns rather than memorising specifics.\n",
    "\n",
    "Common regularisation methods include:\n",
    "- *Dropout*: Randomly \"drops\" neurons during training to prevent them from relying on specific paths.\n",
    "- *L2 Weight Decay* (or Ridge regularisation): Penalises large weights to encourage smaller, more stable parameter values.\n",
    "- *L1* (or Lasso): Encourages sparsity by driving some weights to zero.\n",
    "\n",
    "### Why different optimisers?\n",
    "While *SGD* (Stochastic Gradient Descent) is the original go-to method, more advanced optimisers like *RMSProp* and *Adam* adapt the learning rate for each parameter, often leading to *faster convergence* and better results.\n",
    "\n",
    "- *SGD*: Updates weights by the average gradient from a mini-batch.\n",
    "- *RMSProp*: Keeps a moving average of squared gradients, adapting each parameter’s learning rate.\n",
    "- *Adam*: Combines RMSProp and Momentum ideas, often converging quickly in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34077449",
   "metadata": {},
   "source": [
    "### Install Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e8c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow torch numpy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ixn93N5wjv3j",
   "metadata": {
    "id": "ixn93N5wjv3j"
   },
   "source": [
    "### The CIFAR-10 dataset\n",
    "\n",
    "The *CIFAR-10* (Canadian Institute for Advanced Research) dataset is one of the most popular datasets used for developing and benchmarking computer vision models. It is commonly used in deep learning projects, especially for beginners who want to build image classification models with neural networks. The dataset was created by Alex Krizhevsky, Geoffrey Hinton, and Vinod Nair in 2009 and is available freely for academic and educational use.\n",
    "\n",
    "CIFAR-10 consists of 60,000 color images with a resolution of 32x32 pixels. Each image belongs to one of 10 classes, and the classes are chosen to be mutually exclusive and visually distinct. This relatively small image size makes the dataset lightweight and fast to train on, which is great for learning and experimenting without needing powerful hardware.\n",
    "\n",
    "The dataset is split into a training set of 50,000 images and a test set of 10,000 images. Each class has exactly 6,000 images, providing a balanced distribution that helps in training fair models. All images are in RGB (3 channels), and the pixel values range from 0 to 255 (often normalised to the 0–1 range during preprocessing).\n",
    "\n",
    "CIFAR-10 is especially useful for demonstrating the effects of different training techniques, such as regularistion, dropout, data augmentation, and convolutional architectures. Because it's easy to overfit on this dataset with a complex model, it's a perfect choice to illustrate how regularisation methods help improve generalisation.\n",
    "\n",
    "Let's look at the different categories of data:\n",
    "\n",
    "| Label | Category    | Description                                                 |\n",
    "|-------|-------------|-------------------------------------------------------------|\n",
    "| 0     | Airplane    | Images of aircraft, often in flight, with sky backgrounds. |\n",
    "| 1     | Automobile  | Various cars and trucks, not including pickup trucks.       |\n",
    "| 2     | Bird        | Side or top views of birds, sometimes in natural settings.  |\n",
    "| 3     | Cat         | Domestic cats in different poses and environments.          |\n",
    "| 4     | Deer        | Wild deer in forest or grass backgrounds.                   |\n",
    "| 5     | Dog         | Various dog breeds, similar to cats in image composition.   |\n",
    "| 6     | Frog        | Frogs typically in outdoor settings like grass or ponds.    |\n",
    "| 7     | Horse       | Horses in side view, often in outdoor or farm settings.     |\n",
    "| 8     | Ship        | Boats and ships, including container ships and sailboats.   |\n",
    "| 9     | Truck       | Larger vehicles, including pickup and cargo trucks.         |\n",
    "\n",
    "This diversity of image types provides a good challenge for image classifiers. The small image size forces models to learn efficient, compact feature representations, and the simplicity of the dataset allows us to focus on model design and training techniques without getting overwhelmed by data processing or scale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UR14Fm8Dw9we",
   "metadata": {
    "id": "UR14Fm8Dw9we"
   },
   "source": [
    "### Loading the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0685aed",
   "metadata": {
    "id": "b0685aed"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "# URL and target path\n",
    "cifar_url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "target_path = \"cifar-10-python.tar.gz\"\n",
    "extract_path = \"cifar-10-batches-py\"\n",
    "\n",
    "# Download if not already present\n",
    "if not os.path.exists(target_path):\n",
    "    print(\"Downloading CIFAR-10 dataset...\")\n",
    "    urllib.request.urlretrieve(cifar_url, target_path)\n",
    "\n",
    "# Extract if not already extracted\n",
    "if not os.path.exists(extract_path):\n",
    "    print(\"Extracting CIFAR-10 dataset...\")\n",
    "    with tarfile.open(target_path, \"r:gz\") as tar:\n",
    "        tar.extractall()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r5MXRUFci3cV",
   "metadata": {
    "id": "r5MXRUFci3cV"
   },
   "source": [
    "Load data from extracted pickle files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-qgizwyai3la",
   "metadata": {
    "id": "-qgizwyai3la"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def load_batch(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        batch = pickle.load(f, encoding='bytes')\n",
    "        data = batch[b'data']\n",
    "        labels = batch[b'labels']\n",
    "        # Reshape and normalise\n",
    "        data = data.reshape(-1, 3, 32, 32).astype(\"float32\") / 255.0\n",
    "        return data, np.array(labels)\n",
    "\n",
    "limit_train = 400\n",
    "limit_test = 100\n",
    "\n",
    "# Load a small sample of the training set (e.g., first 400 samples)\n",
    "data, labels = load_batch(f\"{extract_path}/data_batch_1\")\n",
    "\n",
    "X_train = data[:limit_train].transpose(0, 2, 3, 1)  \n",
    "Y_train = labels[:limit_train]\n",
    "\n",
    "# Load a small sample of the test set (e.g., first 100 samples)\n",
    "X_test_full, Y_test_full = load_batch(f\"{extract_path}/test_batch\")\n",
    "X_test = X_test_full[:limit_test].transpose(0, 2, 3, 1)\n",
    "Y_test = Y_test_full[:limit_test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3987d6e",
   "metadata": {
    "id": "e3987d6e"
   },
   "source": [
    "Now that we have our data loaded, we visualise a subset of the training images in a grid, each annotated with its corresponding class label, to gain a better understanding of the data we will be working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cea929c",
   "metadata": {
    "id": "4cea929c",
    "outputId": "e9075daa-4cb4-462c-ecad-c38a305374a3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define label names for CIFAR-10\n",
    "cifar10_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                  'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Function to plot a grid of images\n",
    "def plot_cifar_images(images, labels, class_names, n=5):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(n * n):\n",
    "        plt.subplot(n, n, i + 1)\n",
    "        plt.imshow(images[i])\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot a 5x5 grid of training images\n",
    "plot_cifar_images(X_train, Y_train, cifar10_labels, n=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cca72f",
   "metadata": {
    "id": "49cca72f"
   },
   "source": [
    "### Adding Regularisation to a CNN\n",
    "In this exercise, we extend a basic convolutional neural network (CNN) for the CIFAR-10 dataset by incorporating regularisation techniques to improve generalisation and reduce overfitting. The model consists of two convolutional blocks followed by fully connected layers, and we apply two forms of regularisation throughout:\n",
    "\n",
    "1. *Dropout*: Introduced after each pooling layer and before the final dense layers, dropout randomly deactivates a proportion of neurons during training, forcing the network to develop more redundant and general features.\n",
    "\n",
    "2. *L2 Regularisation* (weight decay): Applied to the convolutional and dense layers, this technique penalises large weights by adding a term to the loss function, encouraging the model to learn simpler, more stable patterns.\n",
    "\n",
    "The model is compiled using the Adam optimiser and trained using sparse categorical cross-entropy, which is suitable for multi-class classification with integer labels. If we compare the performance of this regularised model against a baseline without dropout and L2 penalties, we can observe how regularisation helps control overfitting and improves validation performance on unseen data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7699b146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a common number of epochs for our experiment\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbe758f",
   "metadata": {
    "id": "acbe758f"
   },
   "source": [
    "### Train model without Regularisation\n",
    "\n",
    "In this step, we will train a version of the model *without* any regularisation techniques, in order to compare its performance with the regularised version. Specifically, we will remove both *dropout* and *L2 regularisation* from the architecture.\n",
    "\n",
    "Regularisation methods like dropout and L2 penalty are designed to reduce overfitting by making the model less sensitive to noise in the training data. When we remove them, the model is more likely to memorise the training set, especially if the dataset is small or complex. As a result, we might observe a lower training loss but a higher validation loss, an indication that the model is not generalising well to new data.\n",
    "\n",
    "If we train a non-regularised version of the same model and compare its training and validation losses against the regularised version, we can better understand the impact of regularisation on generalisation performance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c9ceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential  # Sequential model groups layers linearly\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense  # Core layers: convolution, pooling, flatten, dense\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop  # Optimizer algorithms\n",
    "\n",
    "\n",
    "def build_model_no_regularisation(input_shape=(32, 32, 3),\n",
    "                                  num_classes=10,\n",
    "                                  optimizer='adam'):\n",
    "    \"\"\"\n",
    "    Builds a simple convolutional neural network (CNN) for CIFAR-10 classification,\n",
    "    without any dropout or L2 regularisation.\n",
    "\n",
    "    Parameters:\n",
    "    - input_shape (tuple): Dimensions of the input images, e.g. (32,32,3).\n",
    "    - num_classes (int): Number of target classes (10 for CIFAR-10).\n",
    "    - optimizer (str or keras Optimizer): Which optimiser to use ('adam', 'sgd', 'rmsprop').\n",
    "\n",
    "    Returns:\n",
    "    - A compiled Keras Sequential model, ready for training.\n",
    "    \"\"\"\n",
    "    # Initialise a sequential (stacked) model\n",
    "    model = Sequential([\n",
    "        # Conv block 1: 32 filters, 3x3 kernel, ReLU activation, same padding to keep size\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same',\n",
    "               input_shape=input_shape),\n",
    "        # Downsample feature maps by 2x2\n",
    "        MaxPooling2D((2, 2)),\n",
    "\n",
    "        # Conv block 2: 64 filters to learn more complex features\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        # Further downsampling\n",
    "        MaxPooling2D((2, 2)),\n",
    "\n",
    "        # Conv block 3: 128 filters for even deeper feature extraction\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "\n",
    "        # Flatten 3D feature maps into 1D vector for classification layers\n",
    "        Flatten(),\n",
    "        # Fully connected layer with 128 neurons and ReLU activation\n",
    "        Dense(128, activation='relu'),\n",
    "        # Output layer: num_classes neurons with softmax to output probabilities\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Choose optimiser based on user argument\n",
    "    if optimizer == 'sgd':\n",
    "        # Stochastic gradient descent with momentum for smoothing\n",
    "        opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        # RMSprop adapts learning rate per parameter\n",
    "        opt = RMSprop(learning_rate=0.001)\n",
    "    else:\n",
    "        # Default: Adam combines momentum and adaptive rates\n",
    "        opt = Adam(learning_rate=0.001)\n",
    "\n",
    "    # Compile model: specify loss and metrics\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss='sparse_categorical_crossentropy',  # for integer labels\n",
    "        metrics=['accuracy']  # track classification accuracy\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca81d3fc",
   "metadata": {
    "id": "ca81d3fc",
    "outputId": "533a5408-8c69-443a-a233-120d86cd0eed"
   },
   "outputs": [],
   "source": [
    "# Build the non-regularised model with the Adam optimiser\n",
    "model_no_reg = build_model_no_regularisation(optimizer='adam')\n",
    "\n",
    "# Train the model on the training data\n",
    "# Evaluate generalisation performance using the test set as validation data\n",
    "# Train for the same number of epochs as the regularised model\n",
    "history_no_reg = model_no_reg.fit(\n",
    "    X_train, Y_train,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    epochs=num_epochs,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a2f0c0",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "Over the twenty epochs, your convolutional model learned very quickly on the examples it saw during training. Its training accuracy went from about 16 per cent in the first epoch to over 91 per cent by the last, and its training loss fell steeply from around 2.30 down to 0.31. \n",
    "\n",
    "In contrast, its performance on the unseen validation set was much more erratic where validation accuracy peaked at 42 per cent in epoch 18 but finished at just 29 per cent, while validation loss generally drifted upwards, ending at about 2.51. In plain terms, the network got extremely good at the images it practised on but struggled to apply what it’d learned consistently to new images, which is a tell-tale sign of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11fefe3",
   "metadata": {
    "id": "d11fefe3"
   },
   "source": [
    "### Train model with Regularisation\n",
    "In this step, we will train our convolutional neural network with both dropout and L2 regularisation applied. Specifically, we will set the `dropout_rate` to 0.2, apply an L2 `weight_decay` (regularisation strength) of `1e-4`, and use the `Adam` optimiser.\n",
    "\n",
    "These values are commonly used as sensible defaults when starting to regularise a model:\n",
    "\n",
    "* *Dropout rate of 0.2* means that during training, 20% of the neurons in each dropout layer will be randomly deactivated in each forward pass. This is a moderate amount that typically helps reduce overfitting without significantly impairing the model’s ability to learn. It's a good starting point because it introduces regularisation without being too aggressive.\n",
    "\n",
    "* *Weight decay of 1e-4* applies a small penalty to large weights, encouraging the model to keep its parameter values small and stable. This helps avoid overly complex models that can memorise the training data. A value of `1e-4` is often used in practice as it tends to strike a balance between under- and over-regularisation.\n",
    "\n",
    "* *Adam optimiser* is chosen for its adaptive learning rate and efficiency in training deep networks. It combines the benefits of both RMSProp and momentum-based gradient descent, and it generally performs well out-of-the-box across a wide range of tasks.\n",
    "\n",
    "Starting with these parameters will hopefully improve the model’s ability to generalise to unseen data, reduce the risk of overfitting, and maintain stable and efficient training behaviour. These settings can later be tuned based on validation performance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b6bd0c",
   "metadata": {
    "id": "80b6bd0c"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_model_regularisation(dropout_rate=0.3, weight_decay=1e-4, optimizer='adam'):\n",
    "    \"\"\"\n",
    "    Builds a CNN model with dropout and L2 regularisation for image classification on CIFAR-10.\n",
    "\n",
    "    Parameters:\n",
    "    - dropout_rate (float): fraction of neurons to drop during training.\n",
    "    - weight_decay (float): strength of L2 regularisation.\n",
    "    - optimizer (str or keras.optimizers.Optimizer): optimiser to use for training.\n",
    "\n",
    "    Returns:\n",
    "    - A compiled Keras Sequential model.\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential([\n",
    "\n",
    "        # First convolutional layer with 32 filters of size 3x3\n",
    "        # Uses ReLU activation and 'same' padding to preserve spatial dimensions\n",
    "        # Applies L2 regularisation to the kernel weights\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same',\n",
    "               kernel_regularizer=l2(weight_decay), input_shape=(32, 32, 3)),\n",
    "\n",
    "        # Max pooling reduces the feature map size by half (32x32 -> 16x16)\n",
    "        MaxPooling2D((2, 2)),\n",
    "\n",
    "        # Dropout randomly disables a fraction of neurons to prevent overfitting\n",
    "        Dropout(dropout_rate),\n",
    "\n",
    "        # Second convolutional layer with 64 filters of size 3x3\n",
    "        # Again uses ReLU activation and L2 regularisation\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same',\n",
    "               kernel_regularizer=l2(weight_decay)),\n",
    "\n",
    "        # Another max pooling operation (16x16 -> 8x8)\n",
    "        MaxPooling2D((2, 2)),\n",
    "\n",
    "        # Additional dropout for regularisation\n",
    "        Dropout(dropout_rate),\n",
    "\n",
    "        # Flatten the 3D feature maps into a 1D vector for the dense layers\n",
    "        Flatten(),\n",
    "\n",
    "        # Fully connected layer with 128 neurons and L2 regularisation\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(weight_decay)),\n",
    "\n",
    "        # Dropout before the final classification layer\n",
    "        Dropout(dropout_rate),\n",
    "\n",
    "        # Output layer with 10 neurons (one per CIFAR-10 class)\n",
    "        # Softmax activation converts outputs to class probabilities\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile the model with specified optimiser and sparse categorical cross-entropy loss\n",
    "    # Accuracy is used as the evaluation metric\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dbfe2f",
   "metadata": {
    "id": "c3dbfe2f",
    "outputId": "944db700-39d6-455a-855b-722594ff5322"
   },
   "outputs": [],
   "source": [
    "# Number of training epochs\n",
    "num_epochs = 20\n",
    "\n",
    "# Build the CNN model with regularisation\n",
    "# Dropout rate set to 0.2 (20% of neurons dropped during training)\n",
    "# L2 weight decay set to 1e-4 to penalise large weights\n",
    "# Using the Adam optimiser for adaptive learning rate and stability\n",
    "model_with_reg = build_model_regularisation(\n",
    "    dropout_rate=0.2,\n",
    "    weight_decay=1e-4,\n",
    "    optimizer='adam'\n",
    ")\n",
    "\n",
    "# Train the model on the training data for 20 epochs\n",
    "# Use a validation set to monitor generalisation (X_test, Y_test)\n",
    "# 'verbose=1' prints progress bar and training metrics\n",
    "history_with_reg = model_with_reg.fit(\n",
    "    X_train, Y_train,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    epochs=num_epochs,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c3f568",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "This version generalised better to the test images. Although its training accuracy only climbed to about 76 per cent (versus 92 per cent in the pure conv model), its final validation accuracy settled at 37 per cent rather than 29 per cent, and its validation loss ended lower (≈2.25 vs ≈2.51). In other words, it didn’t memorise the training set quite as perfectly, but it did a noticeably better job when faced with unseen data, so it’s less over-fitted and overall more reliable on new images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7f379c",
   "metadata": {
    "id": "fa7f379c"
   },
   "source": [
    "### Comparing Validation Loss\n",
    "\n",
    "To evaluate the effectiveness of regularisation, we will compare the *validation loss* of the regularised and non-regularised models side by side. Validation loss provides a useful indication of how well the model generalises to unseen data, lower values typically suggest better performance on data outside the training set.\n",
    "\n",
    "Regularisation techniques such as dropout and L2 penalty are designed to prevent overfitting by reducing the model’s reliance on specific weights and features. In contrast, a non-regularised model may achieve very low training loss but struggle to perform well on the validation set due to memorising noise or irrelevant patterns.\n",
    "\n",
    "Let's plot the validation loss over epochs for both models so we can observe:\n",
    "\n",
    "* Whether the regularised model maintains a lower and more stable validation loss,\n",
    "* Whether the non-regularised model begins to overfit (i.e. validation loss increases while training loss continues to decrease),\n",
    "* And how quickly each model converges during training.\n",
    "\n",
    "This visual comparison helps us understand the trade-offs between model complexity and generalisation, and demonstrates the practical benefits of applying regularisation in deep learning workflows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee53a0b",
   "metadata": {
    "id": "1ee53a0b",
    "outputId": "64cdbea2-ba6f-4ad1-b8ca-183dd05e85c1"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt  # Import matplotlib for plotting\n",
    "\n",
    "# Plot the validation loss for the model with regularisation\n",
    "plt.plot(history_with_reg.history['val_loss'], label='With Regularisation')\n",
    "\n",
    "# Plot the validation loss for the model without regularisation\n",
    "plt.plot(history_no_reg.history['val_loss'], label='No Regularisation')\n",
    "\n",
    "# Set the title of the plot\n",
    "plt.title('Validation Loss comparison')\n",
    "\n",
    "# Label the x-axis (number of training epochs)\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "# Label the y-axis (loss value on validation set)\n",
    "plt.ylabel('Validation Loss')\n",
    "\n",
    "# Display the legend to differentiate between the two curves\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaedacbf",
   "metadata": {
    "id": "eaedacbf"
   },
   "source": [
    "If the model with dropout and L2 regularisation shows a lower validation loss or a smoother, more stable curve across epochs compared to the non-regularised model, it provides strong evidence that the regularisation is effectively reducing overfitting and improving generalisation. Although, in practice, there may be more work needed to improve performance.\n",
    "\n",
    "A *lower validation loss* means the model is performing better on data it hasn’t seen during training, which is the ultimate goal in most machine learning tasks. Regularisation helps by discouraging the model from learning overly complex patterns or memorising the training data, which often leads to poor performance on new inputs.\n",
    "\n",
    "A *smoother curve* (i.e. fewer sharp spikes or fluctuations in the validation loss) indicates that the model's learning is more stable and less sensitive to noise or individual training samples. This is another benefit of regularisation, especially dropout, which introduces randomness during training and forces the model to rely on distributed representations rather than specific neurons or weights.\n",
    "\n",
    "In contrast, if the non-regularised model shows rapid improvements in training performance but its validation loss either stagnates or increases, this typically signals overfitting: the model is learning the training data too well at the expense of general performance. Therefore, observing these trends in the validation loss curve is a useful diagnostic tool for deciding whether regularisation is necessary or whether additional techniques (like data augmentation) might be beneficial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1c4c85",
   "metadata": {
    "id": "8a1c4c85"
   },
   "source": [
    "### Experimenting with different optimisers\n",
    "\n",
    "Optimisers play a critical role in how a neural network learns during training. They determine how the model's weights are updated in response to the calculated gradients from the loss function. To explore the effect of different optimisation strategies, you can modify the `optimizer` argument in the model-building functions to use `'sgd'`, `'rmsprop'`, or `'adam'`. Each of these optimisers implements a different approach to adjusting learning rates and updating parameters, and as a result, they produce different training behaviours and generalisation characteristics.\n",
    "\n",
    "#### SGD (Stochastic Gradient Descent)\n",
    "\n",
    "* This is the most basic optimiser, using the raw gradients to update weights after each batch.\n",
    "* It typically requires careful tuning of the learning rate and often benefits from techniques like momentum or learning rate schedules.\n",
    "* Although it can be slow to converge, especially on complex datasets like CIFAR-10, it is sometimes preferred when generalisation is more important than speed, as it may avoid overfitting better in some cases.\n",
    "\n",
    "#### RMSProp (Root Mean Square Propagation)\n",
    "\n",
    "* RMSProp improves on SGD by adapting the learning rate for each weight individually based on a moving average of recent squared gradients.\n",
    "* This helps the optimiser handle non-stationary objectives and avoid issues with vanishing or exploding gradients.\n",
    "* It is particularly effective for training models on noisy or sparse data and is well-suited to recurrent neural networks.\n",
    "\n",
    "#### Adam (Adaptive Moment Estimation)\n",
    "\n",
    "* Adam is one of the most popular and effective optimisers in deep learning.\n",
    "* It combines the benefits of both RMSProp (adaptive learning rates) and momentum (accumulated gradient history), making it robust and efficient across a wide range of tasks.\n",
    "* Adam generally converges quickly and performs well out of the box, making it a strong default choice for most problems.\n",
    "\n",
    "If you experiment with these different optimisers, you can observe how the choice affects convergence speed, final accuracy, and stability of training and validation losses. This is an important part of model tuning and helps build an understanding of how different learning dynamics can influence model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8799c0",
   "metadata": {
    "id": "db8799c0"
   },
   "source": [
    "### What have we learnt?\n",
    "\n",
    "Throughout this experiment, we’ve seen how both *regularisation* and *optimiser choice* significantly influence the performance and reliability of a neural network.\n",
    "\n",
    "*Regularisation* techniques such as *Dropout* and *L2 weight decay* are crucial for reducing overfitting. This is a common issue where a model learns to perform very well on the training data but fails to generalise to unseen examples. Dropout achieves this by randomly deactivating neurons during training, which forces the model to learn more robust, distributed representations. L2 regularisation, on the other hand, penalises large weights, encouraging the network to find simpler solutions that generalise better. When used effectively, regularisation leads to smoother and lower validation loss curves, indicating better generalisation and more reliable performance on new data.\n",
    "\n",
    "We’ve also learnt that the choice of *optimiser* affects how efficiently a model converges during training. Traditional *SGD* takes small, fixed steps in the direction of the gradient, which can be slow and sensitive to learning rate settings. More advanced optimisers like *RMSProp* and *Adam* adapt the learning rate for each parameter and incorporate information from past gradients. These adaptations allow the model to navigate complex loss landscapes more effectively and converge more quickly and reliably. Adam, in particular, is widely used due to its robustness and minimal need for manual tuning.\n",
    "\n",
    "These experiments highlight the importance of applying the right tools to control overfitting and ensure smooth convergence. If you combine regularisation with well-chosen optimisers, you will be able to build models that perform well on training data, as well as, maintain strong and consistent performance on unseen data, which is essentially our main goal.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
