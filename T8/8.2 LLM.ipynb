{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Language Models (LLMs)\n",
    "### Introduction\n",
    "You may have heard or used ChatGPT recently. ChatGPT, Google Bard, and other conversational AIs, are examples of Large Language Models, or LLMs for short.  LLMs are recent advances in deep learning models to work on human language tasks, such as text generation and question answering. A large language model is a trained deep-learning model that understands and generates text in a human-like fashion. Behind the scenes, it is a large transformer model.\n",
    "\n",
    "A transformer model can be implemented together with what we call an attention mechanism. This attention mechanism allows the model to weigh the importance of different words (or tokens) in a sequence, allowing the model to capture long range dependencies and relationships.\n",
    "\n",
    "This means that transformer models perform better due to the fact that they learn the context surrounding the use of a word.\n",
    "\n",
    "To process text input with a transformer model, you first need to tokenise it into a sequence of characters, sub-words, or whole words. These tokens are then encoded as numbers which we can then convert into embeddings. Embeddings are vector-space representations of the tokens that preserve their positioning. In short, they are represented by an array of numbers, where each number points to a word in the vocabulary of the language.\n",
    "\n",
    "The encoder in the transformer transforms the embeddings of all the tokens into a context vector.  The context vector allows the transformer decoder to generate output based on clues. For instance, our initial input would be in the form of a text prompt that can be passed on to the transformer decoder to produce the next most probable word given our input. We can then repeat this by reusing the same decoder, and using the previously predicted next-word as the input, and so on. By repeating this process, a transformer model can generate an entire passage of text word by word. The transformer model learns the rules of human language implicitly through the training examples presented to it. If the training data is large enough, it is possible for the model to learn not only the rules of grammar, but also semantics, and even whole concepts.\n",
    "\n",
    "We will use `PyTorch` for constructing our neural network. This will allow us to look at the details underlying the model rather than use tensorflow, which obscures some of the detail from you.\n",
    "\n",
    "We will build neural networks with the `torch.nn` namespace, which provides the classes you need to build a neural network. Every module in PyTorch subclasses the `nn.Module`, and so it works a little differently from `tensorflow`, but does have the advantage of constructing sophisticated neural networks relatively quickly using a modular approach.\n",
    "\n",
    "Deep learning models can take a lot of resources and time to train, even on a small data set, so this example will take some time to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Python libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch numpy matplotlib nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture\n",
    "The transformer architecture follows an encoder-decoder structure. The encoder is tasked with mapping an input sequence to a sequence of continuous representations. The decoder takes the output of the encoder together with the decoder output at the previous time step to generate an output sequence. When generating an output sequence, the transformer does not rely on mechanisms such as recurrence and convolutions. Instead, LLMs architectures tend to consist of multiple layers, including feedforward layers, embedding layers, and attention layers.  \n",
    "\n",
    "A typical task for transformers is language translation, where we use a typical sequence-to-sequence model (i.e. from English to German), which have both an encoder and decoder.\n",
    "\n",
    "For models like those of Chat GPT, where we wish to generate text based on a prompt, text generation models only require the decoder element. This is due to the fact that the input and output sequences are essentially the same as the model generates text token-by-token according to the previous context.  Consequently, for text generation, we only need to focus on implementing the decoder portion of the transformer model. \n",
    "\n",
    "- *Tokenisation*: \n",
    "We break our training data text into tokens, in our case, we will be working at the character-level (this gives us more data than using word-bsaed corpora, but also keeps the training manageable). Even so we would still need a huge dataset to obtain good generative results.\n",
    "\n",
    "- *Token Embedding*: \n",
    "The model first creates token embeddings. Each token (word, subword, or character) in the input sequence is represented by a high-dimensional vector, which is initialized randomly and learned during training. The embedding dimension is a hyperparameter.\n",
    "\n",
    "- *Positional Encoding*: \n",
    "This helps the model to understand the order of the tokens in the sequence provided. Positional encoding is achieved by adding a vector to each token embedding, which encodes the token's position in the sequence.  This is important since words or tokens in natural language have an order to them that is driven by the syntax of the language in question, or the semantics.\n",
    "\n",
    "- *Masking*: This layer stops the model from looking ahead, or learning from padding tokens (more on this later).\n",
    "\n",
    "- *Decoder Stack*: The normalised output is then passed into a stack of decoders. Each decoder block in the stack consists of a self-attention mechanism (multi-headed self-attention) and a feed-forward neural network, with layer normalisation and residual connections throughout. Each decoder block refines learning over multiple layers.  After the data passes through the decoder stack, we reach the final part of the model, the output layer represented by the language model head.\n",
    "\n",
    "- *Output layer (Language model head)*: This layer is responsible for predicting token probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset\n",
    "Our dataset will consist of a custom collection of short texts we have gathered containing short sentences based on Haikus from various webpages. The idea is that the final model will be able to generate text similar to the chosen dataset given a text prompt we provide after training has completed.  The first step is to preprocess the data ready for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUr haiku data\n",
    "training_data = \"\"\"\n",
    "memorial day a shadow for each white cross.\n",
    "spring rain as the doctor speaks i think of lilacs.\n",
    "spring moonset a rice ball for breakfast.\n",
    "sunny afternoon an old man lingers near the mailbox.\n",
    "cinco de mayo horses roll in the shallows.\n",
    "quitting time the smell of rain in the lobby.\n",
    "waves slowly cresting towards shore a faint moon.\n",
    "overnight rain the scent of orange blossoms in a desert town.\n",
    "misty summer rain calling pheasant in Zen temple.\n",
    "day is done poppies amidst the dying grass.\n",
    "watching clouds the white petals of a crushed crocus.\n",
    "mountain stream two well placed rocks the path home.\n",
    "night shift in the parking lot car lights dim near morning.\n",
    "a wild violet on the sunny hill noon time nap.\n",
    "a sunny day pink haze of the cherry blossoms over the hill.\n",
    "polished oak the freesia's shadow ends in coffee foam.\n",
    "nobody here a table in the mountain speckled with petals.\n",
    "first date not even noticing the new moon.\n",
    "vanishing difference gliding geese settle onto their reflections.\n",
    "distant hawk a gust of cherry petals crosses the lawn.\n",
    "Orange sunrise peaks through The tubes and wires of father's Life support machine.\n",
    "moonlessness so many ways I want to touch you.\n",
    "earth day even the shadows at dusk smell green.\n",
    "so cold a goose honks its way across the night sky.\n",
    "damp straw the day old colt mesmerized by the radio.\n",
    "prying at the window wind.\n",
    "in winter's wind the call of a friend who now has cancer.\n",
    "rainy bridge the river flowing faster.\n",
    "spring breeze the balcony's shadows on my book.\n",
    "winter drizzle all the passing faces look into the cafe.\n",
    "smells of spring adrift in the morning air bubbles under ice.\n",
    "spring morning your hand on my chest a bird.\n",
    "crossroads the brown core of an apple.\n",
    "hot afternoon a flirting couple in the memorial's shade.\n",
    "last red in the sky a small girl's moon face rises over the counter.\n",
    "day moon the woman with silver hair steps back into the shade.\n",
    "morning delivery snow comes in with the FedEx man.\n",
    "easter an anxious mother calls in the wind.\n",
    "windless day the prolific weeds at the grave site.\n",
    "spring day cool and grey cup of tea.\n",
    "grey today the waves rush memories.\n",
    "spring equinox Pray the ancestor grave In cold rain.\n",
    "folding chair the newborn colt tries to stand.\n",
    "scattered sun one chickadee louder than the rest.\n",
    "orthopaedic clinic a three legged chair outside the entrance.\n",
    "the plane's landing lights a wave of barking sweeps the neighbourhood.\n",
    "the way grass parts as the pheasant passes spring's end.\n",
    "at half mast a butterfly passes by the still flag.\n",
    "\"\"\"\n",
    "    \n",
    "training_data = training_data.lower().replace('\\n', ' ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data[: 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "#### Tokenisation\n",
    "We first need to tokenise the text.  Tokenisation involves breaking down the text into smaller parts known as tokens. A token might be a single character, sub-word, or a whole word. The choice of token size depends on the specific task and method being used.\n",
    "\n",
    "For instance, GPT-3 (the model underlying ChatGPT) uses a form of tokenisation called *Byte Pair Encoding (BPE)*, which tokenises text into subwords. These subwords are sequences of characters that frequently appear together, for instance *str* in words like *str*ing, *str*ong, *str*eet, *str*ap. This approach represents a mid-point between character-level and word-level tokenisation.\n",
    "\n",
    "We create a very simple tokeniser that will encode the characters `a-z`, and numbers `0-9`. We will also encode the period `.` and the whitespace character `\" \"`. Our texts are quite short typically composed a few words per line. Therefore, we will use character-level tokenisation. This means our model will be tasked with generating human-like language on the basis of characters to form complete words and phrases.\n",
    "\n",
    "#### Vocabulary\n",
    "Once we have tokenised the data, the unique set of tokens is known as the vocabulary of the language. The size of a vocabulary depends on the complexity of the language and the granularity of the tokenisation. Our vocabulary is very small by contrast. GPT-3's tokeniser uses a vocabulary of 50,257 tokens! This large vocabulary includes a broad range of English words, common word parts, and even whole phrases, enabling GPT-3 to understand and generate highly nuanced text. This is beyond the scope of our little model.\n",
    "\n",
    "#### Padding\n",
    "For better understanding of the patterns governing human language we need some notion of memory to record the beginning of the sequence of words when we reach the end.\n",
    "\n",
    "A transformer expects a fixed size input. However, real-world text comes in sequences of variable length. To accommodate this, we use a technique called padding. Padding involves extending short sequences with further tokens to match the length of the longest sequence in the batch of training examples. \n",
    "\n",
    "For our model, we will use a type of padding known as left-padding. This involves prepending a special token (`<pad>`) to the beginning of shorter sequences until they match the length of the longest sequence in our dataset. Padding will ensure that every token in every sequence, regardless of its original length, will be used in the training process.\n",
    "\n",
    "We create a class to group related functionality together for our simple tokeniser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "class Tokeniser:\n",
    "    def __init__(self):\n",
    "        self.dictionary = {}\n",
    "        self.reverse_dictionary = {}\n",
    "        self.stopwords = string.punctuation.replace(\".\", \"\").replace(\"?\", \"\")\n",
    "        self.stopwords += 'â€™“”¦–\\xa0¹'\n",
    "        # Add the padding token\n",
    "        self.__add_to_dict('<pad>')\n",
    "        \n",
    "        # Add numbers to our stop list\n",
    "        for i in range(10):\n",
    "            self.__add_to_dict(str(i))\n",
    "\n",
    "        # Add space and punctuation to the dictionary\n",
    "        self.__add_to_dict('.')\n",
    "        self.__add_to_dict('?')\n",
    "        self.__add_to_dict(' ')\n",
    "\n",
    "        # Add characters to the dictionary as our vocabulary\n",
    "        for i in range(26):\n",
    "            self.__add_to_dict(chr(ord('a') + i))\n",
    "        \n",
    "    def __add_to_dict(self, character):\n",
    "        if character not in self.dictionary:\n",
    "            self.dictionary[character] = len(self.dictionary)\n",
    "            self.reverse_dictionary[self.dictionary[character]] = character\n",
    "\n",
    "    def tokenise(self, text):\n",
    "        text = text.lower().replace('\\n',' ').replace('–', '')\n",
    "        for i in self.stopwords:\n",
    "            text = text.replace(i, \"\")\n",
    "        return [self.dictionary[c] for c in text if c not in self.stopwords]\n",
    "\n",
    "    def character_to_token(self, character):\n",
    "        return self.dictionary[character]\n",
    "\n",
    "    def token_to_character(self, token):\n",
    "        return self.reverse_dictionary[token]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this tokeniser produces after processing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = Tokeniser()\n",
    "\n",
    "tokenised_training_data = tokeniser.tokenise(training_data)\n",
    "\n",
    "print(tokenised_training_data[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenisation converts each token in the text into a unique integer identifier, or index, using a dictionary called a vocabulary. The vocabulary consists of a list of all unique tokens that the model will be trained to recognise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the mapping of characters to indices - the vocabulary\n",
    "tokeniser.dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two dictionaries, one whose key is represented by each character of our vocabulary, and another with keys associated with the integer values assigned to each vocabulary item. This allows us to map between the two representations.  For instance, we can convert the training data from integer values, back to the original text. We can use this approach later when generating the text character by character in the prediction step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the training data (reconstruct it from the indices character-by-character)\n",
    "text = \"\"\n",
    "\n",
    "for c in tokenised_training_data:\n",
    "    text += tokeniser.reverse_dictionary.get(c, \"\")\n",
    "\n",
    "print(text[:100]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input embedding\n",
    "The next step is to turn tokens into a numerical representation that can be used by our machine learning model. This is performed by the embedding layer.  When tokenising the text, we replaced the characters with integers as we saw. However, to capture the relationship between different tokens, we need the embedding layer to act as a bridge between discrete tokens and what is known as continuous vector space. A vector space is composed of a set whose elements (called vectors) can be added together or multiplied by numbers called scalars (represented by real numbers).\n",
    "\n",
    "If our tokens are words, the embedding layer helps capture the similarity between words like \"King\" and \"Queen\" by placing their respective vectors close together in vector space. This closeness stems from the fact that \"King\" and \"Queen\" may often appear in similar contexts, i.e. surrounded by similar tokens related to being the head of state or a member of the royal court.  In short, the embedding layer transforms each token, represented as an integer, into a continuous vector in a high-dimensional space.  \n",
    "\n",
    "The `torch.nn.Embedding` class allows us to create the embedding layer. During training, this layer's weights (representing our token embeddings) are updated to better capture the semantic relationships between different tokens.\n",
    "\n",
    "In the code below, the variable `number_of_tokens` stores the total number of unique tokens our model can encounter in the input. This number typically equals the size of our token dictionary. Whereas the parameter `d_model`, specifies dimensionality (size) of the embedding vectors. Higher dimensions enable our model to encode more information on each token, but can also increase the complexity and train timing of the model.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "class TokenEmbedding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch module that converts tokens into embeddings.\n",
    "\n",
    "    Input dimension is: (batch_size, sequence_length)\n",
    "    Output dimension is: (batch_size, sequence_length, d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, number_of_tokens):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = torch.nn.Embedding(\n",
    "            num_embeddings=number_of_tokens,\n",
    "            embedding_dim=d_model\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to `TokenEmbedding` will be a batch of sequences, with each token represented by an integer. The output, is a batch of the same sequences, but with each integer now being replaced by a high-dimensional vector encapsulating semantic information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers work on the basis of self-attention, which is responsible for computing relevance scores for elements in the input sequence relative to each other. Transformers do not actually consider the sequence order since the self-attention mechanism treats input elements as independent of each other.\n",
    "\n",
    "In natural language the order of elements is very important, otherwise we would find it difficult to communicate. Without taking into account the positional information of tokens in a language, a transformer might interpret different sentences as being equivalent if they contain similar words but in a different order. Therefore we need positional encoding to inject the concept of *sequence order* into the transformer model.\n",
    "\n",
    "The positional encoding for a position *p* with dimension *i* in the input sequence is computed using the sine and cosine functions, as follows:\n",
    "\n",
    "- PE(p, 2<sup>i</sup> ) = *sin*(p / 10000 ^ (2<sup>i</sup> / d_model))\n",
    "- PE(p, 2<sup>i+1</sup>) = *cos*(p / 10000 ^ (2<sup>i</sup> / d_model))\n",
    "\n",
    "As we said, the term `d_model`, above, represents the dimensionality of the input and output vectors in our model.\n",
    "\n",
    "Using large values for `d_model` means that each word will be represented by a larger vector, which will allow the model to capture more complex representations, but would also require more computational resources as a result. \n",
    "\n",
    "The *sin* and *cos* functions are applied alternately to each dimension of the positional encoding vector.  The final positional encodings carry values between -1 and 1, and they what is known as a *wavelength*, which increases with each dimension. \n",
    "\n",
    "In short, we are using the *sin* and *cos* functions of different frequencies to generate the actual positional encoding. The patterns generated by the *sin* and *cos* functions allow the model to identify different positions and generalise to sequence lengths that it did not observe during training. \n",
    "\n",
    "These positional encodings are added to the input embeddings before being processed by the transformer. This allows the transformer to learn to give more attention to adjacent words. This captures the idea that words occurring in proximity to others often have some syntactic or semantic relevance. In short, we can learn the semantics of a word, by the words that surround it. Though in our case, words are represented by the tokenised characters.\n",
    "\n",
    "The class below enables us to write the positional embedding, which we then add to the input layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module that creates a positional encoding matrix. This matrix will later be added to the \n",
    "    transformer's input embeddings to provide a sense of position of the sequence elements.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.positional_encoding = self.create_positional_encoding()\n",
    "\n",
    "    def create_positional_encoding(self):\n",
    "        \"\"\"\n",
    "        Creates a positional encoding matrix of size (max_sequence_length, d_model).\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialise positional encoding matrix\n",
    "        positional_encoding = np.zeros((self.max_sequence_length, self.d_model))\n",
    "\n",
    "        # Calculate positional encoding for each position and each dimension: Here is where we use the sin and cos functions:\n",
    "        for pos in range(self.max_sequence_length):\n",
    "            for i in range(0, self.d_model, 2):\n",
    "                # Apply sin to even indices in the array; indices in Python start at 0 so i is even.\n",
    "                positional_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i) / self.d_model)))\n",
    "                \n",
    "                if i + 1 < self.d_model:\n",
    "                    # Apply cos to odd indices in the array; we add 1 to i because indices in Python start at 0.\n",
    "                    positional_encoding[pos, i + 1] = np.cos(pos / (10000 ** ((2 * i) / self.d_model)))\n",
    "\n",
    "        # Convert numpy array to PyTorch tensor and return it\n",
    "        return torch.from_numpy(positional_encoding).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Adds the positional encoding to the input embeddings at the corresponding positions.\n",
    "        \"\"\"\n",
    "        # Add positional encodings to input embeddings. \n",
    "        # The \":\" indexing ensures we only add positional encodings up\n",
    "        # to the length of the sequence in the batch. x.size(0) is the batch size, so this \n",
    "        # is a way to make sure we are not adding extra positional encodings.\n",
    "        return x + self.positional_encoding[ : x.size(1), : ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking layer\n",
    "Now we come to the masking layer.  The padding tokens we used to increase the length of shorter sequences do not provide any semantic information, and so we want the model to ignore them when it is processing the input data.  This is the role of the masking layer. \n",
    "\n",
    "The mask consists of an array with the same length as the sequence, and with ones at the positions corresponding to actual tokens and zeros for all other positions containing the padding tokens.  When we update the attention scores we apply the mask to the attention scores matrix, which will set the scores for any padding positions to a very large negative number (e.g., -1e9). Using a large negative number is necessary as the scores will be passed onto a softmax function, which will convert any large negative values to zero. This means that all padding positions will have no influence on the final output of the attention layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention layer\n",
    "The attention mechanism is an important component and allows us to highlight the important elements of an input sequence to the model.  The attention mechanism calculates an attention score for each token considering all other tokens in the sequence.\n",
    "\n",
    "Attention scores are derived using query, key, and value vectors. These vectors are generated by multiplying the input embeddings with learned matrices. These vectors contribute to calculating the attention scores, to determine the impact of one token on another.  The attention scores for the tokens are calculated as the dot product between the query and key vectors. These scores are then scaled down by dividing by the square root of the query/key/value dimension for more stable gradients.\n",
    "\n",
    "For masked positions, the attention score will be set to a large negative number, making these positions unavailable in subsequent computations.  In the `MaskedSelfAttention` class below, the operation `masked_fill(mask == 0, -1e9)` is applied to every position in `attention_weights`, where the position in `mask` is equal zero (i.e., a padding token). At this point, we replace the `attention_weights` value with `-1e9` to make sure that all padding tokens are ignored, or given no attention. The attention scores are normalised using the softmax function. This forces to them fall in the range 0 and 1 and will also ensure they sum up to 1. These normalised attention scores are then multiplied with the value vectors and summed to obtain the output of the attention layer.\n",
    "\n",
    "In summary, when the attention mechanism is presented with a sequence of tokens, it takes the query vector attributed to some specific token in the sequence and computes its score against each key in the database. This allows it to capture how the token under consideration relates to the others in the sequence. Afterwhich, it scales the values according to the attention weights (calculated from the scores) to maintain focus on tokens relevant to the query. Finally, it produces an attention output for the token under consideration. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSelfAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module for a self attention layer.\n",
    "    This layer is used in the MultiHeadedSelfAttention module.\n",
    "\n",
    "    Input dimension is: (batch_size, sequence_length, embedding_dimension)\n",
    "    Output dimension is: (batch_size, sequence_length, head_dimension)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dimension, head_dimension):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.head_dimension = head_dimension\n",
    "        self.query_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\n",
    "        self.key_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\n",
    "        self.value_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Compute the self attention.\n",
    "\n",
    "        x dimension is: (batch_size, sequence_length, embedding_dimension)\n",
    "        output dimension is: (batch_size, sequence_length, head_dimension)\n",
    "        mask dimension is: (batch_size, sequence_length)\n",
    "\n",
    "        mask values are: 0 or 1. 0 means the token is masked, 1 means the token is not masked.\n",
    "        \"\"\"\n",
    "\n",
    "        # x dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
    "        # query, key, value dimensions are: (batch_size, sequence_length, head_dimension)\n",
    "        query = self.query_layer(x)\n",
    "        key = self.key_layer(x)\n",
    "        value = self.value_layer(x)\n",
    "\n",
    "        # Calculate the attention weights.\n",
    "        # attention_weights dimensions are: (batch_size, sequence_length, sequence_length)\n",
    "        attention_weights = torch.matmul(query, key.transpose(-2, -1))\n",
    "\n",
    "        # Scale the attention weights.\n",
    "        attention_weights = attention_weights / np.sqrt(self.head_dimension)\n",
    "\n",
    "        # Apply the mask to the attention weights, by setting the masked tokens to a very low value.\n",
    "        # This will make the softmax output 0 for these values.\n",
    "        mask = mask.reshape(attention_weights.shape[0], 1, attention_weights.shape[2])\n",
    "        attention_weights = attention_weights.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Softmax makes sure all scores are between 0 and 1 and the sum of scores is 1.\n",
    "        # attention_scores dimensions are: (batch_size, sequence_length, sequence_length)\n",
    "        attention_scores = self.softmax(attention_weights)\n",
    "\n",
    "        # The attention scores are multiplied by the value\n",
    "        # Values of tokens with high attention score get highlighted because they are multiplied by a larger number,\n",
    "        # and tokens with low attention score get drowned out because they are multiplied by a smaller number.\n",
    "        # Output dimensions are: (batch_size, sequence_length, head_dimension)\n",
    "        return torch.bmm(attention_scores, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head attention\n",
    "The attention mechanism, allows the model to focus on different parts of the input sequence when generating each token in the output sequence. Instead of having one single attention layer, we can have a group to gain a different perspective on the input from each one.\n",
    "\n",
    "If you show a group of people a picture, and talk to them about what they notice, you will obtain different details — one might notice the colors, another the shapes in the picture, and another might comment on the composition. Each of these perspectives gives you information about the whole picture that might be missed by only having the perspective of one individual.\n",
    "\n",
    "This idea has similarities with multi-head attention. Here we have multiple sets (or heads) of the attention mechanism, each independently focuses on different aspects of the input. \n",
    "\n",
    "Each head may learn to pay attention to different positions of the input sequence and extract different types of information. For example, one attention head might learn to focus on the smaller aspects of the language like its syntax, while another might learn semantic information.\n",
    "\n",
    "The `MaskedMultiHeadedSelfAttention` class allows us to specify the embedding layer input and the number of heads (`number_of_heads`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiHeadedSelfAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module for a multi head attention layer.\n",
    "\n",
    "    Input dimension is: (batch_size, sequence_length, embedding_dimension)\n",
    "    Output dimension is: (batch_size, sequence_length, embedding_dimension)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dimension, number_of_heads):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.head_dimension = embedding_dimension // number_of_heads\n",
    "        self.number_of_heads = number_of_heads\n",
    "\n",
    "        # Create the self attention modules\n",
    "        self.self_attentions = torch.nn.ModuleList(\n",
    "            [MaskedSelfAttention(embedding_dimension, self.head_dimension) for _ in range(number_of_heads)])\n",
    "\n",
    "        # Create a linear layer to combine the outputs of the self attention modules\n",
    "        self.output_layer = torch.nn.Linear(number_of_heads * self.head_dimension, embedding_dimension)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Compute the multi head attention.\n",
    "\n",
    "        x dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
    "        mask dimensions are: (batch_size, sequence_length)\n",
    "        mask values are: 0 or 1. 0 means the token is masked, 1 means the token is not masked.\n",
    "        \"\"\"\n",
    "        # Compute the self attention for each head\n",
    "        # self_attention_outputs dimensions are:\n",
    "        # (number_of_heads, batch_size, sequence_length, head_dimension)\n",
    "        self_attention_outputs = [self_attention(x, mask) for self_attention in self.self_attentions]\n",
    "\n",
    "        # Concatenate the self attention outputs\n",
    "        # self_attention_outputs_concatenated dimensions are:\n",
    "        # (batch_size, sequence_length, number_of_heads * head_dimension)\n",
    "        concatenated_self_attention_outputs = torch.cat(self_attention_outputs, dim=2)\n",
    "\n",
    "        # Apply the output layer to the concatenated self attention outputs\n",
    "        # output dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
    "        return self.output_layer(concatenated_self_attention_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The decoder layer\n",
    "Our model also consists of several decoder layers. The first decoder takes the positional encoding layer as input. Each further decoder then takes as input the output of the previous decoder.  \n",
    "\n",
    "The model therefore uses the output of previous layers to inform its understanding of the current layer. Each layer learns to represent different features of the input. The higher-level layers can capture more complex or abstract features, which are composed of simpler features captured by the lower-level layers.  So, in short, it is the stacking of multiple decoder layers, which enables the model to capture more complex patterns and understand deeper contextual relationships among the data.\n",
    "\n",
    "The final layer is the language model head, which is going to output the probabilies of next tokens, which we will discuss later.\n",
    "\n",
    "In the case of an autoregressive model like GPT-2, each decoder layer is composed of a self-attention mechanism and a feed-forward neural network. The self-attention mechanism allows the model to weigh the importance of different tokens in the input when predicting the next token, while the feed-forward network enables the model to learn more traditional, positional relationships between the tokens.\n",
    "\n",
    "The decoder layer is shown below, where we make use of the `DecoderLayer` and `DecoderStack` classes. The class `DecoderLayer` encapsulates the processes executed within a single decoder layer, while the `DecoderStack` class handles the stacking of multiple `DecoderLayers`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module for an encoder layer.\n",
    "\n",
    "    An encoder layer consists of a multi-headed self attention layer, a feed forward layer and dropout.\n",
    "\n",
    "    Input dimension is: (batch_size, sequence_length, embedding_dimension)\n",
    "    Output dimension is: (batch_size, sequence_length, embedding_dimension)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_dimension,\n",
    "            number_of_heads,\n",
    "            feed_forward_dimension,\n",
    "            dropout_rate\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.number_of_heads = number_of_heads\n",
    "        self.feed_forward_dimension = feed_forward_dimension\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.multi_headed_self_attention = MaskedMultiHeadedSelfAttention(embedding_dimension, number_of_heads)\n",
    "        self.feed_forward = FeedForward(embedding_dimension, feed_forward_dimension)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.layer_normalisation_1 = torch.nn.LayerNorm(embedding_dimension)\n",
    "        self.layer_normalisation_2 = torch.nn.LayerNorm(embedding_dimension)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Compute the encoder layer.\n",
    "\n",
    "        x dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
    "        mask dimensions are: (batch_size, sequence_length)\n",
    "        mask values are: 0 or 1. 0 means the token is masked, 1 means the token is not masked.\n",
    "        \"\"\"\n",
    "\n",
    "        # Layer normalization 1\n",
    "        normalised_x = self.layer_normalisation_1(x)\n",
    "\n",
    "        # Multi headed self attention\n",
    "        attention_output = self.multi_headed_self_attention(normalised_x, mask)\n",
    "\n",
    "        # Residual output\n",
    "        residual_output = x + attention_output\n",
    "\n",
    "        # Layer normalization 2\n",
    "        normalised_residual_output = self.layer_normalisation_2(residual_output)\n",
    "\n",
    "        # Feed forward\n",
    "        feed_forward_output = self.feed_forward(normalised_residual_output)\n",
    "\n",
    "        # Dropout, only when training.\n",
    "        if self.training:\n",
    "            feed_forward_output = self.dropout(feed_forward_output)\n",
    "\n",
    "        # Residual output\n",
    "        return residual_output + feed_forward_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DecoderStack` is comoposed of several decoder layers in sequence, defined by the variable `number_of_layers` that we pass when instantiating the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderStack(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module for a stack of decoders.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_dimension,\n",
    "            number_of_layers,\n",
    "            number_of_heads,\n",
    "            feed_forward_dimension,\n",
    "            dropout_rate,\n",
    "            max_sequence_length\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.number_of_layers = number_of_layers\n",
    "        self.number_of_heads = number_of_heads\n",
    "        self.feed_forward_dimension = feed_forward_dimension\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "\n",
    "        # Create the encoder layers\n",
    "        self.encoder_layers = torch.nn.ModuleList(\n",
    "            [DecoderLayer(embedding_dimension, number_of_heads, feed_forward_dimension, dropout_rate) for _ in\n",
    "             range(number_of_layers)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        decoder_outputs = x\n",
    "        for decoder_layer in self.encoder_layers:\n",
    "            decoder_outputs = decoder_layer(decoder_outputs, mask)\n",
    "\n",
    "        return decoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feed forward layer is composed of a fully connected layer, which applies the *ReLU* activation function to the data. This enables the model to learn the complex patterns and relationships present in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module for a feed forward layer.\n",
    "\n",
    "    A feed forward layer is a fully connected layer with a ReLU activation function in between.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dimension, feed_forward_dimension):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.feed_forward_dimension = feed_forward_dimension\n",
    "        self.linear_1 = torch.nn.Linear(embedding_dimension, feed_forward_dimension)\n",
    "        self.linear_2 = torch.nn.Linear(feed_forward_dimension, embedding_dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute the feed forward layer.\n",
    "        \"\"\"\n",
    "        return self.linear_2(torch.relu(self.linear_1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model\n",
    "The `LanguageModel` class, below, brings together all the layers we have created so far, including the token embedding, positional encoding, normalisation, and our stack of decoder layers. After training our first model, we may revisit this to tune the parameters, for instance by adjusting the number of decoder layers (`number_of_layers`) and the number of attention heads (`number_of_heads`).  We also add functions for saving and loading our trained model later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module for a language model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            number_of_tokens,  # The number of tokens in the vocabulary\n",
    "            max_sequence_length=512,  # The maximum sequence length to use for attention\n",
    "            embedding_dimension=512,  # The dimension of the token embeddings\n",
    "            number_of_layers=6,  # The number of decoder layers to use\n",
    "            number_of_heads=4,  # The number of attention heads to use\n",
    "            feed_forward_dimension=None,  # The dimension of the feed forward layer\n",
    "            dropout_rate=0.1  # The dropout rate to use\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.number_of_tokens = number_of_tokens\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.number_of_layers = number_of_layers\n",
    "        self.number_of_heads = number_of_heads\n",
    "\n",
    "        if feed_forward_dimension is None:\n",
    "            # GPT-2 paper uses 4 * embedding_dimension for the feed forward dimension\n",
    "            self.feed_forward_dimension = embedding_dimension * 4\n",
    "        else:\n",
    "            self.feed_forward_dimension = feed_forward_dimension\n",
    "\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Create the token embedding layer\n",
    "        self.token_embedding = TokenEmbedding(embedding_dimension, number_of_tokens)\n",
    "\n",
    "        # Create the positional encoding layer\n",
    "        self.positional_encoding = PositionalEncoding(embedding_dimension, max_sequence_length)\n",
    "\n",
    "        # Create the normalization layer\n",
    "        self.layer_normalisation = torch.nn.LayerNorm(embedding_dimension)\n",
    "\n",
    "        # Create the decoder stack\n",
    "        self.decoder = DecoderStack(\n",
    "            embedding_dimension=embedding_dimension,\n",
    "            number_of_layers=number_of_layers,\n",
    "            number_of_heads=number_of_heads,\n",
    "            feed_forward_dimension=self.feed_forward_dimension,\n",
    "            dropout_rate=dropout_rate,\n",
    "            max_sequence_length=max_sequence_length\n",
    "        )\n",
    "\n",
    "        # Create the language model head\n",
    "        self.lm_head = LMHead(embedding_dimension, number_of_tokens)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Compute the token embeddings\n",
    "        # token_embeddings dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
    "        token_embeddings = self.token_embedding(x)\n",
    "\n",
    "        # Compute the positional encoding\n",
    "        # positional_encoding dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
    "        positional_encoding = self.positional_encoding(token_embeddings)\n",
    "\n",
    "        # Post embedding layer normalization\n",
    "        positional_encoding_normalised = self.layer_normalisation(positional_encoding)\n",
    "\n",
    "        decoder_outputs = self.decoder(positional_encoding_normalised, mask)\n",
    "        lm_head_outputs = self.lm_head(decoder_outputs)\n",
    "\n",
    "        return lm_head_outputs\n",
    "    \n",
    "    def save_checkpoint(self, path):\n",
    "        print(f'Saving checkpoint {path}')\n",
    "        torch.save({\n",
    "            'number_of_tokens': self.number_of_tokens,\n",
    "            'max_sequence_length': self.max_sequence_length,\n",
    "            'embedding_dimension': self.embedding_dimension,\n",
    "            'number_of_layers': self.number_of_layers,\n",
    "            'number_of_heads': self.number_of_heads,\n",
    "            'feed_forward_dimension': self.feed_forward_dimension,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'model_state_dict': self.state_dict()\n",
    "        }, path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_checkpoint(path) -> 'LanguageModel':\n",
    "        checkpoint = torch.load(path)\n",
    "        model = LanguageModel(\n",
    "            number_of_tokens=checkpoint['number_of_tokens'],\n",
    "            max_sequence_length=checkpoint['max_sequence_length'],\n",
    "            embedding_dimension=checkpoint['embedding_dimension'],\n",
    "            number_of_layers=checkpoint['number_of_layers'],\n",
    "            number_of_heads=checkpoint['number_of_heads'],\n",
    "            feed_forward_dimension=checkpoint['feed_forward_dimension'],\n",
    "            dropout_rate=checkpoint['dropout_rate']\n",
    "        )\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language model head\n",
    "\n",
    "The `LanguageModel` class concludes with the `LMHead` layer. This layer is essentially a linear transformation that maps the high-dimensional output of the decoder stack back down to the dimension of the token vocabulary.\n",
    "\n",
    "Recall that we want to assign a probability to each word in the vocabulary given the preceding context. This is the main role of `LMHead`, where it maps the embedding dimension back to the high-dimensional space containing the individual tokens representing our vocabulary. This allows us to assign a score to each of the tokens. These scores are then passed through a softmax function to convert them into probabilities. \n",
    "\n",
    "So `LMHead` is the main component responsible for transforming the high-dimensional output of the decoder stack to the likelihood of each token being the next token in the sequence.  The `LMHead` is implemented as a subclass of PyTorch's `torch.nn.Module`. It uses basic type of neural network layer that applies a linear transformation to the input to map the decoder's output dimension to the number of tokens in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMHead(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module for the language model head.\n",
    "    The language model head is a linear layer that maps the embedding dimension to the vocabulary size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dimension, number_of_tokens):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.number_of_tokens = number_of_tokens\n",
    "        self.linear = torch.nn.Linear(embedding_dimension, number_of_tokens)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute the language model head.\n",
    "\n",
    "        x dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
    "        output dimensions are: (batch_size, sequence_length, number_of_tokens)\n",
    "        \"\"\"\n",
    "        # Compute the linear layer\n",
    "        # linear_output dimensions are: (batch_size, sequence_length, number_of_tokens)\n",
    "        linear_output = self.linear(x)\n",
    "\n",
    "        return linear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive wrapper\n",
    "\n",
    "Recall that our model will take an initial prompt given by us to generate the next word.  It will then take the sequence composed of our initial prompt, plus the last predicted word to then generate the next and so on.\n",
    "\n",
    "To allow our language model to generate text one token at a time, we need an autoregressive model. An autoregressive model, does exactly what we have described, it takes the output from previous steps and feeds them as the input to subsequent steps. \n",
    "\n",
    "To achieve this we will use the `AutoregressiveWrapper`.  The autoregressive wrapper takes a sequence of tokens as input, where the sequence length is one token more than the maximum sequence length allowed. \n",
    "\n",
    "The `AutoregressiveWrapper` class also includes a method for calculating the probabilities of the next token in the sequence. It generates these probabilities by applying a softmax function to the unnormalised scores output by the model for each token in the vocabulary, associated with the last token in the sequence. \n",
    "\n",
    "The `temperature` parameter, in the code below, is used to adjust the sharpness of the probability distribution. Lower temperature values force the output to be increase the likelihood that it will choose the most probable token. With higher values for `temperature`, we see the output becoming more random in terms of the probable token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoregressiveWrapper(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module that wraps a GPT model and makes it autoregressive.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gpt_model):\n",
    "        super().__init__()\n",
    "        self.model = gpt_model\n",
    "        self.max_sequence_length = self.model.max_sequence_length\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Autoregressive forward pass\n",
    "        \"\"\"\n",
    "        inp, target = x[:, :-1], x[:, 1:]\n",
    "        mask = mask[:, :-1]\n",
    "\n",
    "        output = self.model(inp, mask)\n",
    "        return output, target\n",
    "\n",
    "    def next_token_probabilities(self, x, mask, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Calculate the token probabilities for the next token in the sequence.\n",
    "        \"\"\"\n",
    "        logits = self.model(x, mask)[:, -1]\n",
    "\n",
    "        # Apply the temperature\n",
    "        if temperature != 1.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "        # Apply the softmax\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        return probabilities\n",
    "\n",
    "    def save_checkpoint(self, path):\n",
    "        self.model.save_checkpoint(path)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_checkpoint(path) -> 'AutoregressiveWrapper':\n",
    "        model = LanguageModel.load_checkpoint(path)\n",
    "        return AutoregressiveWrapper(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer\n",
    "Now we have a model set up with all the necessary layers of the architecture. We can now begin the training phase. The `Trainer` is a helper class that loops over the epochs and shuffles the data at the start of each epoch. We do this to prevent the batches from being the same each time, which would cause the model to overfit to these specific batches.\n",
    "\n",
    "To get started, we pass in the model, tokeniser, and chosen optimiser to our `Trainer` class. We also configure the loss function.\n",
    "\n",
    "Next we create batches of sequences and their respective masks. Batch size means that in every forward pass through the model we consider a group of sequences upto the specified batch size from the training data simultaneously. The higher the batch size the better the model can learn patterns in the data, but a higher batch size also means we will need more memory to process them.\n",
    "\n",
    "We then train for several epochs, where an epoch represents a complete pass over the whole training data. For each epoch we shuffle the sequences at random, and begin to create several batches of data. This involves creating the input and mask tensors for the batch. Remember that padding tokens are masked, meaning they will not be considered in the attention step.\n",
    "\n",
    "Next we do a forward pass through the model with a batch. This means we let the model make predictions using the given data. The model predictions are then compared to the target value, which is the sequence shifted by one step so that the next token can be seen. \n",
    "\n",
    "For each batch we train the model using the the input and mask tensors, and compute the output. We also compute the losses based on the model output and the target for each epoch.  The model outputs probabilities for what token should be the next, and the loss function knows what the correct answer should be. If the model prediction was way off the target value, then we have a higher loss value. \n",
    "\n",
    "Once we know the loss, we can backpropagate it and clip the gradients to prevent the issue of exploding gradients.  We can then update the model parameters by taking a step in the direction of the gradient.  In essence, we are calculating the direction in which the weights should be adjusted, in order to improve the prediction of the model. If the training is going well, we should see the loss reduce over time.\n",
    "\n",
    "Afterwhich, we reset the gradients so that the gradients from the previous batch are not used in the next step.  Lastly, we append the loss to the list of losses, so that the average loss can be computed for each epoch, and we store these average loss values for plotting later, before moving onto the next batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, tokeniser: Tokeniser, optimiser=None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        if optimiser is None:\n",
    "            self.optimiser = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "        else:\n",
    "            self.optimiser = optimiser\n",
    "        self.tokeniser = tokeniser\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def train(self, data: list[str], epochs, batch_size):\n",
    "        loss_per_epoch = []\n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "\n",
    "            # Shuffle the sequences\n",
    "            random.shuffle(data)\n",
    "\n",
    "            # Create batches of sequences and their respective mask.\n",
    "            batches = []\n",
    "            for i in range(0, len(data), batch_size):\n",
    "                sequence_tensor = torch.tensor(data[i: i + batch_size], dtype=torch.long)\n",
    "\n",
    "                # Create the mask tensor for the batch, where 1 means the token is not a padding token\n",
    "                mask_tensor = torch.ones_like(sequence_tensor)\n",
    "                mask_tensor[sequence_tensor == self.tokeniser.character_to_token('<pad>')] = 0\n",
    "\n",
    "                batches.append((sequence_tensor, mask_tensor))\n",
    "\n",
    "            # Train the model on each batch\n",
    "            for batch in batches:\n",
    "                self.model.train()\n",
    "\n",
    "                # Create the input and mask tensors\n",
    "                input_tensor = torch.zeros((batch_size, self.model.max_sequence_length + 1), dtype=torch.long)\n",
    "                mask_tensor = torch.zeros((batch_size, self.model.max_sequence_length + 1), dtype=torch.long)\n",
    "\n",
    "                for i, input_entry in enumerate(batch[0]):\n",
    "                    input_tensor[i] = input_entry\n",
    "\n",
    "                for i, mask_entry in enumerate(batch[1]):\n",
    "                    mask_tensor[i] = mask_entry\n",
    "\n",
    "                # Compute the model output\n",
    "                model_output, target = self.model.forward(\n",
    "                    x=input_tensor,\n",
    "                    mask=mask_tensor\n",
    "                )\n",
    "                # Compute the losses\n",
    "                # The loss is computed on the model output and the target\n",
    "                loss = self.loss_function(model_output.transpose(1, 2), target)\n",
    "\n",
    "                # Backpropagate the loss.\n",
    "                loss.backward()\n",
    "\n",
    "                # Clip the gradients. This is used to prevent exploding gradients.\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "\n",
    "                # Update the model parameters. This is done by taking a step in the direction of the gradient.\n",
    "                self.optimiser.step()\n",
    "\n",
    "                # Reset the gradients. This is done so that the gradients from the previous batch\n",
    "                # are not used in the next step.\n",
    "                self.optimiser.zero_grad()\n",
    "\n",
    "                # Append the loss to the list of losses, so that the average loss can be computed for this epoch.\n",
    "                losses.append(loss.item())\n",
    "\n",
    "            # Print the loss\n",
    "            epoch_loss = np.average(losses)\n",
    "            loss_per_epoch.append(epoch_loss)\n",
    "            print('Epoch:', epoch, 'Loss:', epoch_loss)\n",
    "\n",
    "        return loss_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "We will create two more helper functions; one to create the input sequences upto a certain max length, and one to tokenise and pad the training data. This will help to keep the training code simpler later on, and we can also use it in the generation step after training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_sequences(max_sequence_length, tokenised_training_data):\n",
    "    # Create sequences of length max_sequence_length + 1\n",
    "    # The last token of each sequence is the target token\n",
    "    sequences = []\n",
    "    for i in range(0, len(tokenised_training_data) - max_sequence_length - 1):\n",
    "        sequences.append(tokenised_training_data[i: i + max_sequence_length + 1])\n",
    "    return sequences\n",
    "\n",
    "def tokenise_and_pad_training_data(max_sequence_length, tokeniser, training_data):\n",
    "    # Tokenise the training data\n",
    "    tokenised_training_data = tokeniser.tokenise(training_data)\n",
    "    for _ in range(max_sequence_length):\n",
    "        # Prepend padding tokens\n",
    "        tokenised_training_data.insert(0, tokeniser.character_to_token('<pad>'))\n",
    "    return tokenised_training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "We will now load our training data again and then move onto the code for kickstarting the training of the actual model.  Training can take a long time with these models, so you can restrict the training set for now so that it runs within a reasonable amount of time. \n",
    "\n",
    "*Note*: this could still be a few hours!! You can interrupt the code execution from the *Kernel* menu, if you wish to go back and tweak the parameters if training takes too long or is too computational expensive for your hardware. \n",
    "\n",
    "You can also tweak the amount of training data to use below (`character_limit`), which can also help reduce training time, but will impact the performance of the model predictions (not that the results will be great):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "character_limit = len(training_data)\n",
    "training_data = training_data[:character_limit]\n",
    "\n",
    "# Set up the tokeniser and other parameters\n",
    "tokeniser = Tokeniser()\n",
    "\n",
    "# We can tweak the dimensions of the embedding to speed up (small values) or \n",
    "# improve training (large values -> 256, 512).\n",
    "embedding_dimension = 256\n",
    "\n",
    "max_sequence_length = 512 # You can also try smaller sequence lengths, e.g. 20\n",
    "\n",
    "number_of_tokens = tokeniser.size()\n",
    "\n",
    "# Create the model: you can play with these network configuration parameters to see how the model performs\n",
    "model = AutoregressiveWrapper(\n",
    "    LanguageModel(\n",
    "        embedding_dimension=embedding_dimension,\n",
    "        number_of_tokens=number_of_tokens,\n",
    "        number_of_heads=4,\n",
    "        number_of_layers=3,\n",
    "        dropout_rate=0.1,\n",
    "        max_sequence_length=max_sequence_length\n",
    "    )\n",
    ")\n",
    "\n",
    "tokenised_and_padded_training_data = tokenise_and_pad_training_data(max_sequence_length, tokeniser, training_data)\n",
    "\n",
    "sequences = create_training_sequences(max_sequence_length, tokenised_and_padded_training_data)\n",
    "\n",
    "# Train the model\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Set up our Trainer class and pass in the model, tokeniser, and optimiser to be used.\n",
    "trainer = Trainer(model, tokeniser, optimiser)\n",
    "\n",
    "# Start training, and return the loss for each epoch to plot later\n",
    "loss_per_epoch = trainer.train(sequences, epochs=2, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "Once training is complete we can plot the loss to see how it decreases over time. The plot is in log scale, so you will observe smaller variations in the loss towards the end of the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the loss per epoch in log scale\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(loss_per_epoch)\n",
    "\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict\n",
    "Now the model has been trained, we wish to see if it actually learnt to generate coherent words from the character sequences it has learnt (our tokens). We write another helper class for generating text, which takes the trained model and the tokeniser as parameters. The `generate` function is then used to pass the length of the sequence we wish to generate and a starting prompt in the form of a word or phrase.\n",
    "\n",
    "We switch the model from *training* mode to *eval* mode (`self.model.eval()`). In the eval mode the model will not apply dropout. \n",
    "\n",
    "As you can see from the code comments we have a similar approach as training. We need to convert our prompt to tokens, and produce a mask.  We keep doing this until we have generated a sequence of tokens upto our maximum settings.  \n",
    "\n",
    "Each prediction of the next token from the model is stored and returned at the end.  In other words, this is done by auto-regressively generating new tokens and adding them to the input sequence. After a token is added we run the new input sequence with the extra token through the model again, and we append a further predicted token. \n",
    "\n",
    "We continue until the maximum number of characters we wanted to generate is reached, or until we have generated the `eos_token` (end of sequence token). This is a token that can be custom defined as an indication to stop generating new tokens. For example, we could set this to the full stop character (`.`), but we will leave this blank for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pads a sequence on the left with a specified padding token until it reaches a desired final length\n",
    "def pad_left(sequence, final_length, padding_token):\n",
    "    return [padding_token] * (final_length - len(sequence)) + sequence\n",
    "\n",
    "# Generator class for autoregressive token-by-token sequence generation\n",
    "class Generator:\n",
    "    def __init__(self, model, tokeniser):\n",
    "        self.model = model              # Trained language model for generation\n",
    "        self.tokeniser = tokeniser      # Tokeniser to convert between text and tokens\n",
    "\n",
    "    def generate(\n",
    "            self,\n",
    "            max_tokens_to_generate: int,  # Maximum number of tokens to generate\n",
    "            prompt: str = None,           # Optional prompt string to condition the generation\n",
    "            temperature: float = 1.0,     # Sampling temperature (higher = more random)\n",
    "            eos_token: int = None,        # Optional end-of-sequence token to stop early\n",
    "            padding_token: int = 0):      # Token used for left-padding inputs\n",
    "\n",
    "        self.model.eval()  # Set the model to evaluation mode (disables dropout etc.)\n",
    "\n",
    "        # Convert the prompt into a list of tokens, or start with padding if no prompt is provided\n",
    "        if prompt is None:\n",
    "            start_tokens = [self.tokeniser.character_to_token(padding_token)]\n",
    "        else:\n",
    "            start_tokens = self.tokeniser.tokenise(prompt)\n",
    "\n",
    "        # Pad the token sequence to fit model input size\n",
    "        input_tensor = torch.tensor(\n",
    "            pad_left(\n",
    "                sequence=start_tokens,\n",
    "                final_length=self.model.max_sequence_length + 1,\n",
    "                padding_token=padding_token\n",
    "            ),\n",
    "            dtype=torch.long\n",
    "        )\n",
    "\n",
    "        # Ensure input tensor is 2D (batch of 1)\n",
    "        if len(input_tensor.shape) == 1:\n",
    "            input_tensor = input_tensor[None, :]\n",
    "\n",
    "        out = input_tensor  # Initialise output with the input prompt\n",
    "\n",
    "        for _ in range(max_tokens_to_generate):\n",
    "            # Only use the most recent max_sequence_length tokens as input\n",
    "            x = out[:, -self.model.max_sequence_length:]\n",
    "\n",
    "            # Create attention mask: 1 for real tokens, 0 for padding\n",
    "            mask = torch.ones_like(x)\n",
    "            mask[x == padding_token] = 0\n",
    "\n",
    "            # Use model to get probabilities for the next token\n",
    "            next_token_probabilities = self.model.next_token_probabilities(\n",
    "                x=x,\n",
    "                temperature=temperature,\n",
    "                mask=mask\n",
    "            )\n",
    "\n",
    "            # Sample one token according to the predicted distribution\n",
    "            next_token = torch.multinomial(next_token_probabilities, num_samples=1)\n",
    "\n",
    "            # Append the sampled token to the current sequence\n",
    "            out = torch.cat([out, next_token], dim=1)\n",
    "\n",
    "            # Stop generation if end-of-sequence token is generated\n",
    "            if eos_token is not None and next_token.item() == eos_token:\n",
    "                break\n",
    "\n",
    "        # Convert generated token IDs back into characters and join into a string\n",
    "        generated_tokens = out[0].tolist()\n",
    "        return ''.join([self.tokeniser.token_to_character(token) for token in generated_tokens])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we create a `prompt` function to make re-running the generator simpler. Inside this function, we instantiate the `Generator`. The prompt we give is converted to tokens, and then padded so it has the correct sequence length. We want to generate a sequence of characters upto the value specified in variable `max_tokens_to_generate`, so you can tweak this as desired:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(model, tokeniser, prompt_text, max_tokens_to_generate = 30):\n",
    "    \n",
    "    generator = Generator(model, tokeniser)\n",
    "    \n",
    "    generated_text = generator.generate(\n",
    "        max_tokens_to_generate=max_tokens_to_generate,\n",
    "        prompt=prompt_text,\n",
    "        padding_token=tokeniser.character_to_token('<pad>')\n",
    "    )\n",
    "\n",
    "    print(generated_text.replace('<pad>', ''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt(model, tokeniser, \"a spring\")\n",
    "\n",
    "prompt(model, tokeniser, \"a spider\")\n",
    "\n",
    "prompt(model, tokeniser, \"a small bear\")\n",
    "\n",
    "prompt(model, tokeniser, \"blue sky\")\n",
    "\n",
    "prompt(model, tokeniser, \"August blue sky\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading the model\n",
    "Once you have trained the model, it is useful if you can save it, so you do not have to spend the time training a new model.  We have added the save and load functionality already, so we can simply call the `save_checkpoint` method and pass in a filename for the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_checkpoint('trained_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to load our pre-trained model we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.load_checkpoint('trained_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to check it ran successfully, we can provide another prompt and generate text as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt(model, tokeniser, \"August\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented a text generator to generate text from a prompt using a transformer model commonly used in Large Language Models like ChatGPT. As you can see even with very little training, the model can generate whole words from characters.  Another thing you will note is the training time involved in training these models.  For best performance you need a very large dataset to produce human-like responses, which will take a very large amount of computational resources and time. The performance of our model is not great. There are other things we can do to improve the model. \n",
    "\n",
    "As we stack multiple layers in our model, each subsequent layer may be able capture more and more complex concepts. The first layer might understand similar words for colours, like red and black. Higher layers might recognise that these colour words play a specific role in a sentence, such as being communicating what things look like. And with even higher layers, the model may start to comprehend more complex semantic relationships, such as colours being associated with feelings, and so forth.\n",
    "\n",
    "Although we are not likely to have the computational power to generate a much larger transformer model to obtain better results, let's experiment with the pre-trained LLM GPT2 for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT2\n",
    "Now that we have learnt about transformers and how we construct them, we will look at a pre-trained model, the GPT-2 model, and generate text based on an input sequence used as the text prompt as before. This exercise will cement some of the concepts we have covered on transformers, so you can compare our small model, with one trained on a much larger data set.  \n",
    "\n",
    "GPT-2 is the successor to GPT, the original NLP framework by OpenAI. The complete GPT-2 model has approximately 1.5 billion parameters. The model was trained on data from 8 million web pages collected from outbound links from Reddit.\n",
    "\n",
    "### Model architecture\n",
    "The language model utilises a transformer based architecture and is comprised of several key components that we have seen before, including input embeddings, encoder layers, decoder layers and output layers:\n",
    "\n",
    "- *Input Embedding*: In this the input text is converted to numerical representations that can be understood by the model. The embedding layer is being deployed for this task which maps each word or token in the input seq to a high dim vector.\n",
    "\n",
    "- *Encoder layer*: GPT2 consists of multiple identical encoder layers stacked over each other. Each encoder layer has two sub layers which are a self attention mechanism and feed forward network. The self attention mechanism allows the model to weigh the importance of different words or tokens with inp. seq thereby capturing the dependencies and relationships betw. them. The feed forward network processes the self attn outputs to gen more complex representations.\n",
    "\n",
    "- *Decoder layer*: It follows the encoder layers and has a similar structure as it also consists of self attention and feed forward layers. Just that in this the decoder layer is conditioned on the context from the prev. tokens enabling autoregressive generation. This means the model predicts the next word in the seq based on the context it has learned so far.\n",
    "\n",
    "- *Output layer*: The final layer of GPT2 is a linear transformation followed by a softmax activation function. This layer produces the probability distribution over the vocab for the next word in the sequence. It alows the model to generate text by sampling from the distribution, or choosing the word with the highest probability.\n",
    "\n",
    "We will first install some additional tools in the form of the `pytorch-transformers` library to bring in the GPT2 tokeniser and Language Model Head modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now import the GPT2 tokeniser and the Language model head. Recall that GPT2 tokenises texts by sub-word, so we will import it and use it to tokenise the text prompt, before presenting it to the model for prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set up the tokeniser and model to be used for prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the model instance to obtain more information about its architecture.  It differs slightly from what we have covered so far, but you will recognise some of the main layers, eg. the embedding layers, normalisation layer (note there are more than one), and the attention layer, not the dimensionality of the layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how the model performs on such a large dataset of Reddit inspired websites, we can give it a text prompt as we have done before to get it started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of new tokens to generate beyond the initial prompt\n",
    "num_tokens_to_generate = 60\n",
    "\n",
    "# Define the initial text prompt\n",
    "prompt_text = \"Once upon a time\"\n",
    "\n",
    "# Convert the prompt into a list of token IDs using the tokeniser\n",
    "indexed_tokens = tokeniser.encode(prompt_text)\n",
    "\n",
    "# Iteratively generate new tokens\n",
    "for i in range(num_tokens_to_generate):\n",
    "    # Convert the current list of token IDs into a tensor of shape [1, sequence_length]\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    \n",
    "    # Disable gradient computation for inference (saves memory and speeds up computation)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor)  # Get the model’s output predictions for the next token\n",
    "\n",
    "        predictions = outputs[0]  # Extract the raw logits from the output tuple\n",
    "        # Get the most probable next token (argmax over the vocabulary at the last position)\n",
    "        predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
    "\n",
    "        # Append the predicted token to the sequence\n",
    "        indexed_tokens = indexed_tokens + [predicted_index]\n",
    "\n",
    "# Decode the final sequence of token IDs back into text\n",
    "predicted_text = tokeniser.decode(indexed_tokens + [predicted_index])\n",
    "\n",
    "# Print the generated continuation of the prompt\n",
    "print(predicted_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, not quite the childrens story we were expecting, and not the best performance. With more data and longer training this can be improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What have we learnt?\n",
    "We built a tiny character-level transformer and saw, in miniature, how the core mechanics behind GPT-style language models work. Even a handful of training steps taught the network to turn random characters into whole words, demonstrating that the self-attention mechanism can pick up local patterns almost immediately. We also observed, however, that quality scales steeply with both data and parameters - without billions of tokens and millions of weights the model soon plateaus, generating short, repetitive fragments rather than coherent prose.\n",
    "\n",
    "Stacking attention layers is central to the transformer’s power. Lower layers latch onto surface similarities, i.e. colours such as *red* and *black* cluster together because they often occupy the same syntactic slots. Mid-level layers start to track roles, recognising that colour words typically modify nouns and convey appearance. Only in the uppermost layers do broader associations emerge, for instance linking colours to emotions or style. Each layer therefore builds on the abstractions of the one beneath it, turning raw character sequences into ever richer semantic representations.\n",
    "\n",
    "We also saw the practical trade-offs of training large models. Bigger architectures demand exponentially more computation and memory, and the marginal gains from additional layers or tokens eventually outstrip what a single workstation can supply. To appreciate the performance ceiling we switched to GPT-2, a pre-trained transformer with 1.5 billion parameters and a vast web-scale corpus behind it. Comparing its output with that of our toy model hopefully reinforces the link between model capacity, data volume, and the fluency of the generated text. Achieving state of the art results requires huge computational power, large volumes of data, and a lot of time (potentially months) of training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
